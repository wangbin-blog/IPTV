#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
IPTVç›´æ’­æºæ™ºèƒ½å¤„ç†ç³»ç»Ÿ
ä¿®å¤ç‰ˆ - ä¸GitHub Actionså·¥ä½œæµå®Œå…¨å…¼å®¹
"""

import requests
import pandas as pd
import re
import os
import subprocess
import time
import stat
import json
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional, Tuple, Any
from urllib.parse import urlparse
import argparse

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("iptv_processor.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class IPTVProcessor:
    """IPTVç›´æ’­æºæ™ºèƒ½å¤„ç†å™¨ - ä¿®å¤ç‰ˆ"""
    
    def __init__(self, config_file: str = None):
        """åˆå§‹åŒ–é…ç½®"""
        self.config = self._load_config(config_file)
        self.sources = self._init_sources()
        self.template = self._load_template()
        self._setup_directories()
        
    def _load_config(self, config_file: str = None) -> Dict[str, Any]:
        """åŠ è½½é…ç½®ï¼Œæ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–"""
        base_config = {
            # è·¯å¾„é…ç½®
            'template_file': os.getenv('TEMPLATE_FILE', 'demo.txt'),
            'output_dir': os.getenv('OUTPUT_DIR', './output'),
            'log_file': 'iptv_processor.log',
            
            # åŠŸèƒ½å‚æ•°
            'max_sources_per_channel': int(os.getenv('MAX_SOURCES_PER_CHANNEL', '8')),
            'min_stream_speed': float(os.getenv('MIN_STREAM_SPEED', '0.8')),
            'test_duration': int(os.getenv('TEST_DURATION', '5')),
            
            # ç½‘ç»œå‚æ•°
            'request_timeout': int(os.getenv('REQUEST_TIMEOUT', '10')),
            'max_redirects': int(os.getenv('MAX_REDIRECTS', '3')),
            'retry_times': int(os.getenv('RETRY_TIMES', '2')),
            'retry_delay': int(os.getenv('RETRY_DELAY', '2')),
            
            # æ€§èƒ½å‚æ•°
            'max_fetch_threads': int(os.getenv('MAX_FETCH_THREADS', '15')),
            'max_test_threads': int(os.getenv('MAX_TEST_THREADS', '20')),
            'speed_test_timeout': int(os.getenv('SPEED_TEST_TIMEOUT', '8')),
            
            # ç”¨æˆ·ä»£ç†
            'user_agent': 'Mozilla/5.0 (compatible; IPTV-Processor/2.0)'
        }
        
        # ä»é…ç½®æ–‡ä»¶åŠ è½½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r') as f:
                    file_config = json.load(f)
                    base_config.update(file_config)
            except Exception as e:
                logger.warning(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
                
        return base_config
    
    def _setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        os.makedirs(self.config['output_dir'], exist_ok=True)
        # è®¾ç½®ç›®å½•æƒé™
        try:
            os.chmod(self.config['output_dir'], 0o755)
        except:
            pass  # æƒé™è®¾ç½®å¤±è´¥ä¸å½±å“ä¸»è¦åŠŸèƒ½
    
    def _init_sources(self) -> List[str]:
        """åˆå§‹åŒ–ç›´æ’­æºåˆ—è¡¨ - å¢å¼ºç¨³å®šæ€§"""
        base_sources = [
            "https://raw.githubusercontent.com/zwc456baby/iptv_alive/master/live.txt",
            "https://live.zbds.top/tv/iptv6.txt", 
            "https://live.zbds.top/tv/iptv4.txt",
            "http://home.jundie.top/Cat/tv/live.txt"
        ]
        
        # å¤‡ç”¨é•œåƒæº
        backup_sources = [
            "https://ghproxy.com/https://raw.githubusercontent.com/zwc456baby/iptv_alive/master/live.txt",
            "https://mirror.ghproxy.com/https://raw.githubusercontent.com/IPTV-World/IPTV-World/master/cn.m3u"
        ]
        
        return base_sources + backup_sources
    
    def _load_template(self) -> Optional[Dict[str, Any]]:
        """å®‰å…¨åŠ è½½é¢‘é“æ¨¡æ¿"""
        template_path = self.config['template_file']
        if not os.path.exists(template_path):
            logger.warning(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")
            return None
            
        try:
            with open(template_path, 'r', encoding='utf-8') as f:
                content = f.read()
                return self._parse_template(content)
        except Exception as e:
            logger.error(f"æ¨¡æ¿åŠ è½½å¤±è´¥: {e}")
            return None
    
    def _parse_template(self, content: str) -> Dict[str, Any]:
        """è§£ææ¨¡æ¿å†…å®¹ - å¢å¼ºå®¹é”™æ€§"""
        template = {'channels': [], 'categories': {}, 'order': []}
        current_category = None
        
        for line in content.splitlines():
            line = line.strip()
            if not line or line.startswith('#'):
                continue
                
            if "#genre#" in line:
                current_category = line.replace("#genre#", "").strip()
            elif ',' in line:
                try:
                    channel = line.split(',')[0].strip()
                    if channel:  # ç¡®ä¿é¢‘é“åä¸ä¸ºç©º
                        template['channels'].append(channel)
                        template['order'].append(channel)
                        if current_category:
                            template['categories'][channel] = current_category
                except IndexError:
                    continue  # è·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ
                    
        return template
    
    def _fetch_with_retry(self, url: str, retry: int = 0) -> Optional[str]:
        """å¢å¼ºçš„å¸¦é‡è¯•æœºåˆ¶çš„æºè·å–"""
        try:
            headers = {
                'User-Agent': self.config['user_agent'],
                'Accept': 'text/plain, */*'
            }
            
            response = requests.get(
                url,
                headers=headers,
                timeout=self.config['request_timeout'],
                allow_redirects=True,
                verify=True  # å¯ç”¨SSLéªŒè¯
            )
            response.raise_for_status()
            
            # éªŒè¯å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '')
            if 'text/plain' not in content_type and 'application/octet-stream' not in content_type:
                logger.warning(f"å¼‚å¸¸å†…å®¹ç±»å‹: {content_type} for {url}")
                
            return response.text
            
        except requests.exceptions.SSLError:
            return self._handle_ssl_error(url, retry)
        except requests.exceptions.ProxyError:
            return self._handle_proxy_error(url, retry)
        except requests.exceptions.RequestException as e:
            return self._handle_network_error(url, retry, e)
        except Exception as e:
            logger.error(f"æœªçŸ¥é”™è¯¯ [{url}]: {e}")
            return None
    
    def _handle_ssl_error(self, url: str, retry: int) -> Optional[str]:
        """å¤„ç†SSLé”™è¯¯"""
        if retry < self.config['retry_times']:
            logger.warning(f"SSLé”™è¯¯ï¼Œå°è¯•é™çº§HTTPSâ†’HTTP: {url}")
            insecure_url = url.replace('https://', 'http://')
            return self._fetch_with_retry(insecure_url, retry + 1)
        return None
    
    def _handle_proxy_error(self, url: str, retry: int) -> Optional[str]:
        """å¤„ç†ä»£ç†é”™è¯¯"""
        if retry == 0:
            logger.warning("æ£€æµ‹åˆ°ä»£ç†é—®é¢˜ï¼Œå°è¯•ç»•è¿‡ä»£ç†...")
            # ä¸´æ—¶ç¦ç”¨ä»£ç†
            session = requests.Session()
            session.trust_env = False
            try:
                response = session.get(url, timeout=self.config['request_timeout'])
                return response.text
            except:
                pass
        return None
    
    def _handle_network_error(self, url: str, retry: int, error: Exception) -> Optional[str]:
        """å¤„ç†ç½‘ç»œé”™è¯¯"""
        if retry < self.config['retry_times']:
            delay = self.config['retry_delay'] * (retry + 1)
            logger.warning(f"è¯·æ±‚å¤±è´¥ [{url}]ï¼Œ{delay}ç§’åé‡è¯•... é”™è¯¯: {error}")
            time.sleep(delay)
            return self._fetch_with_retry(url, retry + 1)
        logger.error(f"æœ€ç»ˆè¯·æ±‚å¤±è´¥ [{url}]: {error}")
        return None
    
    def fetch_all_sources(self) -> Optional[str]:
        """å¹¶å‘è·å–æ‰€æœ‰ç›´æ’­æº - å¢å¼ºç¨³å®šæ€§"""
        logger.info(f"å¼€å§‹ä» {len(self.sources)} ä¸ªæºæŠ“å–æ•°æ®...")
        
        successful_sources = []
        with ThreadPoolExecutor(max_workers=self.config['max_fetch_threads']) as executor:
            futures = {executor.submit(self._fetch_with_retry, url): url for url in self.sources}
            
            for future in as_completed(futures):
                url = futures[future]
                try:
                    content = future.result()
                    if content:
                        successful_sources.append(content)
                        logger.info(f"âœ… æˆåŠŸè·å–: {url}")
                    else:
                        logger.warning(f"âŒ è·å–å¤±è´¥: {url}")
                except Exception as e:
                    logger.error(f"ğŸš¨ å¤„ç†å¼‚å¸¸ [{url}]: {e}")
        
        if successful_sources:
            combined_content = "\n".join(successful_sources)
            logger.info(f"æˆåŠŸè·å– {len(successful_sources)}/{len(self.sources)} ä¸ªæº")
            return combined_content
        else:
            logger.error("æ‰€æœ‰æºè·å–å¤±è´¥")
            return None
    
    def parse_streams(self, content: str) -> List[Dict[str, str]]:
        """è§£æç›´æ’­æºå†…å®¹ - å¢å¼ºå®¹é”™æ€§"""
        streams = []
        
        if not content or not content.strip():
            logger.warning("å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è§£æ")
            return streams
        
        lines = content.splitlines()
        logger.info(f"å¼€å§‹è§£æ {len(lines)} è¡Œå†…å®¹...")
        
        # è‡ªåŠ¨è¯†åˆ«æ ¼å¼
        if content.startswith("#EXTM3U"):
            streams.extend(self._parse_m3u_content(content))
        else:
            streams.extend(self._parse_txt_content(content))
            
        logger.info(f"è§£æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(streams)} ä¸ªæµ")
        return streams
    
    def _parse_m3u_content(self, content: str) -> List[Dict[str, str]]:
        """è§£æM3Uæ ¼å¼å†…å®¹"""
        streams = []
        current_channel = None
        
        for line in content.splitlines():
            line = line.strip()
            if not line:
                continue
                
            if line.startswith("#EXTINF"):
                # å¤šç§æ ¼å¼æ”¯æŒ
                name_match = re.search(r'tvg-name="([^"]+)"', line)
                if name_match:
                    current_channel = name_match.group(1)
                else:
                    # å¤‡ç”¨è§£ææ–¹å¼
                    parts = line.split(',', 1)
                    if len(parts) > 1:
                        current_channel = parts[1].strip()
            elif line.startswith(('http://', 'https://', 'rtmp://', 'rtsp://')):
                if current_channel:
                    streams.append({
                        'channel': current_channel,
                        'url': line
                    })
                    current_channel = None
                    
        return streams
    
    def _parse_txt_content(self, content: str) -> List[Dict[str, str]]:
        """è§£æTXTæ ¼å¼å†…å®¹"""
        streams = []
        
        for line in content.splitlines():
            line = line.strip()
            if not line or line.startswith('#'):
                continue
                
            # æ”¯æŒå¤šç§åˆ†éš”ç¬¦
            for sep in [',', '|', ';']:
                if sep in line:
                    parts = line.split(sep, 1)
                    if len(parts) == 2 and parts[1].startswith(('http://', 'https://')):
                        streams.append({
                            'channel': parts[0].strip(),
                            'url': parts[1].strip()
                        })
                        break
                        
        return streams
    
    def _test_stream_quality(self, url: str) -> Optional[float]:
        """FFmpegæµè´¨é‡æµ‹è¯• - å¢å¼ºç¨³å®šæ€§"""
        try:
            # éªŒè¯URLæ ¼å¼
            if not re.match(r'^https?://', url):
                return None
                
            cmd = [
                'ffmpeg',
                '-hide_banner',
                '-loglevel', 'error',
                '-i', url,
                '-t', str(self.config['test_duration']),
                '-f', 'null',
                '-',
                *self.config.get('ffmpeg_args', [])
            ]
            
            # è¶…æ—¶æ§åˆ¶
            result = subprocess.run(
                cmd,
                stderr=subprocess.PIPE,
                stdout=subprocess.PIPE,
                timeout=self.config['speed_test_timeout'],
                check=False  # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œæ‰‹åŠ¨å¤„ç†
            )
            
            if result.returncode != 0:
                return None
                
            # åˆ†æè¾“å‡º
            output = result.stderr.decode('utf-8', errors='ignore')
            speed_match = re.search(r'speed=\s*([\d.]+)x', output)
            
            if not speed_match:
                return None
                
            speed = float(speed_match.group(1))
            return speed if speed >= self.config['min_stream_speed'] else None
            
        except subprocess.TimeoutExpired:
            logger.debug(f"æµ‹é€Ÿè¶…æ—¶: {url}")
            return None
        except Exception as e:
            logger.debug(f"æµ‹é€Ÿå¤±è´¥ [{url}]: {e}")
            return None
    
    def optimize_streams(self, streams: List[Dict[str, str]]) -> pd.DataFrame:
        """ä¼˜åŒ–ç›´æ’­æºæ•°æ® - å¢å¼ºæ€§èƒ½"""
        if not streams:
            logger.warning("æ²¡æœ‰å¯å¤„ç†çš„æµæ•°æ®")
            return pd.DataFrame(columns=['channel', 'url'])
            
        logger.info("å¼€å§‹ä¼˜åŒ–ç›´æ’­æºæ•°æ®...")
        
        # è½¬æ¢ä¸ºDataFrameå¹¶æ¸…æ´—
        df = pd.DataFrame(streams)
        initial_count = len(df)
        
        # æ•°æ®æ¸…æ´—
        df = df.dropna()
        df = df.drop_duplicates(subset=['channel', 'url'])
        logger.info(f"å»é‡å: {len(df)}/{initial_count} æ¡è®°å½•")
        
        # æ¨¡æ¿è¿‡æ»¤
        if self.template and 'channels' in self.template:
            before_filter = len(df)
            df = df[df['channel'].isin(self.template['channels'])]
            logger.info(f"æ¨¡æ¿è¿‡æ»¤å: {len(df)}/{before_filter} æ¡è®°å½•")
        
        if len(df) == 0:
            logger.warning("è¿‡æ»¤åæ— æœ‰æ•ˆæ•°æ®")
            return pd.DataFrame(columns=['channel', 'url'])
        
        # åˆ†ç»„å¤„ç†
        grouped = df.groupby('channel')['url'].apply(list).reset_index()
        
        # å¤šçº¿ç¨‹æµ‹é€Ÿä¼˜åŒ–
        def optimize_channel_sources(urls: List[str]) -> List[str]:
            if not urls:
                return []
                
            results = {}
            with ThreadPoolExecutor(max_workers=self.config['max_test_threads']) as executor:
                future_to_url = {executor.submit(self._test_stream_quality, url): url for url in urls}
                
                completed = 0
                total = len(urls)
                for future in as_completed(future_to_url):
                    url = future_to_url[future]
                    results[url] = future.result()
                    completed += 1
                    if completed % 10 == 0:
                        logger.info(f"æµ‹é€Ÿè¿›åº¦: {completed}/{total}")
            
            # ç­›é€‰å¹¶æ’åº
            valid_urls = {url: score for url, score in results.items() if score is not None}
            sorted_urls = sorted(valid_urls.keys(), key=lambda x: valid_urls[x], reverse=True)
            
            max_sources = self.config['max_sources_per_channel']
            return sorted_urls[:max_sources]
        
        grouped['url'] = grouped['url'].apply(optimize_channel_sources)
        final_count = sum(len(urls) for urls in grouped['url'])
        logger.info(f"ä¼˜åŒ–å®Œæˆ: {len(grouped)} ä¸ªé¢‘é“, {final_count} ä¸ªæœ‰æ•ˆæº")
        
        return grouped
    
    def generate_outputs(self, data: pd.DataFrame) -> bool:
        """ç”Ÿæˆè¾“å‡ºæ–‡ä»¶ - å¢å¼ºç¨³å®šæ€§"""
        try:
            output_dir = self.config['output_dir']
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡æœ¬æ ¼å¼
            txt_success = self._generate_txt_output(data, output_dir)
            # ç”ŸæˆM3Uæ ¼å¼
            m3u_success = self._generate_m3u_output(data, output_dir)
            
            if txt_success and m3u_success:
                logger.info("âœ… æ‰€æœ‰è¾“å‡ºæ–‡ä»¶ç”ŸæˆæˆåŠŸ")
                return True
            else:
                logger.error("âŒ éƒ¨åˆ†æ–‡ä»¶ç”Ÿæˆå¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"æ–‡ä»¶ç”Ÿæˆå¼‚å¸¸: {e}")
            return False
    
    def _generate_txt_output(self, data: pd.DataFrame, output_dir: str) -> bool:
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼è¾“å‡º"""
        try:
            txt_path = os.path.join(output_dir, 'iptv.txt')
            
            with open(txt_path, 'w', encoding='utf-8') as f:
                current_category = None
                
                for _, row in data.iterrows():
                    channel, urls = row['channel'], row['url']
                    
                    # åˆ†ç±»æ ‡é¢˜
                    if self.template and 'categories' in self.template:
                        category = self.template['categories'].get(channel)
                        if category and category != current_category:
                            f.write(f"\n{category},#genre#\n")
                            current_category = category
                    
                    # å†™å…¥é¢‘é“
                    for url in urls:
                        f.write(f"{channel},{url}\n")
            
            # è®¾ç½®æ–‡ä»¶æƒé™
            try:
                os.chmod(txt_path, 0o644)
            except:
                pass
                
            logger.info(f"ğŸ“„ æ–‡æœ¬æ–‡ä»¶å·²ç”Ÿæˆ: {txt_path}")
            return True
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
            return False
    
    def _generate_m3u_output(self, data: pd.DataFrame, output_dir: str) -> bool:
        """ç”ŸæˆM3Uæ ¼å¼è¾“å‡º"""
        try:
            m3u_path = os.path.join(output_dir, 'iptv.m3u')
            
            with open(m3u_path, 'w', encoding='utf-8') as f:
                f.write("#EXTM3U\n")
                current_category = None
                
                for _, row in data.iterrows():
                    channel, urls = row['channel'], row['url']
                    
                    # åˆ†ç±»æ ‡é¢˜
                    if self.template and 'categories' in self.template:
                        category = self.template['categories'].get(channel)
                        if category and category != current_category:
                            f.write(f'#EXTINF:-1 tvg-name="{category}" group-title="{category}",{category}\n')
                            current_category = category
                    
                    # å†™å…¥é¢‘é“
                    for url in urls:
                        f.write(f'#EXTINF:-1 tvg-name="{channel}",{channel}\n{url}\n')
            
            # è®¾ç½®æ–‡ä»¶æƒé™
            try:
                os.chmod(m3u_path, 0o644)
            except:
                pass
                
            logger.info(f"ğŸµ M3Uæ–‡ä»¶å·²ç”Ÿæˆ: {m3u_path}")
            return True
            
        except Exception as e:
            logger.error(f"M3Uæ–‡ä»¶ç”Ÿæˆå¤±è´¥: {e}")
            return False
    
    def run(self) -> bool:
        """ä¸»æ‰§è¡Œæµç¨‹ - è¿”å›æ‰§è¡ŒçŠ¶æ€"""
        logger.info("ğŸš€ IPTVå¤„ç†å™¨å¼€å§‹è¿è¡Œ")
        
        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®é‡‡é›†
            content = self.fetch_all_sources()
            if not content:
                logger.error("æ•°æ®é‡‡é›†å¤±è´¥")
                return False
            
            # ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®å¤„ç†
            streams = self.parse_streams(content)
            if not streams:
                logger.error("æ•°æ®è§£æå¤±è´¥")
                return False
                
            optimized_data = self.optimize_streams(streams)
            if len(optimized_data) == 0:
                logger.error("æ•°æ®ä¼˜åŒ–åæ— æœ‰æ•ˆç»“æœ")
                return False
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šç»“æœè¾“å‡º
            success = self.generate_outputs(optimized_data)
            if success:
                logger.info("ğŸ‰ å¤„ç†æµç¨‹å®Œæˆ")
            else:
                logger.error("å¤„ç†æµç¨‹å¤±è´¥")
                
            return success
            
        except KeyboardInterrupt:
            logger.warning("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
            return False
        except Exception as e:
            logger.critical(f"æœªå¤„ç†çš„å¼‚å¸¸: {e}", exc_info=True)
            return False

def main():
    """å‘½ä»¤è¡Œå…¥å£ç‚¹"""
    parser = argparse.ArgumentParser(description='IPTVç›´æ’­æºå¤„ç†å™¨')
    parser.add_argument('--config', '-c', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', '-o', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max-threads', type=int, help='æœ€å¤§çº¿ç¨‹æ•°')
    parser.add_argument('--timeout', type=int, help='è¶…æ—¶æ—¶é—´')
    
    args = parser.parse_args()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœæä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼‰
    if args.output_dir:
        os.environ['OUTPUT_DIR'] = args.output_dir
    if args.max_threads:
        os.environ['MAX_FETCH_THREADS'] = str(args.max_threads)
    if args.timeout:
        os.environ['REQUEST_TIMEOUT'] = str(args.timeout)
    
    processor = IPTVProcessor(args.config)
    success = processor.run()
    
    # è¿”å›é€‚å½“çš„é€€å‡ºç 
    exit(0 if success else 1)

if __name__ == "__main__":
    main()
