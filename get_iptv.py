#!/usr/bin/env python3
import requests
import pandas as pd
import re
import os
import time
import subprocess
import sys
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
from typing import Dict, List, Optional, Tuple
from difflib import SequenceMatcher
import platform
import shutil

class IPTVManager:
    def __init__(self):
        # é…ç½®æ–‡ä»¶
        self.SOURCE_URLS = [
                "https://raw.githubusercontent.com/zwc456baby/iptv_alive/master/live.txt",
    "https://raw.githubusercontent.com/iptv-org/iptv/gh-pages/countries/cn.m3u",
    "https://ghfast.top/raw.githubusercontent.com/Supprise0901/TVBox_live/main/live.txt",
    "https://gh-proxy.com/https://raw.githubusercontent.com/wwb521/live/main/tv.m3u",
    "https://gh-proxy.com/https://raw.githubusercontent.com/zeee-u/lzh06/main/fl.m3u",
    "https://raw.githubusercontent.com/Guovin/iptv-database/master/result.txt",  
    "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/cn.m3u",
    "https://raw.githubusercontent.com/suxuang/myIPTV/main/ipv4.m3u",
    "https://raw.githubusercontent.com/vbskycn/iptv/master/tv/iptv4.txt",
    "http://47.120.41.246:8899/zb.txt",
    "https://live.zbds.top/tv/iptv4.txt",
        ]
        
        self.REQUEST_CONFIG = {
            'timeout': 20,
            'retries': 3,
            'headers': {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
        }
        
        self.CHANNEL_CONFIG = {
            'max_sources_per_channel': 8,
            'speed_test_timeout': 8,
            'min_similarity_score': 60,
            'max_workers': min(8, os.cpu_count() or 4),  # æ™ºèƒ½è®¾ç½®å·¥ä½œçº¿ç¨‹æ•°
        }
        
        self.FILE_CONFIG = {
            'template_file': 'demo.txt',
            'output_txt': 'iptv.txt',
            'output_m3u': 'iptv.m3u',
            'temp_dir': 'temp',
            'log_file': 'iptv.log'
        }
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        self._setup_logging()
        self._init_session()
        self._create_directories()
        self._compile_regex()
        
        # çŠ¶æ€å˜é‡
        self.ffmpeg_available = False
        self.processed_count = 0
        self.total_count = 0
        self.start_time = 0
        self.is_terminal = sys.stdout.isatty()  # æ£€æµ‹æ˜¯å¦åœ¨ç»ˆç«¯ä¸­è¿è¡Œ

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.FILE_CONFIG['log_file'], encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def _init_session(self):
        """åˆå§‹åŒ–è¯·æ±‚ä¼šè¯"""
        self.session = requests.Session()
        self.session.headers.update(self.REQUEST_CONFIG['headers'])
        # æ·»åŠ é‡è¯•ç­–ç•¥
        adapter = requests.adapters.HTTPAdapter(max_retries=3)
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)

    def _create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        os.makedirs(self.FILE_CONFIG['temp_dir'], exist_ok=True)

    def _compile_regex(self):
        """ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼"""
        self.ipv4_pattern = re.compile(r'^https?://(\d{1,3}\.){3}\d{1,3}')
        self.ipv6_pattern = re.compile(r'^https?://\[([a-fA-F0-9:]+)\]')
        self.extinf_pattern = re.compile(r'#EXTINF:.*?tvg-name="([^"]+)".*?,(.+)')
        self.category_pattern = re.compile(r'^(.*?),#genre#$')
        self.url_pattern = re.compile(r'https?://[^\s,]+')
        self.channel_pattern = re.compile(r'^([^,]+),?')

    def _print_progress(self, current: int, total: int, prefix: str = '', suffix: str = '', bar_length: int = 50):
        """æ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆä»…åœ¨ç»ˆç«¯ä¸­æ˜¾ç¤ºï¼‰"""
        if not self.is_terminal or total == 0:
            return
            
        percent = min(1.0, float(current) / total)
        arrow_length = int(round(percent * bar_length))
        arrow = '=' * arrow_length
        if arrow_length < bar_length:
            arrow += '>'
        spaces = ' ' * (bar_length - len(arrow))
        
        # è®¡ç®—é¢„è®¡å‰©ä½™æ—¶é—´
        eta_str = ""
        if current > 0 and hasattr(self, 'start_time') and self.start_time > 0:
            elapsed = time.time() - self.start_time
            if elapsed > 0:
                eta = (elapsed / current) * (total - current)
                if eta < 60:
                    eta_str = f"ETA: {eta:.0f}s"
                else:
                    eta_str = f"ETA: {eta/60:.1f}m"
        
        progress_text = f"\r{prefix}[{arrow}{spaces}] {int(round(percent * 100))}% {current}/{total} {eta_str} {suffix}"
        sys.stdout.write(progress_text.ljust(100))
        sys.stdout.flush()

    def check_dependencies(self) -> bool:
        """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
        try:
            import requests
            import pandas as pd
            
            # æ£€æŸ¥pandasç‰ˆæœ¬
            pd_version = pd.__version__
            self.logger.info(f"âœ… Pandasç‰ˆæœ¬: {pd_version}")
            
        except ImportError as e:
            self.logger.error(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
            self.logger.error("ğŸ’¡ è¯·è¿è¡Œ: pip install requests pandas")
            return False
            
        # æ£€æŸ¥FFmpeg
        self.ffmpeg_available = self._check_ffmpeg()
        return True

    def _check_ffmpeg(self) -> bool:
        """æ£€æŸ¥FFmpegå¯ç”¨æ€§"""
        # é¦–å…ˆæ£€æŸ¥ç¯å¢ƒå˜é‡ä¸­çš„ffmpeg
        ffmpeg_path = shutil.which('ffmpeg')
        if not ffmpeg_path:
            self.logger.warning("âš ï¸ FFmpegæœªåœ¨PATHä¸­æ‰¾åˆ°")
            return False
            
        try:
            result = subprocess.run(
                [ffmpeg_path, '-version'], 
                capture_output=True, 
                timeout=5, 
                text=True,
                check=False
            )
            if result.returncode == 0:
                version_line = result.stdout.split('\n')[0] if result.stdout else "æœªçŸ¥ç‰ˆæœ¬"
                self.logger.info(f"âœ… FFmpegå¯ç”¨: {version_line}")
                return True
            else:
                self.logger.warning("âš ï¸ FFmpegæ£€æŸ¥å¤±è´¥")
                return False
        except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
            self.logger.warning(f"âš ï¸ FFmpegæ£€æŸ¥å¼‚å¸¸: {e}")
            return False

    def validate_url(self, url: str) -> bool:
        """éªŒè¯URLæ ¼å¼"""
        try:
            result = urlparse(url)
            if not all([result.scheme in ['http', 'https'], result.netloc]):
                return False
            
            # æ£€æŸ¥å¸¸è§çš„ä¸åˆæ³•URLæ¨¡å¼
            invalid_patterns = [
                'example.com',
                'localhost',
                '127.0.0.1',
                '0.0.0.0',
            ]
            
            if any(pattern in url.lower() for pattern in invalid_patterns):
                return False
                
            return True
        except Exception:
            return False

    def fetch_streams_from_url(self, url: str, retry: int = 0) -> Optional[str]:
        """ä»URLè·å–æµæ•°æ®"""
        try:
            response = self.session.get(url, timeout=self.REQUEST_CONFIG['timeout'])
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                content_length = len(response.text)
                if content_length < 100:  # å†…å®¹å¤ªçŸ­å¯èƒ½æ˜¯é”™è¯¯é¡µé¢
                    self.logger.warning(f"âš ï¸ å†…å®¹è¿‡çŸ­ ({content_length}å­—ç¬¦) - {url}")
                    return None
                    
                self.logger.debug(f"âœ… æˆåŠŸè·å– {urlparse(url).netloc}: {content_length} å­—ç¬¦")
                return response.text
            else:
                self.logger.warning(f"âš ï¸ HTTP {response.status_code} - {url}")
                
        except requests.exceptions.Timeout:
            self.logger.warning(f"â° è¯·æ±‚è¶…æ—¶ - {url}")
        except requests.exceptions.ConnectionError:
            self.logger.warning(f"ğŸ”Œ è¿æ¥é”™è¯¯ - {url}")
        except requests.exceptions.RequestException as e:
            self.logger.warning(f"ğŸŒ ç½‘ç»œé”™è¯¯: {e} - {url}")
        except Exception as e:
            self.logger.warning(f"âŒ è¯·æ±‚å¼‚å¸¸: {e} - {url}")
            
        # é‡è¯•é€»è¾‘
        if retry < self.REQUEST_CONFIG['retries']:
            wait_time = 2 ** retry
            self.logger.info(f"ğŸ”„ é‡è¯•({retry+1}/{self.REQUEST_CONFIG['retries']}) {wait_time}så: {url}")
            time.sleep(wait_time)
            return self.fetch_streams_from_url(url, retry + 1)
            
        return None

    def fetch_all_streams(self) -> str:
        """è·å–æ‰€æœ‰æºçš„æµæ•°æ®"""
        self.logger.info("ğŸš€ å¼€å§‹æ™ºèƒ½å¤šæºæŠ“å–...")
        self.logger.info(f"ğŸ“¡ æºæ•°é‡: {len(self.SOURCE_URLS)}")
        
        all_streams = []
        successful_sources = 0
        failed_sources = []
        self.start_time = time.time()
        
        def fetch_with_progress(url):
            nonlocal successful_sources
            content = self.fetch_streams_from_url(url)
            if content:
                all_streams.append(content)
                successful_sources += 1
                return True, url
            else:
                failed_sources.append(urlparse(url).netloc)
                return False, url
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æŠ“å–
        max_workers = min(len(self.SOURCE_URLS), self.CHANNEL_CONFIG['max_workers'])
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(fetch_with_progress, url): url for url in self.SOURCE_URLS}
            
            for i, future in enumerate(as_completed(future_to_url), 1):
                url = future_to_url[future]
                try:
                    success, source_url = future.result(timeout=30)
                    status = "âœ…" if success else "âŒ"
                    
                    # æ›´æ–°è¿›åº¦
                    self._print_progress(
                        i, len(self.SOURCE_URLS),
                        prefix="ğŸŒ æŠ“å–è¿›åº¦:",
                        suffix=f"æˆåŠŸ: {successful_sources}/{i} | å½“å‰: {urlparse(url).netloc} {status}"
                    )
                except Exception as e:
                    self.logger.error(f"âŒ å¤„ç† {url} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    failed_sources.append(urlparse(url).netloc)
        
        if self.is_terminal:
            print()  # æ¢è¡Œ
            
        self.logger.info(f"ğŸ“Š æŠ“å–å®Œæˆ: {successful_sources}/{len(self.SOURCE_URLS)} ä¸ªæºæˆåŠŸ")
        if failed_sources:
            self.logger.info(f"âš ï¸ å¤±è´¥æº: {', '.join(failed_sources[:5])}{'...' if len(failed_sources) > 5 else ''}")
        
        return "\n".join(all_streams)

    def parse_m3u(self, content: str) -> List[Dict]:
        """è§£æM3Uæ ¼å¼"""
        streams = []
        current_program = None
        current_group = None
        
        for line in content.splitlines():
            line = line.strip()
            if not line:
                continue
                
            if line.startswith("#EXTINF"):
                current_program = "æœªçŸ¥é¢‘é“"
                current_group = "é»˜è®¤åˆ†ç»„"
                
                # æå–é¢‘é“åç§°
                if match := re.search(r'tvg-name="([^"]+)"', line):
                    current_program = match.group(1).strip()
                elif match := re.search(r'#EXTINF:.*?,(.+)', line):
                    current_program = match.group(1).strip()
                
                # æå–åˆ†ç»„ä¿¡æ¯
                if match := re.search(r'group-title="([^"]+)"', line):
                    current_group = match.group(1).strip()
                    
            elif line.startswith(('http://', 'https://')):
                if current_program and self.validate_url(line):
                    streams.append({
                        "program_name": current_program,
                        "stream_url": line,
                        "group": current_group
                    })
                current_program = None
                current_group = None
        
        return streams

    def parse_txt(self, content: str) -> List[Dict]:
        """è§£æTXTæ ¼å¼"""
        streams = []
        current_group = "é»˜è®¤åˆ†ç»„"
        
        for line in content.splitlines():
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            
            # æ£€æµ‹åˆ†ç±»è¡Œ
            if match := self.category_pattern.match(line):
                current_group = match.group(1).strip()
                continue
                
            # å¤„ç†é¢‘é“è¡Œ
            if match := self.channel_pattern.match(line):
                program_name = match.group(1).strip()
                url_match = self.url_pattern.search(line)
                if url_match:
                    stream_url = url_match.group()
                    if self.validate_url(stream_url):
                        streams.append({
                            "program_name": program_name,
                            "stream_url": stream_url,
                            "group": current_group
                        })
        
        return streams

    def organize_streams(self, content: str) -> pd.DataFrame:
        """æ•´ç†æµæ•°æ®"""
        if not content:
            self.logger.error("âŒ æ²¡æœ‰å†…å®¹å¯å¤„ç†")
            return pd.DataFrame()
            
        self.logger.info("ğŸ” è§£ææµæ•°æ®...")
        
        # è‡ªåŠ¨æ£€æµ‹æ ¼å¼å¹¶è§£æ
        if content.startswith("#EXTM3U"):
            streams = self.parse_m3u(content)
        else:
            streams = self.parse_txt(content)
        
        if not streams:
            self.logger.error("âŒ æœªèƒ½è§£æå‡ºä»»ä½•æµæ•°æ®")
            return pd.DataFrame()
            
        df = pd.DataFrame(streams)
        
        # æ•°æ®æ¸…ç†
        initial_count = len(df)
        if initial_count == 0:
            self.logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æµæ•°æ®")
            return pd.DataFrame()
            
        df = df.dropna()
        df = df[df['program_name'].str.len() > 0]
        df = df[df['stream_url'].str.startswith(('http://', 'https://'))]
        
        # å»é‡
        df = df.drop_duplicates(subset=['program_name', 'stream_url'])
        
        # æ¸…ç†é¢‘é“åç§°
        df['program_name'] = df['program_name'].str.strip()
        
        self.logger.info(f"ğŸ§¹ æ•°æ®æ¸…ç†: {initial_count} â†’ {len(df)} ä¸ªæµ")
        return df

    def load_template(self) -> Optional[Dict]:
        """åŠ è½½é¢‘é“æ¨¡æ¿"""
        template_file = self.FILE_CONFIG['template_file']
        if not os.path.exists(template_file):
            self.logger.error(f"âŒ æ¨¡æ¿æ–‡ä»¶ {template_file} ä¸å­˜åœ¨")
            return None
            
        self.logger.info("ğŸ“‹ åŠ è½½æ¨¡æ¿æ–‡ä»¶...")
        categories = {}
        current_category = None
        
        try:
            with open(template_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                        
                    # æ£€æµ‹åˆ†ç±»è¡Œ
                    if match := self.category_pattern.match(line):
                        current_category = match.group(1).strip()
                        categories[current_category] = []
                    elif current_category and line and not line.startswith('#'):
                        # é¢‘é“è¡Œ
                        if match := self.channel_pattern.match(line):
                            channel_name = match.group(1).strip()
                            if channel_name:
                                categories[current_category].append(channel_name)
        except Exception as e:
            self.logger.error(f"âŒ è¯»å–æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
            return None
        
        if not categories:
            self.logger.error("âŒ æ¨¡æ¿æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„é¢‘é“åˆ†ç±»")
            return None
            
        self.logger.info(f"ğŸ“ æ¨¡æ¿åˆ†ç±»: {list(categories.keys())}")
        total_channels = sum(len(channels) for channels in categories.values())
        self.logger.info(f"ğŸ“º æ¨¡æ¿é¢‘é“æ€»æ•°: {total_channels}")
        
        return categories

    def similarity_score(self, str1: str, str2: str) -> int:
        """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦åˆ†æ•°"""
        if not str1 or not str2:
            return 0
            
        # é¢„å¤„ç†å­—ç¬¦ä¸²
        str1_clean = re.sub(r'[^\w]', '', str1.lower())
        str2_clean = re.sub(r'[^\w]', '', str2.lower())
        
        # å®Œå…¨åŒ¹é…
        if str1_clean == str2_clean:
            return 100
        
        # åŒ…å«å…³ç³»ï¼ˆåŒå‘ï¼‰
        if str1_clean in str2_clean:
            return 90
        if str2_clean in str1_clean:
            return 85
        
        # ä½¿ç”¨difflibè®¡ç®—ç›¸ä¼¼åº¦
        try:
            similarity = SequenceMatcher(None, str1_clean, str2_clean).ratio()
            score = int(similarity * 80)
            
            # å…³é”®è¯åŒ¹é…åŠ åˆ†
            keywords = ['cctv', 'å«è§†', 'tv', 'hd', 'fhd', '4k']
            for keyword in keywords:
                if keyword in str1_clean and keyword in str2_clean:
                    score += 5
                    
            return min(score, 100)
        except Exception:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•çš„å…±åŒå­—ç¬¦æ¯”ä¾‹
            common_chars = len(set(str1_clean) & set(str2_clean))
            total_chars = len(set(str1_clean) | set(str2_clean))
            
            if total_chars > 0:
                similarity = (common_chars / total_chars) * 80
                return int(similarity)
        
        return 0

    def speed_test_ffmpeg(self, stream_url: str) -> Tuple[bool, float]:
        """ä½¿ç”¨FFmpegè¿›è¡Œæµåª’ä½“æµ‹é€Ÿ"""
        if not self.ffmpeg_available:
            return False, float('inf')
            
        temp_file = os.path.join(self.FILE_CONFIG['temp_dir'], f'test_{abs(hash(stream_url))}.ts')
        
        try:
            # æ„å»ºFFmpegå‘½ä»¤ï¼ˆå…¼å®¹ä¸åŒç‰ˆæœ¬ï¼‰
            cmd = [
                'ffmpeg',
                '-y',
                '-loglevel', 'quiet',
                '-i', stream_url,
                '-t', '3',
                '-c', 'copy',
                '-f', 'mpegts',
                temp_file
            ]
            
            start_time = time.time()
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                timeout=self.CHANNEL_CONFIG['speed_test_timeout'],
                check=False
            )
            end_time = time.time()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                except:
                    pass
            
            if result.returncode == 0:
                speed = end_time - start_time
                return True, speed
            else:
                return False, float('inf')
                
        except (subprocess.TimeoutExpired, Exception):
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                except:
                    pass
            return False, float('inf')

    def speed_test_simple(self, stream_url: str) -> Tuple[bool, float]:
        """ç®€å•çš„HTTPæµ‹é€Ÿ"""
        try:
            start_time = time.time()
            response = self.session.head(
                stream_url, 
                timeout=self.CHANNEL_CONFIG['speed_test_timeout'],
                allow_redirects=True
            )
            end_time = time.time()
            
            if response.status_code in [200, 302, 301, 307]:
                return True, end_time - start_time
            else:
                return False, float('inf')
        except Exception:
            return False, float('inf')

    def speed_test_sources(self, sources_df: pd.DataFrame) -> pd.DataFrame:
        """å¯¹æºè¿›è¡Œæµ‹é€Ÿ"""
        self.logger.info("âš¡ å¼€å§‹æ™ºèƒ½æµ‹é€Ÿ...")
        self.logger.info(f"ğŸ“Š å¾…æµ‹é€Ÿæºæ€»æ•°: {len(sources_df)}")
        
        if sources_df.empty:
            self.logger.error("âŒ æ²¡æœ‰éœ€è¦æµ‹é€Ÿçš„æº")
            return pd.DataFrame()
            
        results = []
        total_sources = len(sources_df)
        self.total_count = total_sources
        self.processed_count = 0
        self.start_time = time.time()
        
        # è¿›åº¦è®¡æ•°å™¨
        tested_count = 0
        success_count = 0
        
        def test_single_source(row):
            nonlocal tested_count, success_count
            program_name = row['program_name']
            stream_url = row['stream_url']
            
            self.processed_count += 1
            current = self.processed_count
            
            # æ›´æ–°è¿›åº¦æ¡
            self._print_progress(
                current, total_sources,
                prefix="ğŸ“¶ æµ‹é€Ÿè¿›åº¦:",
                suffix=f"æˆåŠŸ: {success_count}/{tested_count} | å½“å‰: {program_name[:12]}..."
            )
            
            # æ™ºèƒ½é€‰æ‹©æµ‹é€Ÿæ–¹å¼
            if any(ext in stream_url.lower() for ext in ['.m3u8', '.ts', '.flv', '.mp4', '.mpeg', '.avi']):
                if self.ffmpeg_available:
                    accessible, speed = self.speed_test_ffmpeg(stream_url)
                else:
                    accessible, speed = self.speed_test_simple(stream_url)
            else:
                accessible, speed = self.speed_test_simple(stream_url)
            
            tested_count += 1
            if accessible:
                success_count += 1
            
            return {
                'program_name': program_name,
                'stream_url': stream_url,
                'accessible': accessible,
                'speed': speed
            }
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘æµ‹é€Ÿ
        max_workers = min(total_sources, self.CHANNEL_CONFIG['max_workers'])
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(test_single_source, row) for _, row in sources_df.iterrows()]
            
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=15)
                    results.append(result)
                except Exception as e:
                    self.logger.error(f"âŒ æµ‹é€Ÿå¼‚å¸¸: {e}")
        
        # å®Œæˆè¿›åº¦æ¡
        if self.is_terminal:
            self._print_progress(total_sources, total_sources, prefix="âœ… æµ‹é€Ÿå®Œæˆ:", suffix="\n")
        
        # è¿‡æ»¤ä¸å¯è®¿é—®çš„æº
        accessible_df = pd.DataFrame(results)
        if accessible_df.empty:
            self.logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„æºé€šè¿‡æµ‹é€Ÿ")
            return accessible_df
            
        accessible_df = accessible_df[accessible_df['accessible']].copy()
        
        success_rate = len(accessible_df) / total_sources if total_sources > 0 else 0
        self.logger.info(f"ğŸ“Š æµ‹é€Ÿç»“æœ: {len(accessible_df)}/{total_sources} ä¸ªæºå¯ç”¨ (æˆåŠŸç‡: {success_rate:.1%})")
        
        return accessible_df

    def filter_and_sort_sources(self, sources_df: pd.DataFrame, template_categories: Dict) -> pd.DataFrame:
        """æ ¹æ®æ¨¡æ¿è¿‡æ»¤å’Œæ’åºæº"""
        self.logger.info("ğŸ¯ å¼€å§‹é¢‘é“åŒ¹é…...")
        
        # è·å–æ‰€æœ‰æ¨¡æ¿é¢‘é“ï¼ˆä¿æŒé¡ºåºï¼‰
        all_template_channels = []
        for category_channels in template_categories.values():
            all_template_channels.extend(category_channels)
        
        self.logger.info(f"ğŸ“‹ æ¨¡æ¿é¢‘é“æ•°: {len(all_template_channels)}")
        self.logger.info(f"ğŸ“¡ å¯ç”¨æºæ•°é‡: {len(sources_df)}")
        
        channel_mapping = {}
        match_results = []
        
        # ä¸ºæ¯ä¸ªæ¨¡æ¿é¢‘é“å¯»æ‰¾æœ€ä½³åŒ¹é…
        for template_channel in all_template_channels:
            best_match = None
            best_score = 0
            best_source_channel = None
            
            # åœ¨æºæ•°æ®ä¸­å¯»æ‰¾æœ€ä½³åŒ¹é…
            for source_channel in sources_df['program_name'].unique():
                score = self.similarity_score(template_channel, source_channel)
                if score > best_score and score >= self.CHANNEL_CONFIG['min_similarity_score']:
                    best_score = score
                    best_match = template_channel
                    best_source_channel = source_channel
            
            if best_match and best_source_channel:
                channel_mapping[best_source_channel] = best_match
                match_results.append((best_match, best_source_channel, best_score))
        
        # æ‰“å°åŒ¹é…ç»“æœ
        if match_results:
            self.logger.info("\nğŸ† æœ€ä½³åŒ¹é…ç»“æœ:")
            displayed_matches = 0
            for template_channel, source_channel, score in sorted(match_results, key=lambda x: x[2], reverse=True):
                if displayed_matches < 15:
                    status = "âœ…" if score >= 80 else "âš ï¸"
                    self.logger.info(f"  {status} {template_channel[:18]:<18} â† {source_channel[:18]:<18} (åŒ¹é…åº¦: {score}%)")
                    displayed_matches += 1
            
            if len(match_results) > 15:
                self.logger.info(f"  ... è¿˜æœ‰ {len(match_results) - 15} ä¸ªåŒ¹é…")
        else:
            self.logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„é¢‘é“")
        
        # è¿‡æ»¤æ•°æ®ï¼Œåªä¿ç•™åŒ¹é…çš„é¢‘é“
        if not channel_mapping:
            self.logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çš„é¢‘é“")
            return pd.DataFrame()
            
        matched_mask = sources_df['program_name'].isin(channel_mapping.keys())
        filtered_df = sources_df[matched_mask].copy()
        
        if filtered_df.empty:
            self.logger.error("âŒ è¿‡æ»¤åæ²¡æœ‰æ•°æ®")
            return filtered_df
            
        # å°†æºé¢‘é“åç§°æ˜ å°„å›æ¨¡æ¿é¢‘é“åç§°
        filtered_df['program_name'] = filtered_df['program_name'].map(channel_mapping)
        
        self.logger.info(f"ğŸ‰ é¢‘é“åŒ¹é…å®Œæˆ: {len(filtered_df)} ä¸ªæµåŒ¹é…åˆ° {len(set(channel_mapping.values()))} ä¸ªæ¨¡æ¿é¢‘é“")
        return filtered_df

    def generate_final_data(self, speed_tested_df: pd.DataFrame, template_categories: Dict) -> Dict:
        """ç”Ÿæˆæœ€ç»ˆæ•°æ®"""
        self.logger.info("ğŸ“º ç”Ÿæˆæ’­æ”¾åˆ—è¡¨...")
        
        final_data = {}
        total_sources = 0
        
        # ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿åˆ†ç±»å’Œé¢‘é“é¡ºåº
        for category, channels in template_categories.items():
            final_data[category] = {}
            
            for channel in channels:
                # è·å–è¯¥é¢‘é“çš„æ‰€æœ‰æº
                channel_sources = speed_tested_df[speed_tested_df['program_name'] == channel]
                
                if not channel_sources.empty:
                    # æŒ‰é€Ÿåº¦æ’åºå¹¶å–å‰8ä¸ª
                    sorted_sources = channel_sources.sort_values('speed').head(
                        self.CHANNEL_CONFIG['max_sources_per_channel']
                    )
                    final_data[category][channel] = sorted_sources[['stream_url', 'speed']].to_dict('records')
                    source_count = len(sorted_sources)
                    total_sources += source_count
                    
                    # æ˜¾ç¤ºæºè´¨é‡ä¿¡æ¯
                    if source_count > 0:
                        best_speed = sorted_sources.iloc[0]['speed']
                        speed_str = f"{best_speed:.2f}s" if best_speed < 10 else ">10s"
                        self.logger.info(f"  âœ… {category[:8]:<8}-{channel[:16]:<16}: {source_count}æº (æœ€ä½³: {speed_str})")
                else:
                    final_data[category][channel] = []
                    self.logger.warning(f"  âŒ {category[:8]:<8}-{channel[:16]:<16}: æ— å¯ç”¨æº")
        
        self.logger.info(f"ğŸ“Š æ€»å…±æ”¶é›†åˆ° {total_sources} ä¸ªæœ‰æ•ˆæº")
        return final_data

    def save_output_files(self, final_data: Dict) -> bool:
        """ä¿å­˜è¾“å‡ºæ–‡ä»¶"""
        self.logger.info("ğŸ’¾ ä¿å­˜æ–‡ä»¶...")
        
        success = True
        
        # ä¿å­˜TXTæ ¼å¼
        try:
            output_txt = self.FILE_CONFIG['output_txt']
            with open(output_txt, 'w', encoding='utf-8') as f:
                for category, channels in final_data.items():
                    f.write(f"{category},#genre#\n")
                    
                    for channel, sources in channels.items():
                        for source in sources:
                            f.write(f"{channel},{source['stream_url']}\n")
                    
                    f.write("\n")
            self.logger.info(f"âœ… TXTæ–‡ä»¶å·²ä¿å­˜: {os.path.abspath(output_txt)}")
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜TXTæ–‡ä»¶å¤±è´¥: {e}")
            success = False
        
        # ä¿å­˜M3Uæ ¼å¼
        try:
            output_m3u = self.FILE_CONFIG['output_m3u']
            with open(output_m3u, 'w', encoding='utf-8') as f:
                f.write("#EXTM3U\n")
                
                for category, channels in final_data.items():
                    for channel, sources in channels.items():
                        for source in sources:
                            f.write(f'#EXTINF:-1 tvg-name="{channel}" group-title="{category}",{channel}\n')
                            f.write(f"{source['stream_url']}\n")
            self.logger.info(f"âœ… M3Uæ–‡ä»¶å·²ä¿å­˜: {os.path.abspath(output_m3u)}")
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜M3Uæ–‡ä»¶å¤±è´¥: {e}")
            success = False
            
        return success

    def print_statistics(self, final_data: Dict):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ğŸ“ˆ ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š")
        print("="*60)
        
        total_channels = 0
        total_sources = 0
        categories_with_sources = 0
        
        for category, channels in final_data.items():
            category_channels = 0
            category_sources = 0
            
            for channel, sources in channels.items():
                if sources:
                    category_channels += 1
                    category_sources += len(sources)
            
            if category_channels > 0:
                categories_with_sources += 1
                avg_sources = category_sources / category_channels if category_channels > 0 else 0
                print(f"  ğŸ“º {category:<12}: {category_channels:2d}é¢‘é“, {category_sources:3d}æº (å¹³å‡: {avg_sources:.1f}æº/é¢‘é“)")
                total_channels += category_channels
                total_sources += category_sources
        
        print("-"*60)
        print(f"ğŸ“Š æ€»è®¡: {total_channels}é¢‘é“, {total_sources}æº")
        print(f"ğŸ“ æœ‰æ•ˆåˆ†ç±»: {categories_with_sources}/{len(final_data)}")
        
        # ç»Ÿè®¡æ— æºçš„é¢‘é“
        no_source_channels = []
        for category, channels in final_data.items():
            for channel, sources in channels.items():
                if not sources:
                    no_source_channels.append(f"{category}-{channel}")
        
        if no_source_channels:
            print(f"âš ï¸  æ— æºé¢‘é“: {len(no_source_channels)}ä¸ª")
            if len(no_source_channels) <= 10:
                for channel in no_source_channels[:10]:
                    print(f"    âŒ {channel}")
            if len(no_source_channels) > 10:
                print(f"    ... è¿˜æœ‰ {len(no_source_channels) - 10} ä¸ªæ— æºé¢‘é“")

    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            temp_dir = self.FILE_CONFIG['temp_dir']
            if os.path.exists(temp_dir):
                for file in os.listdir(temp_dir):
                    file_path = os.path.join(temp_dir, file)
                    if os.path.isfile(file_path):
                        try:
                            os.remove(file_path)
                        except:
                            pass
                self.logger.debug("ğŸ§¹ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")
        except Exception as e:
            self.logger.error(f"âŒ æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    def create_demo_template(self) -> bool:
        """åˆ›å»ºç¤ºä¾‹æ¨¡æ¿æ–‡ä»¶"""
        demo_content = """å¤®è§†é¢‘é“,#genre#
CCTV-1
CCTV-2
CCTV-3
CCTV-4
CCTV-5
CCTV-6
CCTV-7
CCTV-8
CCTV-9
CCTV-10
CCTV-11
CCTV-12
CCTV-13
CCTV-14
CCTV-15

å«è§†é¢‘é“,#genre#
æ¹–å—å«è§†
æµ™æ±Ÿå«è§†
æ±Ÿè‹å«è§†
ä¸œæ–¹å«è§†
åŒ—äº¬å«è§†
å¤©æ´¥å«è§†
å±±ä¸œå«è§†
å¹¿ä¸œå«è§†
æ·±åœ³å«è§†
å®‰å¾½å«è§†

åœ°æ–¹é¢‘é“,#genre#
åŒ—äº¬æ–°é—»
ä¸Šæµ·æ–°é—»
å¹¿å·ç»¼åˆ
æ·±åœ³éƒ½å¸‚
é‡åº†å«è§†
å››å·å«è§†
æ²³å—å«è§†
æ¹–åŒ—å«è§†

é«˜æ¸…é¢‘é“,#genre#
CCTV-1 HD
CCTV-5 HD
æ¹–å—å«è§† HD
æµ™æ±Ÿå«è§† HD
æ±Ÿè‹å«è§† HD
ä¸œæ–¹å«è§† HD
"""
        try:
            with open(self.FILE_CONFIG['template_file'], 'w', encoding='utf-8') as f:
                f.write(demo_content)
            self.logger.info(f"âœ… å·²åˆ›å»ºç¤ºä¾‹æ¨¡æ¿æ–‡ä»¶: {self.FILE_CONFIG['template_file']}")
            self.logger.info("ğŸ“ è¯·ç¼–è¾‘æ­¤æ–‡ä»¶ï¼Œæ·»åŠ æ‚¨éœ€è¦çš„é¢‘é“åˆ—è¡¨")
            return True
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºæ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def run(self):
        """ä¸»è¿è¡Œå‡½æ•°"""
        print("=" * 70)
        print("ğŸ¬ IPTVæ™ºèƒ½ç®¡ç†å·¥å…· - ä¼˜åŒ–ç‰ˆ v2.1")
        print("=" * 70)
        print("âœ¨ ä¼˜åŒ–ç‰¹æ€§: ä¿®å¤å¯¼å…¥+ç»ˆç«¯æ£€æµ‹+æ™ºèƒ½çº¿ç¨‹+å…¼å®¹æ€§æå‡")
        print("-" * 70)
        
        # æ£€æŸ¥ä¾èµ–
        if not self.check_dependencies():
            self.logger.error("âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return
        
        # æ£€æŸ¥æ¨¡æ¿æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºç¤ºä¾‹
        if not os.path.exists(self.FILE_CONFIG['template_file']):
            self.logger.info("ğŸ“„ æœªæ‰¾åˆ°æ¨¡æ¿æ–‡ä»¶ï¼Œåˆ›å»ºç¤ºä¾‹æ¨¡æ¿...")
            if not self.create_demo_template():
                return
            self.logger.info("ğŸ’¡ è¯·ç¼–è¾‘ demo.txt æ–‡ä»¶ï¼Œæ·»åŠ æ‚¨éœ€è¦çš„é¢‘é“ï¼Œç„¶åé‡æ–°è¿è¡Œç¨‹åº")
            return
        
        start_time = time.time()
        
        try:
            # 1. åŠ è½½æ¨¡æ¿
            self.logger.info("\nğŸ“ æ­¥éª¤ 1/7: åŠ è½½é¢‘é“æ¨¡æ¿")
            template_categories = self.load_template()
            if not template_categories:
                return
            
            # 2. è·å–æ‰€æœ‰æºæ•°æ®
            self.logger.info("\nğŸ“ æ­¥éª¤ 2/7: è·å–æºæ•°æ®")
            content = self.fetch_all_streams()
            if not content:
                self.logger.error("âŒ æœªèƒ½è·å–ä»»ä½•æºæ•°æ®")
                return
            
            # 3. æ•´ç†æºæ•°æ®
            self.logger.info("\nğŸ“ æ­¥éª¤ 3/7: æ•´ç†æºæ•°æ®")
            sources_df = self.organize_streams(content)
            if sources_df.empty:
                self.logger.error("âŒ æœªèƒ½è§£æå‡ºæœ‰æ•ˆçš„æµæ•°æ®")
                return
            
            # 4. è¿‡æ»¤å’ŒåŒ¹é…é¢‘é“
            self.logger.info("\nğŸ“ æ­¥éª¤ 4/7: é¢‘é“åŒ¹é…")
            filtered_df = self.filter_and_sort_sources(sources_df, template_categories)
            if filtered_df.empty:
                self.logger.error("âŒ æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æ¨¡æ¿é¢‘é“")
                return
            
            # 5. æµ‹é€Ÿ
            self.logger.info("\nğŸ“ æ­¥éª¤ 5/7: æºæµ‹é€Ÿ")
            speed_tested_df = self.speed_test_sources(filtered_df)
            if speed_tested_df.empty:
                self.logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„æºé€šè¿‡æµ‹é€Ÿ")
                return
            
            # 6. ç”Ÿæˆæœ€ç»ˆæ•°æ®
            self.logger.info("\nğŸ“ æ­¥éª¤ 6/7: ç”Ÿæˆæ’­æ”¾åˆ—è¡¨")
            final_data = self.generate_final_data(speed_tested_df, template_categories)
            
            # 7. ä¿å­˜æ–‡ä»¶
            self.logger.info("\nğŸ“ æ­¥éª¤ 7/7: ä¿å­˜æ–‡ä»¶")
            if not self.save_output_files(final_data):
                self.logger.error("âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥")
                return
            
            # 8. æ‰“å°ç»Ÿè®¡
            self.print_statistics(final_data)
            
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            self.logger.info("\nğŸ‰ å¤„ç†å®Œæˆ!")
            self.logger.info(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
            self.logger.info("ğŸ“‚ ç”Ÿæˆæ–‡ä»¶ä½ç½®:")
            self.logger.info(f"  ğŸ“„ {os.path.abspath(self.FILE_CONFIG['output_txt'])}")
            self.logger.info(f"  ğŸ“„ {os.path.abspath(self.FILE_CONFIG['output_m3u'])}")
            
        except KeyboardInterrupt:
            self.logger.warning("â¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        except Exception as e:
            self.logger.error(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self.cleanup()

def main():
    """ä¸»å‡½æ•°"""
    try:
        manager = IPTVManager()
        manager.run()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·é€€å‡ºç¨‹åº")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
