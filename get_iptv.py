import requests
import pandas as pd
import re
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®åŒºï¼šå¯æ ¹æ®éœ€æ±‚ä¿®æ”¹
URLS = [
    "https://raw.githubusercontent.com/zwc456baby/iptv_alive/master/live.txt",
    "https://raw.githubusercontent.com/iptv-org/iptv/gh-pages/countries/cn.m3u",
    "https://ghfast.top/raw.githubusercontent.com/Supprise0901/TVBox_live/main/live.txt",
    "https://gh-proxy.com/https://raw.githubusercontent.com/wwb521/live/main/tv.m3u",
    "https://gh-proxy.com/https://raw.githubusercontent.com/zeee-u/lzh06/main/fl.m3u",
    "https://raw.githubusercontent.com/Guovin/iptv-database/master/result.txt",  
    "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/cn.m3u",
    "https://raw.githubusercontent.com/suxuang/myIPTV/main/ipv4.m3u",
    "https://raw.githubusercontent.com/kimwang1978/collect-tv-txt/main/others_output.txt",
    "https://raw.githubusercontent.com/vbskycn/iptv/master/tv/iptv4.txt",
    "http://47.120.41.246:8899/zb.txt",
    "https://live.zbds.top/tv/iptv4.txt",
]
DEMO_PATH = "demo.txt"  # å¸¦åˆ†ç±»çš„æ¨¡æ¿æ–‡ä»¶è·¯å¾„
MAX_INTERFACES_PER_CHANNEL = 8  # å•é¢‘é“æœ€å¤§æ¥å£æ•°
SPEED_TEST_TIMEOUT = 10  # æµ‹é€Ÿè¶…æ—¶ï¼ˆç§’ï¼‰
MAX_WORKERS = 15  # æµ‹é€Ÿå¹¶å‘æ•°
FILENAME_PREFIX = "iptv"  # è¾“å‡ºæ–‡ä»¶å‰ç¼€
CATEGORY_MARKER = "##"  # åˆ†ç±»æ ‡è®°ï¼ˆä»¥##å¼€å¤´çš„è¡Œä¸ºåˆ†ç±»åï¼‰

# æ­£åˆ™è¡¨è¾¾å¼
IPV4_PATTERN = re.compile(r'^http://(\d{1,3}\.){3}\d{1,3}')
IPV6_PATTERN = re.compile(r'^http://\[([a-fA-F0-9:]+)\]')
URL_PATTERN = re.compile(r'^https?://')
SPACE_PATTERN = re.compile(r'\s+')


def read_demo_with_categories(demo_path: str) -> tuple[list[dict], list[str]] | tuple[None, None]:
    """è¯»å–å¸¦åˆ†ç±»çš„demoæ¨¡æ¿ï¼Œè¿”å›åˆ†ç±»ç»“æ„å’Œçº¯é¢‘é“åˆ—è¡¨"""
    if not os.path.exists(demo_path):
        print(f"âŒ é”™è¯¯ï¼šæ¨¡æ¿æ–‡ä»¶ '{demo_path}' ä¸å­˜åœ¨ï¼")
        return None, None
    
    categories = []  # åˆ†ç±»ç»“æ„ï¼š[{"category": "åˆ†ç±»å", "channels": ["é¢‘é“1", "é¢‘é“2"]}]
    current_category = None
    demo_channels = []  # çº¯é¢‘é“åˆ—è¡¨ï¼ˆå»é‡ï¼Œç”¨äºè¿‡æ»¤ï¼‰
    
    with open(demo_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            # è·³è¿‡ç©ºè¡Œå’Œæ™®é€šæ³¨é‡Šï¼ˆ#å¼€å¤´ï¼Œéåˆ†ç±»æ ‡è®°ï¼‰
            if not line:
                continue
            if line.startswith("#") and not line.startswith(CATEGORY_MARKER):
                continue
            
            # è¯†åˆ«åˆ†ç±»è¡Œï¼ˆ##å¼€å¤´ï¼‰
            if line.startswith(CATEGORY_MARKER):
                current_category = SPACE_PATTERN.sub("", line.lstrip(CATEGORY_MARKER).strip())
                if current_category:
                    categories.append({"category": current_category, "channels": []})
                continue
            
            # å¤„ç†é¢‘é“è¡Œï¼ˆå±äºå½“å‰åˆ†ç±»ï¼‰
            if current_category is None:
                print(f"âš ï¸ ç¬¬{line_num}è¡Œï¼šé¢‘é“æœªæŒ‡å®šåˆ†ç±»ï¼Œé»˜è®¤å½’ä¸ºã€Œæœªåˆ†ç±»ã€")
                if not any(c["category"] == "æœªåˆ†ç±»" for c in categories):
                    categories.append({"category": "æœªåˆ†ç±»", "channels": []})
                current_category = "æœªåˆ†ç±»"
            
            # æå–é¢‘é“åï¼ˆå…¼å®¹â€œé¢‘é“å,URLâ€æ ¼å¼ï¼‰
            channel = SPACE_PATTERN.sub("", line.split(",")[0].strip())
            if channel and channel not in demo_channels:
                demo_channels.append(channel)
                # å°†é¢‘é“åŠ å…¥å½“å‰åˆ†ç±»
                for cat in categories:
                    if cat["category"] == current_category:
                        cat["channels"].append(channel)
                        break
    
    # éªŒè¯åˆ†ç±»ç»“æ„
    if not categories:
        print("âš ï¸ è­¦å‘Šï¼šæ¨¡æ¿æ–‡ä»¶æ— æœ‰æ•ˆåˆ†ç±»å’Œé¢‘é“")
        return None, None
    total_channels = sum(len(c["channels"]) for c in categories)
    print(f"ğŸ“º ä»æ¨¡æ¿è¯»å–åˆ†ç±»ï¼š{len(categories)} ä¸ªåˆ†ç±»ï¼Œå…± {total_channels} ä¸ªæœ‰æ•ˆé¢‘é“")
    
    # æ‰“å°åˆ†ç±»è¯¦æƒ…
    for i, cat in enumerate(categories, 1):
        print(f"  {i}. {cat['category']}ï¼š{len(cat['channels'])} ä¸ªé¢‘é“")
    
    return categories, demo_channels


def fetch_streams_from_url(url: str) -> str | None:
    print(f"\nğŸ” æ­£åœ¨çˆ¬å–æºï¼š{url}")
    try:
        response = requests.get(
            url,
            timeout=10,
            headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
        )
        response.encoding = response.apparent_encoding
        if response.status_code == 200:
            print(f"âœ… çˆ¬å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦ï¼š{len(response.text)} å­—ç¬¦")
            return response.text
        print(f"âŒ çˆ¬å–å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
    except requests.exceptions.Timeout:
        print(f"âŒ è¯·æ±‚è¶…æ—¶ï¼ˆè¶…è¿‡10ç§’ï¼‰")
    except requests.exceptions.ConnectionError:
        print(f"âŒ è¿æ¥é”™è¯¯")
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯ï¼š{str(e)[:50]}")
    return None


def fetch_all_streams(urls: list) -> str:
    all_content = []
    for url in urls:
        if content := fetch_streams_from_url(url):
            all_content.append(content)
        else:
            print(f"â­ï¸  è·³è¿‡æ— æ•ˆæºï¼š{url}")
    return "\n".join(all_content)


def parse_m3u(content: str) -> list[dict]:
    streams = []
    current_program = None
    for line in content.splitlines():
        line = line.strip()
        if line.startswith("#EXTINF"):
            if match := re.search(r'tvg-name=(["\']?)([^"\']+)\1', line):
                current_program = SPACE_PATTERN.sub("", match.group(2).strip())
        elif URL_PATTERN.match(line) and current_program:
            streams.append({"program_name": current_program, "stream_url": line})
            current_program = None
    print(f"ğŸ“Š è§£æM3Uæ ¼å¼ï¼šæå–åˆ° {len(streams)} ä¸ªç›´æ’­æº")
    return streams


def parse_txt(content: str) -> list[dict]:
    streams = []
    for line in content.splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        if match := re.match(r'(.+?)\s*,\s*(https?://.+)$', line):
            program = SPACE_PATTERN.sub("", match.group(1).strip())
            url = match.group(2).strip()
            streams.append({"program_name": program, "stream_url": url})
    print(f"ğŸ“Š è§£æTXTæ ¼å¼ï¼šæå–åˆ° {len(streams)} ä¸ªç›´æ’­æº")
    return streams


def test_stream_speed(stream_url: str, timeout: int) -> int | None:
    start_time = time.time()
    try:
        for method in [requests.head, requests.get]:
            try:
                kwargs = {"timeout": timeout, "allow_redirects": True}
                if method == requests.get:
                    kwargs["stream"] = True
                response = method(stream_url, **kwargs)
                if response.status_code in [200, 206]:
                    if method == requests.get:
                        response.iter_content(1).__next__()
                    return int((time.time() - start_time) * 1000)
            except:
                continue
        return None
    except Exception:
        return None


def batch_test_speeds(streams_df: pd.DataFrame, max_workers: int, timeout: int) -> pd.DataFrame:
    total = len(streams_df)
    if total == 0:
        return pd.DataFrame()
    
    print(f"\nâš¡ å¼€å§‹æµ‹é€Ÿï¼ˆå…± {total} ä¸ªæºï¼Œå¹¶å‘æ•°ï¼š{max_workers}ï¼Œè¶…æ—¶ï¼š{timeout}ç§’ï¼‰")
    speed_results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(test_stream_speed, row["stream_url"], timeout):
            (row["program_name"], row["stream_url"])
            for _, row in streams_df.iterrows()
        }
        
        for idx, future in enumerate(as_completed(futures), 1):
            program, url = futures[future]
            speed = future.result()
            url_short = url[:50] + "..." if len(url) > 50 else url
            
            if speed is not None:
                speed_results.append({"program_name": program, "stream_url": url, "speed_ms": speed})
                print(f"âœ… [{idx}/{total}] {program:<15} {url_short:<55} è€—æ—¶ï¼š{speed}ms")
            else:
                print(f"âŒ [{idx}/{total}] {program:<15} {url_short:<55} è¶…æ—¶/ä¸å¯ç”¨")
    
    speed_df = pd.DataFrame(speed_results)
    if not speed_df.empty:
        speed_df = speed_df.sort_values("speed_ms").reset_index(drop=True)
    
    print(f"\nğŸ æµ‹é€Ÿå®Œæˆï¼šæœ‰æ•ˆæº {len(speed_df)} ä¸ªï¼Œæ— æ•ˆæº {total - len(speed_df)} ä¸ª")
    return speed_df


def organize_streams(
    content: str,
    categories: list[dict],
    demo_channels: list,
    max_interfaces: int,
    max_workers: int,
    speed_timeout: int
) -> list[dict]:
    """æŒ‰åˆ†ç±»æ•´ç†æ•°æ®ï¼Œè¿”å›å¸¦åˆ†ç±»çš„ç»“æ„åŒ–æ•°æ®"""
    # 1. è§£æåŸå§‹æ•°æ®
    if content.startswith("#EXTM3U"):
        streams = parse_m3u(content)
    else:
        streams = parse_txt(content)
    df = pd.DataFrame(streams)
    if df.empty:
        print("âš ï¸ æœªè§£æåˆ°ä»»ä½•ç›´æ’­æº")
        return []
    
    # 2. è¿‡æ»¤+å»é‡
    df["program_clean"] = df["program_name"].apply(lambda x: SPACE_PATTERN.sub("", x))
    demo_clean = demo_channels
    df_filtered = df[df["program_clean"].isin(demo_clean)].drop("program_clean", axis=1)
    df_filtered = df_filtered.drop_duplicates(subset=["program_name", "stream_url"])
    
    if df_filtered.empty:
        print("âš ï¸ æ— åŒ¹é…demoæ¨¡æ¿çš„ç›´æ’­æº")
        return []
    print(f"\nğŸ” è¿‡æ»¤åå‰©ä½™ {len(df_filtered)} ä¸ªåŒ¹é…æ¨¡æ¿çš„ç›´æ’­æº")
    
    # 3. æ‰¹é‡æµ‹é€Ÿ
    df_with_speed = batch_test_speeds(df_filtered, max_workers, speed_timeout)
    if df_with_speed.empty:
        print("âš ï¸ æ‰€æœ‰åŒ¹é…æºå‡æµ‹é€Ÿå¤±è´¥")
        return []
    
    # 4. æŒ‰åˆ†ç±»+é¢‘é“æ’åºï¼Œé™åˆ¶æ¥å£æ•°
    organized_categories = []
    for cat in categories:
        # ç­›é€‰å½“å‰åˆ†ç±»çš„é¢‘é“
        cat_channels = cat["channels"]
        df_cat = df_with_speed[df_with_speed["program_name"].isin(cat_channels)]
        
        if df_cat.empty:
            continue  # è·³è¿‡æ— æœ‰æ•ˆæºçš„åˆ†ç±»
        
        # æŒ‰æ¨¡æ¿ä¸­é¢‘é“é¡ºåºæ’åº
        df_cat["program_name"] = pd.Categorical(
            df_cat["program_name"], categories=cat_channels, ordered=True
        )
        df_cat_sorted = df_cat.sort_values(["program_name", "speed_ms"]).reset_index(drop=True)
        
        # æŒ‰é¢‘é“åˆ†ç»„ï¼Œé™åˆ¶æ¥å£æ•°
        def limit_interfaces(group):
            limited = group.head(max_interfaces)
            return pd.Series({
                "stream_url": limited["stream_url"].tolist(),
                "interface_count": len(limited)
            })
        
        df_cat_grouped = df_cat_sorted.groupby("program_name").apply(limit_interfaces).reset_index()
        df_cat_grouped = df_cat_grouped[df_cat_grouped["interface_count"] > 0]
        
        if not df_cat_grouped.empty:
            organized_categories.append({
                "category": cat["category"],
                "channels": df_cat_grouped.to_dict("records")  # æ¯ä¸ªé¢‘é“çš„URLå’Œæ¥å£æ•°
            })
    
    return organized_categories


def save_to_txt(organized_categories: list[dict], prefix: str, max_interfaces: int) -> None:
    if not organized_categories:
        return
    
    # è®¡ç®—æ€»æ¥å£æ•°
    total_interfaces = 0
    for cat in organized_categories:
        total_interfaces += sum(ch["interface_count"] for ch in cat["channels"])
    
    filename = f"{prefix}_åˆ†ç±»_å•é¢‘é“é™{max_interfaces}_æ€»æ¥å£{total_interfaces}.txt"
    content_lines = [f"# IPTVç›´æ’­æºï¼ˆæŒ‰åˆ†ç±»æ•´ç†ï¼‰", f"# æ€»åˆ†ç±»æ•°ï¼š{len(organized_categories)}ï¼Œæ€»æ¥å£æ•°ï¼š{total_interfaces}", ""]
    
    for cat in organized_categories:
        # åˆ†ç±»æ ‡é¢˜
        content_lines.append(f"\n{CATEGORY_MARKER} {cat['category']}")
        content_lines.append(f"# åˆ†ç±»ä¸‹æœ‰æ•ˆé¢‘é“æ•°ï¼š{len(cat['channels'])}")
        
        # æŒ‰IPv4/IPv6åˆ†ç»„
        cat_ipv4 = []
        cat_ipv6 = []
        for ch in cat["channels"]:
            program = ch["program_name"]
            count = ch["interface_count"]
            urls = ch["stream_url"]
            note = f"# {program}ï¼ˆä¿ç•™ï¼š{count}/{max_interfaces}ä¸ªï¼‰"
            
            for url in urls:
                line = f"{program},{url}"
                if IPV4_PATTERN.match(url):
                    cat_ipv4.append((note, line))
                    note = ""  # åªæ·»åŠ ä¸€æ¬¡é¢‘é“æ³¨é‡Š
                elif IPV6_PATTERN.match(url):
                    cat_ipv6.append((note, line))
                    note = ""
        
        # æ·»åŠ IPv4å†…å®¹
        if cat_ipv4:
            content_lines.append("\n# --- IPv4 æº ---")
            for note, line in cat_ipv4:
                if note:
                    content_lines.append(note)
                content_lines.append(line)
        
        # æ·»åŠ IPv6å†…å®¹
        if cat_ipv6:
            content_lines.append("\n# --- IPv6 æº ---")
            for note, line in cat_ipv6:
                if note:
                    content_lines.append(note)
                content_lines.append(line)
    
    # å†™å…¥æ–‡ä»¶
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("\n".join([line for line in content_lines if line]))
    
    print(f"\nğŸ“„ TXTæ–‡ä»¶å·²ä¿å­˜ï¼š{os.path.abspath(filename)}")


def save_to_m3u(organized_categories: list[dict], prefix: str, max_interfaces: int) -> None:
    if not organized_categories:
        return
    
    total_interfaces = 0
    for cat in organized_categories:
        total_interfaces += sum(ch["interface_count"] for ch in cat["channels"])
    
    filename = f"{prefix}_åˆ†ç±»_å•é¢‘é“é™{max_interfaces}_æ€»æ¥å£{total_interfaces}.m3u"
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("#EXTM3U\n")
        f.write(f"# IPTVç›´æ’­æºï¼ˆæŒ‰åˆ†ç±»æ•´ç†ï¼‰\n")
        f.write(f"# æ€»åˆ†ç±»æ•°ï¼š{len(organized_categories)}ï¼Œæ€»æ¥å£æ•°ï¼š{total_interfaces}\n")
        f.write(f"# å•é¢‘é“æœ€å¤šä¿ç•™{max_interfaces}ä¸ªæ¥å£ï¼ŒåŒé¢‘é“æŒ‰é€Ÿåº¦æ’åº\n\n")
        
        for cat in organized_categories:
            # åˆ†ç±»æ³¨é‡Šï¼ˆç”¨##æ ‡è®°ï¼Œæ’­æ”¾å™¨å¿½ç•¥ï¼‰
            f.write(f"# {CATEGORY_MARKER} {cat['category']}\n")
            f.write(f"# åˆ†ç±»ä¸‹æœ‰æ•ˆé¢‘é“æ•°ï¼š{len(cat['channels'])}\n\n")
            
            for ch in cat["channels"]:
                program = ch["program_name"]
                count = ch["interface_count"]
                urls = ch["stream_url"]
                f.write(f"# {program}ï¼ˆä¿ç•™ï¼š{count}/{max_interfaces}ä¸ªï¼‰\n")
                
                for url in urls:
                    f.write(f'#EXTINF:-1 tvg-name="{program}",{program}\n')
                    f.write(f"{url}\n\n")
    
    print(f"ğŸ“º M3Uæ–‡ä»¶å·²ä¿å­˜ï¼š{os.path.abspath(filename)}")


def main():
    print("=" * 60)
    print("ğŸ“¡ IPTVç›´æ’­æºæŠ“å–æ•´ç†å·¥å…·ï¼ˆåˆ†ç±»ç‰ˆï¼‰")
    print("=" * 60)
    
    # 1. è¯»å–å¸¦åˆ†ç±»çš„æ¨¡æ¿
    categories, demo_channels = read_demo_with_categories(DEMO_PATH)
    if not categories or not demo_channels:
        print("\nâŒ ç¨‹åºç»ˆæ­¢ï¼šç¼ºå°‘æœ‰æ•ˆåˆ†ç±»/é¢‘é“")
        return
    
    # 2. æŠ“å–ç›´æ’­æº
    print("\n" + "-" * 60)
    all_content = fetch_all_streams(URLS)
    if not all_content.strip():
        print("\nâŒ ç¨‹åºç»ˆæ­¢ï¼šæœªæŠ“å–åˆ°æœ‰æ•ˆå†…å®¹")
        return
    
    # 3. æŒ‰åˆ†ç±»æ•´ç†æ•°æ®
    print("\n" + "-" * 60)
    organized = organize_streams(
        content=all_content,
        categories=categories,
        demo_channels=demo_channels,
        max_interfaces=MAX_INTERFACES_PER_CHANNEL,
        max_workers=MAX_WORKERS,
        speed_timeout
