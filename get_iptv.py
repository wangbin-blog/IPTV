#!/usr/bin/env python3
"""
IPTVæ™ºèƒ½ç®¡ç†å·¥å…· - å®Œæ•´ä¼ä¸šçº§ç‰ˆæœ¬
åŠŸèƒ½ï¼šå¤šæºæŠ“å–ã€é¢‘é“åŒ¹é…ã€é€Ÿåº¦æµ‹è¯•ã€æ’­æ”¾åˆ—è¡¨ç”Ÿæˆã€é…ç½®ç®¡ç†ã€æ•°æ®éªŒè¯
ç‰ˆæœ¬ï¼šv8.0 (å®Œæ•´ä¼ä¸šçº§ç‰ˆæœ¬)
"""

import requests
import pandas as pd
import re
import os
import time
import subprocess
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import hashlib
import json
from datetime import datetime
import shutil

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    handlers=[
        logging.FileHandler('iptv_manager.log', encoding='utf-8', mode='w'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger('IPTVManager')

# å¯é€‰ä¾èµ–å¤„ç†
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    logger.warning("TQDMæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•è¿›åº¦æ˜¾ç¤º")

@dataclass
class AppConfig:
    """åº”ç”¨é…ç½®ç±»"""
    source_urls: List[str]
    request_timeout: int = 15
    max_sources_per_channel: int = 8
    speed_test_timeout: int = 5
    similarity_threshold: int = 50
    max_workers: int = 6
    template_file: str = "demo.txt"
    output_txt: str = "iptv.txt"
    output_m3u: str = "iptv.m3u"
    temp_dir: str = "temp"
    cache_enabled: bool = True
    cache_expiry: int = 3600  # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'AppConfig':
        """ä»å­—å…¸åˆ›å»ºé…ç½®"""
        return cls(**data)

class IPTVManager:
    """IPTVæ™ºèƒ½ç®¡ç†å·¥å…·ä¸»ç±»"""
    
    def __init__(self, config: Optional[AppConfig] = None):
        """åˆå§‹åŒ–IPTVç®¡ç†å™¨"""
        # ä½¿ç”¨é»˜è®¤é…ç½®æˆ–ä¼ å…¥é…ç½®
        self.config = config or AppConfig(
            source_urls=[
                "https://raw.githubusercontent.com/zwc456baby/iptv_alive/master/live.txt",
                "https://live.zbds.top/tv/iptv6.txt", 
                "https://live.zbds.top/tv/iptv4.txt",
                "http://home.jundie.top:81/top/tvbox.txt",
                "https://mirror.ghproxy.com/https://raw.githubusercontent.com/YanG-1989/m3u/main/Gather.m3u",
            ]
        )
        
        # åˆå§‹åŒ–ä¼šè¯å’Œç›®å½•
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self._setup_directories()
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self._compile_patterns()
        
        # çŠ¶æ€å˜é‡
        self.ffmpeg_available = False
        self.processed_count = 0
        self.total_count = 0
        self.cache_dir = Path("cache")
        self.backup_dir = Path("backups")
        self.checkpoint_file = Path("checkpoint.json")
        self.current_stage = "not_started"
        
        # åˆ›å»ºç›®å½•
        self.cache_dir.mkdir(exist_ok=True)
        self.backup_dir.mkdir(exist_ok=True)

    def _setup_directories(self) -> None:
        """è®¾ç½®å¿…è¦çš„ç›®å½•"""
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_path = Path(self.config.temp_dir)
            temp_path.mkdir(exist_ok=True)
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = Path(".").absolute()
            logger.info(f"å·¥ä½œç›®å½•: {output_dir}")
            
        except Exception as e:
            logger.error(f"ç›®å½•è®¾ç½®å¤±è´¥: {e}")
            raise

    def _compile_patterns(self) -> None:
        """ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        self.patterns = {
            'ipv4': re.compile(r'^https?://(\d{1,3}\.){3}\d{1,3}'),
            'ipv6': re.compile(r'^https?://\[([a-fA-F0-9:]+)\]'),
            'extinf': re.compile(r'#EXTINF:.*?tvg-name="([^"]+)".*?,(.+)'),
            'category': re.compile(r'^(.*?),#genre#$'),
            'url': re.compile(r'https?://[^\s,]+'),
            'channel_name': re.compile(r'[#EXTINF:].*?,(.+)$'),
            'clean_name': re.compile(r'[^\w\u4e00-\u9fa5\s-]'),
            'tvg_name': re.compile(r'tvg-name="([^"]*)"'),
            'tvg_id': re.compile(r'tvg-id="([^"]*)"'),
            'group_title': re.compile(r'group-title="([^"]*)"'),
            'extinf_content': re.compile(r',\s*(.+)$')
        }

    def save_config(self, config_path: str = "iptv_config.json") -> bool:
        """ä¿å­˜é…ç½®åˆ°JSONæ–‡ä»¶"""
        try:
            config_dict = self.config.to_dict()
            
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config_dict, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {Path(config_path).absolute()}")
            return True
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜é…ç½®å¤±è´¥: {e}")
            return False

    def load_config(self, config_path: str = "iptv_config.json") -> Optional[AppConfig]:
        """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            if not Path(config_path).exists():
                logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
                return None
            
            with open(config_path, 'r', encoding='utf-8') as f:
                config_dict = json.load(f)
            
            # éªŒè¯å¿…éœ€çš„é…ç½®é¡¹
            required_keys = ['source_urls']
            for key in required_keys:
                if key not in config_dict:
                    logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘å¿…éœ€çš„é”®: {key}")
                    return None
            
            # åˆ›å»ºé…ç½®å¯¹è±¡
            config = AppConfig.from_dict(config_dict)
            
            logger.info(f"âœ… é…ç½®å·²ä»æ–‡ä»¶åŠ è½½: {config_path}")
            return config
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é…ç½®å¤±è´¥: {e}")
            return None

    def validate_config(self) -> bool:
        """éªŒè¯é…ç½®çš„å®Œæ•´æ€§"""
        try:
            config = self.config
            
            # éªŒè¯URLs
            if not config.source_urls:
                logger.error("âŒ é…ç½®é”™è¯¯: æ²¡æœ‰æºURL")
                return False
                
            for url in config.source_urls:
                if not self.validate_url(url):
                    logger.error(f"âŒ é…ç½®é”™è¯¯: æ— æ•ˆçš„æºURL - {url}")
                    return False
            
            # éªŒè¯æ•°å€¼å‚æ•°
            if config.request_timeout <= 0:
                logger.error("âŒ é…ç½®é”™è¯¯: è¯·æ±‚è¶…æ—¶å¿…é¡»å¤§äº0")
                return False
                
            if config.speed_test_timeout <= 0:
                logger.error("âŒ é…ç½®é”™è¯¯: æµ‹é€Ÿè¶…æ—¶å¿…é¡»å¤§äº0")
                return False
                
            if config.similarity_threshold < 0 or config.similarity_threshold > 100:
                logger.error("âŒ é…ç½®é”™è¯¯: ç›¸ä¼¼åº¦é˜ˆå€¼å¿…é¡»åœ¨0-100ä¹‹é—´")
                return False
                
            if config.max_sources_per_channel <= 0:
                logger.error("âŒ é…ç½®é”™è¯¯: æœ€å¤§æºæ•°å¿…é¡»å¤§äº0")
                return False
            
            if config.max_workers <= 0:
                logger.error("âŒ é…ç½®é”™è¯¯: å·¥ä½œçº¿ç¨‹æ•°å¿…é¡»å¤§äº0")
                return False
            
            # éªŒè¯æ–‡ä»¶è·¯å¾„
            template_path = Path(config.template_file)
            if not template_path.exists():
                logger.warning(f"âš ï¸ æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")
                # è¿™é‡Œä¸è¿”å›Falseï¼Œå› ä¸ºç¨‹åºä¼šåˆ›å»ºç¤ºä¾‹æ¨¡æ¿
            
            # éªŒè¯ç›®å½•æƒé™
            try:
                temp_dir = Path(config.temp_dir)
                temp_dir.mkdir(exist_ok=True)
                test_file = temp_dir / "test_write"
                test_file.touch()
                test_file.unlink()
            except Exception as e:
                logger.error(f"âŒ é…ç½®é”™è¯¯: æ— æ³•å†™å…¥ä¸´æ—¶ç›®å½• - {e}")
                return False
                
            logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {e}")
            return False

    def check_dependencies(self) -> bool:
        """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
        try:
            # æ£€æŸ¥åŸºç¡€ä¾èµ–
            import requests
            import pandas as pd
            logger.info("âœ… åŸºç¡€ä¾èµ–æ£€æŸ¥é€šè¿‡")
            
            # æ£€æŸ¥FFmpeg
            self.ffmpeg_available = self._check_ffmpeg()
            
            return True
            
        except ImportError as e:
            logger.error(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
            print("è¯·è¿è¡Œ: pip install requests pandas")
            return False

    def _check_ffmpeg(self) -> bool:
        """æ£€æŸ¥FFmpegæ˜¯å¦å¯ç”¨"""
        try:
            result = subprocess.run(
                ['ffmpeg', '-version'], 
                capture_output=True, 
                timeout=5,
                check=False
            )
            if result.returncode == 0:
                logger.info("âœ… FFmpegå¯ç”¨")
                return True
            else:
                logger.warning("âš ï¸ FFmpegæœªå®‰è£…æˆ–ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨HTTPæµ‹é€Ÿ")
                return False
        except (subprocess.SubprocessError, FileNotFoundError, subprocess.TimeoutExpired):
            logger.warning("âš ï¸ FFmpegæœªå®‰è£…ï¼Œå°†ä½¿ç”¨HTTPæµ‹é€Ÿ")
            return False

    def get_cache_key(self, url: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(url.encode('utf-8')).hexdigest()

    def get_cached_content(self, url: str) -> Optional[str]:
        """è·å–ç¼“å­˜å†…å®¹"""
        if not self.config.cache_enabled:
            return None
            
        cache_file = self.cache_dir / f"{self.get_cache_key(url)}.cache"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                cache_time = cache_data.get('timestamp', 0)
                if time.time() - cache_time < self.config.cache_expiry:
                    logger.debug(f"ä½¿ç”¨ç¼“å­˜: {url}")
                    return cache_data.get('content')
                else:
                    logger.debug(f"ç¼“å­˜å·²è¿‡æœŸ: {url}")
            except Exception as e:
                logger.debug(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
                
        return None

    def set_cached_content(self, url: str, content: str) -> None:
        """è®¾ç½®ç¼“å­˜å†…å®¹"""
        if not self.config.cache_enabled:
            return
            
        try:
            cache_file = self.cache_dir / f"{self.get_cache_key(url)}.cache"
            cache_data = {
                'timestamp': time.time(),
                'content': content,
                'url': url
            }
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False)
        except Exception as e:
            logger.debug(f"å†™å…¥ç¼“å­˜å¤±è´¥: {e}")

    def backup_data(self, stage: str, data: Any) -> bool:
        """å¤‡ä»½å¤„ç†é˜¶æ®µçš„æ•°æ®"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = self.backup_dir / f"backup_{stage}_{timestamp}.json"
            
            if isinstance(data, pd.DataFrame):
                # å¤‡ä»½DataFrame
                data.to_json(backup_file, orient='records', force_ascii=False, indent=2)
            elif isinstance(data, dict):
                # å¤‡ä»½å­—å…¸
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
            elif isinstance(data, str):
                # å¤‡ä»½å­—ç¬¦ä¸²
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump({'content': data}, f, ensure_ascii=False, indent=2)
            else:
                # å¤‡ä»½å…¶ä»–æ•°æ®ç±»å‹
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump({'data': str(data)}, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"âœ… æ•°æ®å¤‡ä»½å®Œæˆ: {stage} -> {backup_file}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¤‡ä»½å¤±è´¥: {e}")
            return False

    def save_checkpoint(self, stage: str, data: Any = None) -> bool:
        """ä¿å­˜å¤„ç†æ£€æŸ¥ç‚¹"""
        try:
            checkpoint_data = {
                'stage': stage,
                'timestamp': time.time(),
                'config': self.config.to_dict()
            }
            
            if data is not None:
                # ä¿å­˜å…³é”®æ•°æ®æ‘˜è¦
                if isinstance(data, pd.DataFrame):
                    checkpoint_data['data_summary'] = {
                        'rows': len(data),
                        'columns': list(data.columns),
                        'sample_channels': data['program_name'].head(5).tolist() if 'program_name' in data.columns else []
                    }
                elif isinstance(data, dict):
                    checkpoint_data['data_summary'] = {
                        'categories': len(data),
                        'total_channels': sum(len(channels) for channels in data.values()) if data else 0
                    }
            
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            
            self.current_stage = stage
            logger.info(f"âœ… æ£€æŸ¥ç‚¹ä¿å­˜: {stage}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")
            return False

    def can_resume_from_checkpoint(self) -> Tuple[bool, Optional[Dict]]:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥ä»æ£€æŸ¥ç‚¹æ¢å¤"""
        try:
            if not self.checkpoint_file.exists():
                return False, None
            
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
            
            # éªŒè¯æ£€æŸ¥ç‚¹æ•°æ®çš„å®Œæ•´æ€§
            required_keys = ['stage', 'timestamp']
            for key in required_keys:
                if key not in checkpoint_data:
                    return False, None
            
            # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…ï¼ˆ24å°æ—¶å†…ï¼‰
            if time.time() - checkpoint_data['timestamp'] > 86400:
                logger.warning("âš ï¸ æ£€æŸ¥ç‚¹å·²è¿‡æœŸï¼ˆè¶…è¿‡24å°æ—¶ï¼‰")
                return False, None
            
            return True, checkpoint_data
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ç‚¹è¯»å–å¤±è´¥: {e}")
            return False, None

    def simple_progress_bar(self, iterable, desc: str = "Processing", total: Optional[int] = None):
        """å®Œæ•´çš„ç®€å•è¿›åº¦æ¡å®ç°"""
        if iterable is None:
            logger.error(f"{desc}: iterable ä¸º None")
            return
            
        try:
            # å°è¯•è·å–æ€»æ•°
            if total is None:
                try:
                    total = len(iterable)
                except (TypeError, AttributeError):
                    total = 0
                    logger.warning(f"æ— æ³•ç¡®å®š {desc} çš„æ€»æ•°")
            
            processed = 0
            start_time = time.time()
            
            for item in iterable:
                if item is None:
                    continue
                    
                yield item
                processed += 1
                
                # è®¡ç®—è¿›åº¦å’Œé¢„è®¡æ—¶é—´
                if total > 0:
                    percent = min(100, (processed / total) * 100)
                    elapsed = time.time() - start_time
                    if processed > 0:
                        eta = (elapsed / processed) * (total - processed)
                        eta_str = f"ETA: {eta:.1f}s"
                    else:
                        eta_str = "è®¡ç®—ä¸­..."
                    
                    bar_length = 50
                    filled_length = int(bar_length * percent / 100)
                    bar = 'â–ˆ' * filled_length + ' ' * (bar_length - filled_length)
                    display_text = f"\r{desc}: [{bar}] {percent:.1f}% ({processed}/{total}) {eta_str}"
                else:
                    display_text = f"\r{desc}: å·²å¤„ç† {processed} é¡¹"
                
                print(display_text, end="", flush=True)
                
            print()  # å®Œæˆåæ¢è¡Œ
            
        except Exception as e:
            logger.error(f"è¿›åº¦æ¡é”™è¯¯: {e}")
            # é™çº§å¤„ç†ï¼šç›´æ¥è¿”å›è¿­ä»£å™¨
            for item in iterable:
                yield item

    def validate_url(self, url: str) -> bool:
        """éªŒè¯URLæ ¼å¼æ˜¯å¦æ­£ç¡®"""
        if not url or not isinstance(url, str):
            return False
            
        try:
            result = urlparse(url)
            valid_scheme = result.scheme in ['http', 'https']
            valid_netloc = bool(result.netloc)
            valid_domain = '.' in result.netloc or 'localhost' in result.netloc or '[' in result.netloc
            
            return all([valid_scheme, valid_netloc, valid_domain])
        except Exception as e:
            logger.debug(f"URLè§£æå¤±è´¥: {url} - {e}")
            return False

    def fetch_streams_from_url(self, url: str) -> Optional[str]:
        """ä»URLè·å–æµæ•°æ®"""
        if not self.validate_url(url):
            logger.error(f"âŒ æ— æ•ˆçš„URL: {url}")
            return None
            
        # æ£€æŸ¥ç¼“å­˜
        cached_content = self.get_cached_content(url)
        if cached_content:
            return cached_content
            
        logger.info(f"ğŸ“¡ æ­£åœ¨çˆ¬å–æº: {url}")
        try:
            response = self.session.get(url, timeout=self.config.request_timeout)
            response.encoding = 'utf-8'
            
            if response.status_code == 200:
                content = response.text
                content_length = len(content)
                logger.info(f"âœ… æˆåŠŸè·å–æ•°æ®: {url} ({content_length} å­—ç¬¦)")
                
                # ç¼“å­˜å†…å®¹
                self.set_cached_content(url, content)
                return content
            else:
                logger.error(f"âŒ è·å–æ•°æ®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code} - {url}")
                
        except requests.exceptions.Timeout:
            logger.error(f"âŒ è¯·æ±‚è¶…æ—¶: {url}")
        except requests.exceptions.ConnectionError:
            logger.error(f"âŒ è¿æ¥é”™è¯¯: {url}")
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ è¯·æ±‚é”™è¯¯: {e} - {url}")
        except Exception as e:
            logger.error(f"âŒ æœªçŸ¥é”™è¯¯: {e} - {url}")
            
        return None

    def fetch_all_streams(self) -> str:
        """è·å–æ‰€æœ‰æºçš„æµæ•°æ®ï¼ˆä½¿ç”¨è¿›åº¦æ¡ï¼‰"""
        logger.info("ğŸš€ å¼€å§‹æ™ºèƒ½å¤šæºæŠ“å–...")
        
        if not self.config.source_urls:
            logger.error("âŒ æ²¡æœ‰é…ç½®æºURL")
            return ""
        
        all_streams = []
        successful_sources = 0
        
        def process_future(future, url: str) -> bool:
            """å¤„ç†å•ä¸ªfutureç»“æœ"""
            nonlocal successful_sources
            try:
                content = future.result()
                if content:
                    all_streams.append(content)
                    successful_sources += 1
                    return True
            except Exception as e:
                logger.error(f"å¤„ç† {url} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
        
        if TQDM_AVAILABLE:
            with tqdm(total=len(self.config.source_urls), desc="ğŸŒ æŠ“å–æºæ•°æ®", unit="source") as pbar:
                with ThreadPoolExecutor(max_workers=min(5, len(self.config.source_urls))) as executor:
                    # åˆ›å»ºfutureåˆ°URLçš„æ˜ å°„
                    future_to_url = {}
                    for url in self.config.source_urls:
                        future = executor.submit(self.fetch_streams_from_url, url)
                        future_to_url[future] = url
                    
                    for future in as_completed(future_to_url):
                        url = future_to_url[future]
                        process_future(future, url)
                        pbar.set_postfix({
                            "æˆåŠŸ": successful_sources, 
                            "å½“å‰æº": url[:30] + "..." if len(url) > 30 else url
                        })
                        pbar.update(1)
        else:
            print(f"æ€»å…± {len(self.config.source_urls)} ä¸ªæºéœ€è¦æŠ“å–")
            with ThreadPoolExecutor(max_workers=min(5, len(self.config.source_urls))) as executor:
                # åˆ›å»ºfutureåˆ—è¡¨å’Œæ˜ å°„
                futures = []
                future_url_map = {}
                
                for url in self.config.source_urls:
                    future = executor.submit(self.fetch_streams_from_url, url)
                    futures.append(future)
                    future_url_map[future] = url
                
                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                for future in self.simple_progress_bar(as_completed(futures), "æŠ“å–è¿›åº¦", len(futures)):
                    url = future_url_map.get(future, "æœªçŸ¥URL")
                    process_future(future, url)
    
        logger.info(f"âœ… æˆåŠŸè·å– {successful_sources}/{len(self.config.source_urls)} ä¸ªæºçš„æ•°æ®")
        return "\n".join(all_streams) if all_streams else ""

    def _extract_program_name(self, extinf_line: str) -> str:
        """å®Œæ•´çš„EXTINFè¡Œè§£æ"""
        if not extinf_line.startswith('#EXTINF'):
            return "æœªçŸ¥é¢‘é“"
        
        try:
            # æ–¹æ³•1: ä»tvg-nameå±æ€§æå–ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            tvg_match = self.patterns['tvg_name'].search(extinf_line)
            if tvg_match and tvg_match.group(1).strip():
                name = tvg_match.group(1).strip()
                if name and name != "æœªçŸ¥é¢‘é“":
                    return name
            
            # æ–¹æ³•2: ä»é€—å·åçš„å†…å®¹æå–
            content_match = self.patterns['extinf_content'].search(extinf_line)
            if content_match and content_match.group(1).strip():
                name = content_match.group(1).strip()
                # æ¸…ç†å¯èƒ½çš„é¢å¤–ä¿¡æ¯
                name = re.sub(r'\[.*?\]|\(.*?\)', '', name).strip()
                if name and name != "æœªçŸ¥é¢‘é“":
                    return name
            
            # æ–¹æ³•3: å°è¯•å…¶ä»–å±æ€§
            for attr_pattern in [self.patterns['tvg_id'], self.patterns['group_title']]:
                attr_match = attr_pattern.search(extinf_line)
                if attr_match and attr_match.group(1).strip():
                    name = attr_match.group(1).strip()
                    if name and name != "æœªçŸ¥é¢‘é“":
                        return name
                        
        except Exception as e:
            logger.debug(f"EXTINFè§£æé”™è¯¯: {extinf_line} - {e}")
        
        return "æœªçŸ¥é¢‘é“"

    def parse_m3u(self, content: str) -> List[Dict[str, str]]:
        """è§£æM3Uæ ¼å¼å†…å®¹"""
        if not content or not isinstance(content, str):
            return []
            
        streams = []
        lines = content.splitlines()
        current_program = None
        current_group = "é»˜è®¤åˆ†ç»„"
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            if not line:
                i += 1
                continue
                
            if line.startswith("#EXTINF"):
                # æå–é¢‘é“ä¿¡æ¯
                current_program = self._extract_program_name(line)
                
                # æå–åˆ†ç»„ä¿¡æ¯
                group_match = self.patterns['group_title'].search(line)
                if group_match:
                    current_group = group_match.group(1).strip()
                else:
                    current_group = "é»˜è®¤åˆ†ç»„"
                    
                # æŸ¥æ‰¾ä¸‹ä¸€è¡Œçš„URL
                if i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    if self.validate_url(next_line):
                        streams.append({
                            "program_name": current_program,
                            "stream_url": next_line,
                            "group": current_group,
                            "original_name": current_program
                        })
                        i += 1  # è·³è¿‡URLè¡Œ
            elif line.startswith(('http://', 'https://')):
                # ç‹¬ç«‹çš„URLè¡Œï¼ˆæ²¡æœ‰EXTINFä¿¡æ¯ï¼‰
                if self.validate_url(line):
                    streams.append({
                        "program_name": "æœªçŸ¥é¢‘é“",
                        "stream_url": line,
                        "group": "é»˜è®¤åˆ†ç»„",
                        "original_name": "æœªçŸ¥é¢‘é“"
                    })
            
            i += 1
        
        return streams

    def parse_txt(self, content: str) -> List[Dict[str, str]]:
        """è§£æTXTæ ¼å¼å†…å®¹"""
        if not content or not isinstance(content, str):
            return []
            
        streams = []
        
        for line in content.splitlines():
            line = line.strip()
            if not line or line.startswith('#') or '#genre#' in line:
                continue
            
            # æ”¯æŒå¤šç§åˆ†éš”ç¬¦æ ¼å¼
            if ',' in line:
                parts = line.split(',', 1)
                if len(parts) == 2:
                    program_name = parts[0].strip()
                    url_part = parts[1].strip()
                    
                    # ä»ç¬¬äºŒéƒ¨åˆ†æå–URL
                    url_match = self.patterns['url'].search(url_part)
                    if url_match:
                        stream_url = url_match.group()
                        if self.validate_url(stream_url):
                            streams.append({
                                "program_name": program_name,
                                "stream_url": stream_url,
                                "group": "é»˜è®¤åˆ†ç»„",
                                "original_name": program_name
                            })
            else:
                # å°è¯•ä»æ•´è¡Œæå–URL
                url_match = self.patterns['url'].search(line)
                if url_match:
                    stream_url = url_match.group()
                    program_name = line.replace(stream_url, '').strip()
                    if not program_name:
                        program_name = "æœªçŸ¥é¢‘é“"
                    
                    if self.validate_url(stream_url):
                        streams.append({
                            "program_name": program_name,
                            "stream_url": stream_url,
                            "group": "é»˜è®¤åˆ†ç»„",
                            "original_name": program_name
                        })
        
        return streams

    def organize_streams(self, content: str) -> pd.DataFrame:
        """æ•´ç†æµæ•°æ®ï¼Œå»é™¤é‡å¤å’Œæ— æ•ˆæ•°æ®"""
        if not content:
            logger.error("âŒ æ²¡æœ‰å†…å®¹å¯å¤„ç†")
            return pd.DataFrame()
            
        logger.info("ğŸ” è§£ææµæ•°æ®...")
        
        try:
            # è‡ªåŠ¨æ£€æµ‹æ ¼å¼å¹¶è§£æ
            if content.startswith("#EXTM3U"):
                streams = self.parse_m3u(content)
            else:
                streams = self.parse_txt(content)
            
            if not streams:
                logger.error("âŒ æœªèƒ½è§£æå‡ºä»»ä½•æµæ•°æ®")
                return pd.DataFrame()
                
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(streams)
            
            # æ•°æ®æ¸…ç†
            initial_count = len(df)
            
            # ç§»é™¤ç©ºå€¼
            df = df.dropna()
            
            # è¿‡æ»¤æ— æ•ˆçš„èŠ‚ç›®åç§°å’ŒURL
            df = df[df['program_name'].str.len() > 0]
            df = df[df['stream_url'].str.startswith(('http://', 'https://'))]
            
            # åº”ç”¨URLéªŒè¯
            df['url_valid'] = df['stream_url'].apply(self.validate_url)
            df = df[df['url_valid']].drop('url_valid', axis=1)
            
            # ç¡®ä¿original_nameåˆ—å­˜åœ¨
            if 'original_name' not in df.columns:
                df['original_name'] = df['program_name']
            
            # å»é‡ï¼ˆåŸºäºèŠ‚ç›®åç§°å’ŒURLï¼‰
            df = df.drop_duplicates(subset=['program_name', 'stream_url'])
            
            final_count = len(df)
            logger.info(f"ğŸ“Š æ•°æ®æ¸…ç†: {initial_count} -> {final_count} ä¸ªæµ")
            
            if final_count == 0:
                logger.warning("âš ï¸ æ•°æ®æ¸…ç†åæ²¡æœ‰æœ‰æ•ˆçš„æµæ•°æ®")
                
            return df
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¤„ç†é”™è¯¯: {e}")
            return pd.DataFrame()

    def load_template(self) -> Optional[Dict[str, List[str]]]:
        """åŠ è½½é¢‘é“æ¨¡æ¿æ–‡ä»¶"""
        template_file = Path(self.config.template_file)
        
        if not template_file.exists():
            logger.error(f"âŒ æ¨¡æ¿æ–‡ä»¶ {template_file} ä¸å­˜åœ¨")
            return None
            
        logger.info(f"ğŸ“‹ åŠ è½½æ¨¡æ¿æ–‡ä»¶: {template_file}")
        categories = {}
        current_category = None
        
        try:
            with open(template_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                        
                    # æ£€æµ‹åˆ†ç±»è¡Œ
                    category_match = self.patterns['category'].match(line)
                    if category_match:
                        current_category = category_match.group(1).strip()
                        categories[current_category] = []
                        logger.debug(f"æ‰¾åˆ°åˆ†ç±»: {current_category}")
                    
                    # æ£€æµ‹é¢‘é“è¡Œ
                    elif current_category and line and not line.startswith('#'):
                        # æå–é¢‘é“åç§°ï¼ˆå»é™¤å¯èƒ½çš„åˆ†éš”ç¬¦å’Œæ³¨é‡Šï¼‰
                        channel_name = line.split(',')[0].strip() if ',' in line else line.strip()
                        if channel_name:
                            categories[current_category].append(channel_name)
                            logger.debug(f"æ·»åŠ é¢‘é“: {channel_name} -> {current_category}")
        
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
            return None
        
        if not categories:
            logger.error("âŒ æ¨¡æ¿æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„é¢‘é“åˆ†ç±»")
            return None
            
        # ç»Ÿè®¡ä¿¡æ¯
        total_channels = sum(len(channels) for channels in categories.values())
        logger.info(f"ğŸ“ æ¨¡æ¿åˆ†ç±»: {list(categories.keys())}")
        logger.info(f"ğŸ“º æ¨¡æ¿é¢‘é“æ€»æ•°: {total_channels}")
        
        return categories

    def clean_channel_name(self, name: str) -> str:
        """æ”¹è¿›çš„é¢‘é“åç§°æ¸…ç†"""
        if not name:
            return ""
        
        try:
            # ä¿ç•™å…³é”®ä¿¡æ¯ï¼šä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ç©ºæ ¼ã€æ¨ªæ 
            cleaned = re.sub(r'[^\w\u4e00-\u9fa5\s-]', '', name.lower())
            # åˆå¹¶å¤šä¸ªç©ºæ ¼
            cleaned = re.sub(r'\s+', ' ', cleaned).strip()
            # ç§»é™¤å¸¸è§çš„æ— æ„ä¹‰åç¼€
            cleaned = re.sub(r'\s+(hd|fhd|4k|ç›´æ’­|é¢‘é“|tv|television)$', '', cleaned)
            return cleaned
        except Exception as e:
            logger.debug(f"é¢‘é“åç§°æ¸…ç†é”™è¯¯: {name} - {e}")
            return name.lower() if name else ""

    def similarity_score(self, str1: str, str2: str) -> int:
        """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-100ï¼‰"""
        if not str1 or not str2 or not isinstance(str1, str) or not isinstance(str2, str):
            return 0
            
        try:
            # æ¸…ç†å­—ç¬¦ä¸²
            clean_str1 = self.clean_channel_name(str1)
            clean_str2 = self.clean_channel_name(str2)
            
            if not clean_str1 or not clean_str2:
                return 0
            
            # å®Œå…¨åŒ¹é…
            if clean_str1 == clean_str2:
                return 100
            
            # åŒ…å«å…³ç³»ï¼ˆåŒå‘ï¼‰
            if clean_str1 in clean_str2:
                return 90
            if clean_str2 in clean_str1:
                return 85
            
            # ä½¿ç”¨é›†åˆè®¡ç®—Jaccardç›¸ä¼¼åº¦
            set1 = set(clean_str1)
            set2 = set(clean_str2)
            
            intersection = len(set1 & set2)
            union = len(set1 | set2)
            
            if union > 0:
                jaccard_similarity = intersection / union
                return int(jaccard_similarity * 80)
                
        except Exception as e:
            logger.debug(f"ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {str1}, {str2} - {e}")
        
        return 0

    def filter_and_sort_sources(self, sources_df: pd.DataFrame, template_channels: List[str]) -> pd.DataFrame:
        """å®Œæ•´çš„é¢‘é“åŒ¹é…å’Œæºç­›é€‰å®ç°"""
        logger.info("ğŸ¯ å¼€å§‹é¢‘é“åŒ¹é…å’Œæºç­›é€‰...")
        
        # ä¸¥æ ¼çš„ç©ºå€¼æ£€æŸ¥
        if sources_df is None or sources_df.empty:
            logger.error("âŒ æºæ•°æ®ä¸ºç©ºæˆ–Noneï¼Œæ— æ³•è¿›è¡ŒåŒ¹é…")
            return pd.DataFrame()
        
        if template_channels is None or not template_channels:
            logger.error("âŒ æ¨¡æ¿é¢‘é“åˆ—è¡¨ä¸ºç©ºæˆ–None")
            return pd.DataFrame()
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['program_name', 'stream_url']
        missing_columns = [col for col in required_columns if col not in sources_df.columns]
        if missing_columns:
            logger.error(f"âŒ æºæ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            return pd.DataFrame()
        
        # åˆ›å»ºåŒ¹é…ç»“æœåˆ—è¡¨
        matched_results = []
        
        logger.info(f"å¼€å§‹åŒ¹é… {len(template_channels)} ä¸ªæ¨¡æ¿é¢‘é“...")
        
        if TQDM_AVAILABLE:
            with tqdm(total=len(template_channels), desc="ğŸ” é¢‘é“åŒ¹é…", unit="channel") as pbar:
                for template_channel in template_channels:
                    best_match_row = None
                    best_score = 0
                    
                    # ä¸ºæ¯ä¸ªæ¨¡æ¿é¢‘é“æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„æº
                    for _, source_row in sources_df.iterrows():
                        source_channel = source_row['program_name']
                        score = self.similarity_score(template_channel, source_channel)
                        
                        if score > best_score and score >= self.config.similarity_threshold:
                            best_score = score
                            best_match_row = source_row.copy()
                            best_match_row['template_channel'] = template_channel
                            best_match_row['match_score'] = score
                    
                    if best_match_row is not None:
                        matched_results.append(best_match_row)
                        pbar.set_postfix({
                            "åŒ¹é…åº¦": f"{best_score}%", 
                            "é¢‘é“": template_channel[:20]
                        })
                    
                    pbar.update(1)
        else:
            for template_channel in self.simple_progress_bar(template_channels, "é¢‘é“åŒ¹é…"):
                best_match_row = None
                best_score = 0
                
                for _, source_row in sources_df.iterrows():
                    source_channel = source_row['program_name']
                    score = self.similarity_score(template_channel, source_channel)
                    
                    if score > best_score and score >= self.config.similarity_threshold:
                        best_score = score
                        best_match_row = source_row.copy()
                        best_match_row['template_channel'] = template_channel
                        best_match_row['match_score'] = score
                
                if best_match_row is not None:
                    matched_results.append(best_match_row)
        
        # è½¬æ¢ä¸ºDataFrameå¹¶æ•´åˆæ•°æ®
        if matched_results:
            result_df = pd.DataFrame(matched_results)
            
            # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
            required_columns = ['program_name', 'stream_url', 'template_channel', 'match_score']
            for col in required_columns:
                if col not in result_df.columns:
                    logger.warning(f"ç¼ºå¤±åˆ—: {col}ï¼Œä½¿ç”¨é»˜è®¤å€¼å¡«å……")
                    if col == 'template_channel':
                        result_df[col] = result_df.get('program_name', 'æœªçŸ¥é¢‘é“')
                    elif col == 'match_score':
                        result_df[col] = 0
            
            # é‡å‘½ååˆ—ä»¥æ˜ç¡®å«ä¹‰
            column_mapping = {
                'program_name': 'original_name',
                'template_channel': 'program_name'
            }
            result_df = result_df.rename(columns=column_mapping)
            
            # ç¡®ä¿original_nameåˆ—å­˜åœ¨
            if 'original_name' not in result_df.columns:
                result_df['original_name'] = result_df.get('program_name', 'æœªçŸ¥é¢‘é“')
            
            # é‡æ–°æ’åˆ—åˆ—é¡ºåº
            preferred_order = ['program_name', 'original_name', 'stream_url', 'match_score', 'group']
            available_columns = [col for col in preferred_order if col in result_df.columns]
            other_columns = [col for col in result_df.columns if col not in preferred_order]
            result_df = result_df[available_columns + other_columns]
            
            # æ˜¾ç¤ºåŒ¹é…ç»“æœç»Ÿè®¡
            unique_matched_channels = result_df['program_name'].nunique()
            logger.info(f"âœ… é¢‘é“åŒ¹é…å®Œæˆ: {len(matched_results)} ä¸ªæµåŒ¹é…åˆ° {unique_matched_channels} ä¸ªæ¨¡æ¿é¢‘é“")
            
            # æ˜¾ç¤ºæœ€ä½³åŒ¹é…ç»“æœ
            if not result_df.empty:
                top_matches = result_df.nlargest(10, 'match_score')[['program_name', 'original_name', 'match_score']]
                print("\nğŸ“Š æœ€ä½³åŒ¹é…ç»“æœï¼ˆå‰10ä¸ªï¼‰:")
                for _, match in top_matches.iterrows():
                    print(f"  âœ… {match['program_name']:<20} <- {match['original_name']:<30} (åˆ†æ•°: {match['match_score']:2d})")
            
            return result_df
        else:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çš„é¢‘é“")
            return pd.DataFrame()

    def speed_test_ffmpeg(self, stream_url: str) -> Tuple[bool, float]:
        """ä½¿ç”¨FFmpegè¿›è¡Œæµåª’ä½“æµ‹é€Ÿï¼ˆæ›´å‡†ç¡®ä½†è¾ƒæ…¢ï¼‰"""
        if not self.ffmpeg_available or not stream_url:
            return False, float('inf')
            
        temp_file = Path(self.config.temp_dir) / f'test_{abs(hash(stream_url))}.ts'
        
        try:
            cmd = [
                'ffmpeg',
                '-y',  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                '-timeout', '3000000',  # 3ç§’è¶…æ—¶ï¼ˆå¾®ç§’ï¼‰
                '-i', stream_url,
                '-t', '2',  # åªæµ‹è¯•2ç§’
                '-c', 'copy',
                '-f', 'mpegts',
                '-max_muxing_queue_size', '1024',
                str(temp_file)
            ]
            
            start_time = time.time()
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                timeout=5,
                check=False
            )
            end_time = time.time()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file.exists():
                try:
                    temp_file.unlink()
                except:
                    pass
            
            if result.returncode == 0:
                speed = end_time - start_time
                return True, speed
            else:
                return False, float('inf')
                
        except (subprocess.TimeoutExpired, Exception) as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file.exists():
                try:
                    temp_file.unlink()
                except:
                    pass
            logger.debug(f"FFmpegæµ‹é€Ÿå¤±è´¥: {stream_url} - {e}")
            return False, float('inf')

    def speed_test_simple(self, stream_url: str) -> Tuple[bool, float]:
        """ç®€å•çš„HTTPæµ‹é€Ÿï¼ˆå¿«é€Ÿä½†ä¸å¤Ÿå‡†ç¡®ï¼‰"""
        if not stream_url:
            return False, float('inf')
            
        try:
            start_time = time.time()
            response = self.session.head(
                stream_url, 
                timeout=self.config.speed_test_timeout,
                allow_redirects=True
            )
            end_time = time.time()
            
            if response.status_code in [200, 302, 301, 307]:
                return True, end_time - start_time
            else:
                return False, float('inf')
        except Exception as e:
            logger.debug(f"HTTPæµ‹é€Ÿå¤±è´¥: {stream_url} - {e}")
            return False, float('inf')

    def speed_test_sources(self, sources_df: pd.DataFrame) -> pd.DataFrame:
        """å®Œæ•´çš„æµ‹é€Ÿå®ç°"""
        logger.info("â±ï¸  å¼€å§‹æ™ºèƒ½æµ‹é€Ÿ...")
        
        if sources_df is None or sources_df.empty:
            logger.error("âŒ æ²¡æœ‰éœ€è¦æµ‹é€Ÿçš„æº")
            return pd.DataFrame()
            
        results = []
        total_sources = len(sources_df)
        
        def test_single_source(row) -> Dict[str, Any]:
            """æµ‹è¯•å•ä¸ªæºçš„è¾…åŠ©å‡½æ•°"""
            try:
                program_name = row['program_name']
                stream_url = row['stream_url']
                
                # æ ¹æ®URLç±»å‹é€‰æ‹©æµ‹é€Ÿæ–¹æ³•
                if any(ext in stream_url.lower() for ext in ['.m3u8', '.ts', '.flv', '.mp4']):
                    if self.ffmpeg_available:
                        accessible, speed = self.speed_test_ffmpeg(stream_url)
                    else:
                        accessible, speed = self.speed_test_simple(stream_url)
                else:
                    accessible, speed = self.speed_test_simple(stream_url)
                
                return {
                    'program_name': program_name,
                    'stream_url': stream_url,
                    'accessible': accessible,
                    'speed': speed,
                    'original_name': row.get('original_name', ''),
                    'match_score': row.get('match_score', 0)
                }
            except Exception as e:
                logger.error(f"æµ‹é€Ÿå•ä¸ªæºæ—¶å‡ºé”™: {e}")
                return {
                    'program_name': row.get('program_name', 'æœªçŸ¥'),
                    'stream_url': row.get('stream_url', ''),
                    'accessible': False,
                    'speed': float('inf'),
                    'original_name': row.get('original_name', ''),
                    'match_score': row.get('match_score', 0)
                }
        
        if TQDM_AVAILABLE:
            with tqdm(total=total_sources, desc="âš¡ æµ‹é€Ÿè¿›åº¦", unit="source") as pbar:
                with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
                    futures = [executor.submit(test_single_source, row) for _, row in sources_df.iterrows()]
                    
                    for future in as_completed(futures):
                        try:
                            result = future.result(timeout=15)
                            results.append(result)
                            status = "âœ…" if result['accessible'] else "âŒ"
                            pbar.set_postfix({
                                "çŠ¶æ€": status, 
                                "é€Ÿåº¦": f"{result['speed']:.2f}s" if result['accessible'] else "è¶…æ—¶",
                                "é¢‘é“": result['program_name'][:15] + "..." if len(result['program_name']) > 15 else result['program_name']
                            })
                            pbar.update(1)
                        except Exception as e:
                            logger.error(f"æµ‹é€Ÿå¼‚å¸¸: {e}")
                            pbar.update(1)
        else:
            print(f"æ€»å…± {total_sources} ä¸ªæºéœ€è¦æµ‹é€Ÿ")
            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
                futures = [executor.submit(test_single_source, row) for _, row in sources_df.iterrows()]
                
                for future in self.simple_progress_bar(as_completed(futures), "æµ‹é€Ÿè¿›åº¦", total_sources):
                    try:
                        result = future.result(timeout=15)
                        results.append(result)
                    except Exception as e:
                        logger.error(f"æµ‹é€Ÿå¼‚å¸¸: {e}")
        
        # è½¬æ¢ä¸ºDataFrameå¹¶æ•´åˆç»“æœ
        try:
            result_df = pd.DataFrame(results)
            if result_df.empty:
                return pd.DataFrame()
            
            # è¿‡æ»¤å¯è®¿é—®çš„æºå¹¶æŒ‰é€Ÿåº¦æ’åº
            accessible_df = result_df[result_df['accessible']].copy()
            accessible_df = accessible_df.sort_values(['program_name', 'speed'])
            
            accessible_count = len(accessible_df)
            logger.info(f"ğŸ“Š æµ‹é€Ÿå®Œæˆ: {accessible_count}/{total_sources} ä¸ªæºå¯ç”¨")
            
            if accessible_count == 0:
                logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„æºé€šè¿‡æµ‹é€Ÿ")
                
            return accessible_df
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æµ‹é€Ÿç»“æœæ—¶å‡ºé”™: {e}")
            return pd.DataFrame()

    def generate_final_data(self, speed_tested_df: pd.DataFrame, template_categories: Dict[str, List[str]]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæ•°æ®ï¼ˆä½¿ç”¨è¿›åº¦æ¡ï¼‰"""
        logger.info("ğŸ¨ ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶...")
        
        final_data = {}
        total_sources = 0
        
        if speed_tested_df is None or speed_tested_df.empty:
            logger.error("âŒ æµ‹é€Ÿæ•°æ®ä¸ºç©º")
            return final_data
        
        if not template_categories:
            logger.error("âŒ æ¨¡æ¿åˆ†ç±»ä¸ºç©º")
            return final_data
        
        # è®¡ç®—æ€»é¢‘é“æ•°
        total_channels = sum(len(channels) for channels in template_categories.values())
        
        if total_channels == 0:
            logger.error("âŒ æ¨¡æ¿ä¸­æ²¡æœ‰é¢‘é“")
            return final_data
        
        logger.info(f"ä¸º {len(template_categories)} ä¸ªåˆ†ç±»ç”Ÿæˆæœ€ç»ˆæ•°æ®...")
        
        if TQDM_AVAILABLE:
            with tqdm(total=total_channels, desc="ğŸ“¦ ç”Ÿæˆæ•°æ®", unit="channel") as pbar:
                for category, channels in template_categories.items():
                    final_data[category] = {}
                    
                    for channel in channels:
                        # è·å–è¯¥é¢‘é“çš„æ‰€æœ‰æº
                        channel_sources = speed_tested_df[speed_tested_df['program_name'] == channel]
                        
                        if not channel_sources.empty:
                            # æŒ‰é€Ÿåº¦æ’åºå¹¶å–å‰Nä¸ª
                            sorted_sources = channel_sources.head(self.config.max_sources_per_channel)
                            final_data[category][channel] = sorted_sources[['stream_url', 'speed']].to_dict('records')
                            source_count = len(sorted_sources)
                            total_sources += source_count
                            pbar.set_postfix({
                                "åˆ†ç±»": category[:10],
                                "é¢‘é“": channel[:15] + "..." if len(channel) > 15 else channel,
                                "æºæ•°": source_count
                            })
                        else:
                            final_data[category][channel] = []
                            pbar.set_postfix({
                                "åˆ†ç±»": category[:10],
                                "é¢‘é“": channel[:15] + "..." if len(channel) > 15 else channel,
                                "æºæ•°": 0
                            })
                        
                        pbar.update(1)
        else:
            current_channel = 0
            for category, channels in template_categories.items():
                final_data[category] = {}
                
                for channel in channels:
                    channel_sources = speed_tested_df[speed_tested_df['program_name'] == channel]
                    
                    if not channel_sources.empty:
                        sorted_sources = channel_sources.head(self.config.max_sources_per_channel)
                        final_data[category][channel] = sorted_sources[['stream_url', 'speed']].to_dict('records')
                        source_count = len(sorted_sources)
                        total_sources += source_count
                    else:
                        final_data[category][channel] = []
                    
                    current_channel += 1
                    percent = min(100, (current_channel / total_channels) * 100)
                    bar_length = 50
                    filled_length = int(bar_length * percent / 100)
                    bar = 'â–ˆ' * filled_length + ' ' * (bar_length - filled_length)
                    print(f"\rç”Ÿæˆæ•°æ®: [{bar}] {percent:.1f}% ({current_channel}/{total_channels})", end="", flush=True)
            print()
        
        logger.info(f"ğŸ“¦ æ€»å…±æ”¶é›†åˆ° {total_sources} ä¸ªæœ‰æ•ˆæº")
        return final_data

    def save_output_files(self, final_data: Dict[str, Any]) -> bool:
        """ä¿å­˜è¾“å‡ºæ–‡ä»¶ï¼ˆä½¿ç”¨è¿›åº¦æ¡ï¼‰"""
        logger.info("ğŸ’¾ ä¿å­˜æ–‡ä»¶...")
        
        if not final_data:
            logger.error("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return False
        
        # è®¡ç®—æ€»è¡Œæ•°ç”¨äºè¿›åº¦æ¡
        total_lines = 0
        for category, channels in final_data.items():
            total_lines += 1  # åˆ†ç±»è¡Œ
            for channel, sources in channels.items():
                total_lines += len(sources)  # é¢‘é“è¡Œ
            total_lines += 1  # ç©ºè¡Œ
        
        success_count = 0
        
        # ä¿å­˜TXTæ ¼å¼
        try:
            if TQDM_AVAILABLE:
                with tqdm(total=total_lines, desc="ğŸ“„ ä¿å­˜TXT", unit="line") as pbar:
                    with open(self.config.output_txt, 'w', encoding='utf-8') as f:
                        for category, channels in final_data.items():
                            f.write(f"{category},#genre#\n")
                            pbar.update(1)
                            
                            for channel, sources in channels.items():
                                for source in sources:
                                    f.write(f"{channel},{source['stream_url']}\n")
                                    pbar.update(1)
                            
                            f.write("\n")
                            pbar.update(1)
            else:
                print("ä¿å­˜TXTæ–‡ä»¶...")
                with open(self.config.output_txt, 'w', encoding='utf-8') as f:
                    for category, channels in final_data.items():
                        f.write(f"{category},#genre#\n")
                        for channel, sources in channels.items():
                            for source in sources:
                                f.write(f"{channel},{source['stream_url']}\n")
                        f.write("\n")
            
            success_count += 1
            logger.info(f"âœ… TXTæ–‡ä»¶å·²ä¿å­˜: {Path(self.config.output_txt).absolute()}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜TXTæ–‡ä»¶å¤±è´¥: {e}")
            return False
        
        # ä¿å­˜M3Uæ ¼å¼
        try:
            if TQDM_AVAILABLE:
                with tqdm(total=total_lines, desc="ğŸ“„ ä¿å­˜M3U", unit="line") as pbar:
                    with open(self.config.output_m3u, 'w', encoding='utf-8') as f:
                        f.write("#EXTM3U\n")
                        pbar.update(1)
                        
                        for category, channels in final_data.items():
                            for channel, sources in channels.items():
                                for source in sources:
                                    f.write(f'#EXTINF:-1 tvg-name="{channel}" group-title="{category}",{channel}\n')
                                    f.write(f"{source['stream_url']}\n")
                                    pbar.update(2)
            else:
                print("ä¿å­˜M3Uæ–‡ä»¶...")
                with open(self.config.output_m3u, 'w', encoding='utf-8') as f:
                    f.write("#EXTM3U\n")
                    for category, channels in final_data.items():
                        for channel, sources in channels.items():
                            for source in sources:
                                f.write(f'#EXTINF:-1 tvg-name="{channel}" group-title="{category}",{channel}\n')
                                f.write(f"{source['stream_url']}\n")
            
            success_count += 1
            logger.info(f"âœ… M3Uæ–‡ä»¶å·²ä¿å­˜: {Path(self.config.output_m3u).absolute()}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜M3Uæ–‡ä»¶å¤±è´¥: {e}")
            return False
            
        return success_count == 2  # ä¸¤ä¸ªæ–‡ä»¶éƒ½ä¿å­˜æˆåŠŸ

    def validate_output_files(self) -> Dict[str, Any]:
        """éªŒè¯è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´æ€§å’Œæ ¼å¼"""
        validation_result = {
            'txt_file': {'exists': False, 'categories': 0, 'sources': 0, 'valid': False},
            'm3u_file': {'exists': False, 'channels': 0, 'sources': 0, 'valid': False},
            'overall_valid': False
        }
        
        try:
            # éªŒè¯TXTæ–‡ä»¶
            txt_path = Path(self.config.output_txt)
            if txt_path.exists():
                validation_result['txt_file']['exists'] = True
                with open(txt_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # ç»Ÿè®¡é¢‘é“å’Œæºæ•°é‡
                    lines = content.strip().split('\n')
                    categories = [line for line in lines if line.endswith(',#genre#')]
                    sources = [line for line in lines if line and not line.endswith(',#genre#') and ',' in line]
                    
                    validation_result['txt_file']['categories'] = len(categories)
                    validation_result['txt_file']['sources'] = len(sources)
                    validation_result['txt_file']['valid'] = len(sources) > 0 and len(categories) > 0
            
            # éªŒè¯M3Uæ–‡ä»¶
            m3u_path = Path(self.config.output_m3u)
            if m3u_path.exists():
                validation_result['m3u_file']['exists'] = True
                with open(m3u_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # ç»Ÿè®¡EXTINFè¡Œå’ŒURLè¡Œ
                    lines = content.strip().split('\n')
                    extinf_lines = [line for line in lines if line.startswith('#EXTINF')]
                    url_lines = [line for line in lines if line.startswith(('http://', 'https://'))]
                    
                    validation_result['m3u_file']['channels'] = len(extinf_lines)
                    validation_result['m3u_file']['sources'] = len(url_lines)
                    validation_result['m3u_file']['valid'] = (len(extinf_lines) == len(url_lines) and 
                                                            len(url_lines) > 0 and
                                                            content.startswith('#EXTM3U'))
            
            # æ€»ä½“éªŒè¯
            validation_result['overall_valid'] = (
                validation_result['txt_file']['valid'] and 
                validation_result['m3u_file']['valid']
            )
            
            logger.info("âœ… è¾“å‡ºæ–‡ä»¶éªŒè¯å®Œæˆ")
            return validation_result
            
        except Exception as e:
            logger.error(f"âŒ è¾“å‡ºæ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
            validation_result['error'] = str(e)
            return validation_result

    def generate_integrity_report(self, final_data: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®å®Œæ•´æ€§æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'overall_status': 'unknown',
            'categories_analysis': {},
            'channel_coverage': {},
            'data_quality': {},
            'recommendations': []
        }
        
        try:
            if not final_data:
                report['overall_status'] = 'empty'
                report['recommendations'].append('æœ€ç»ˆæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æºæ•°æ®å’Œæ¨¡æ¿åŒ¹é…')
                return report
            
            total_categories = len(final_data)
            total_channels = 0
            total_sources = 0
            channels_with_sources = 0
            
            # åˆ†ææ¯ä¸ªåˆ†ç±»
            for category, channels in final_data.items():
                category_channels = 0
                category_sources = 0
                category_channels_with_sources = 0
                
                for channel, sources in channels.items():
                    total_channels += 1
                    category_channels += 1
                    
                    if sources:
                        total_sources += len(sources)
                        category_sources += len(sources)
                        channels_with_sources += 1
                        category_channels_with_sources += 1
                
                # åˆ†ç±»åˆ†æ
                report['categories_analysis'][category] = {
                    'channels_total': category_channels,
                    'channels_with_sources': category_channels_with_sources,
                    'sources_total': category_sources,
                    'coverage_rate': round(category_channels_with_sources / category_channels * 100, 2) if category_channels > 0 else 0
                }
            
            # æ€»ä½“è¦†ç›–ç‡
            coverage_rate = round(channels_with_sources / total_channels * 100, 2) if total_channels > 0 else 0
            avg_sources_per_channel = round(total_sources / channels_with_sources, 2) if channels_with_sources > 0 else 0
            
            report['channel_coverage'] = {
                'total_categories': total_categories,
                'total_channels': total_channels,
                'channels_with_sources': channels_with_sources,
                'coverage_rate': coverage_rate,
                'total_sources': total_sources,
                'avg_sources_per_channel': avg_sources_per_channel
            }
            
            # æ•°æ®è´¨é‡è¯„ä¼°
            if coverage_rate >= 80:
                report['overall_status'] = 'excellent'
            elif coverage_rate >= 60:
                report['overall_status'] = 'good'
            elif coverage_rate >= 40:
                report['overall_status'] = 'fair'
            else:
                report['overall_status'] = 'poor'
            
            # æ•°æ®è´¨é‡æŒ‡æ ‡
            report['data_quality'] = {
                'coverage_score': coverage_rate,
                'source_diversity_score': min(100, avg_sources_per_channel * 20),  # æ¯ä¸ªé¢‘é“5ä¸ªæºå¾—100åˆ†
                'category_balance_score': min(100, (total_categories / 10) * 100)  # 10ä¸ªåˆ†ç±»å¾—100åˆ†
            }
            
            # ç”Ÿæˆå»ºè®®
            if coverage_rate < 50:
                report['recommendations'].append('é¢‘é“è¦†ç›–ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ æºURLæˆ–è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼')
            if avg_sources_per_channel < 2:
                report['recommendations'].append('å¹³å‡æºæ•°é‡è¾ƒå°‘ï¼Œå»ºè®®å¢åŠ æºURLæˆ–è°ƒæ•´æœ€å¤§æºæ•°é™åˆ¶')
            if total_categories < 3:
                report['recommendations'].append('åˆ†ç±»æ•°é‡è¾ƒå°‘ï¼Œå»ºè®®å®Œå–„æ¨¡æ¿æ–‡ä»¶')
            
            if not report['recommendations']:
                report['recommendations'].append('æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ç‰¹æ®Šè°ƒæ•´')
            
            logger.info("âœ… å®Œæ•´æ€§æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            return report
            
        except Exception as e:
            logger.error(f"âŒ å®Œæ•´æ€§æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            report['overall_status'] = 'error'
            report['error'] = str(e)
            return report

    def print_statistics(self, final_data: Dict[str, Any]):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*50)
        print("ğŸ“ˆ ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š")
        print("="*50)
        
        if not final_data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ç»Ÿè®¡")
            return
        
        total_channels = 0
        total_sources = 0
        categories_with_sources = 0
        
        for category, channels in final_data.items():
            category_channels = 0
            category_sources = 0
            
            for channel, sources in channels.items():
                if sources:
                    category_channels += 1
                    category_sources += len(sources)
            
            if category_channels > 0:
                categories_with_sources += 1
                print(f"  ğŸ“º {category}: {category_channels}é¢‘é“, {category_sources}æº")
                total_channels += category_channels
                total_sources += category_sources
        
        print("-"*50)
        print(f"ğŸ“Š æ€»è®¡: {total_channels}é¢‘é“, {total_sources}æº")
        print(f"ğŸ“ æœ‰æ•ˆåˆ†ç±»: {categories_with_sources}/{len(final_data)}")
        
        # ç»Ÿè®¡æ— æºçš„é¢‘é“
        no_source_channels = []
        for category, channels in final_data.items():
            for channel, sources in channels.items():
                if not sources:
                    no_source_channels.append(f"{category}-{channel}")
        
        if no_source_channels:
            print(f"âš ï¸  æ— æºé¢‘é“: {len(no_source_channels)}ä¸ª")
            if len(no_source_channels) <= 10:
                for channel in no_source_channels[:10]:
                    print(f"    âŒ {channel}")
            if len(no_source_channels) > 10:
                print(f"    ... è¿˜æœ‰ {len(no_source_channels) - 10} ä¸ªæ— æºé¢‘é“")

    def verify_cleanup(self) -> Dict[str, Any]:
        """éªŒè¯èµ„æºæ˜¯å¦å®Œå…¨æ¸…ç†"""
        verification = {
            'temp_dir_clean': False,
            'cache_size': 0,
            'backup_files': 0,
            'overall_clean': False
        }
        
        try:
            # æ£€æŸ¥ä¸´æ—¶ç›®å½•
            temp_dir = Path(self.config.temp_dir)
            if temp_dir.exists():
                temp_files = list(temp_dir.iterdir())
                verification['temp_dir_clean'] = len(temp_files) == 0
                verification['temp_files_remaining'] = len(temp_files)
            else:
                verification['temp_dir_clean'] = True
            
            # æ£€æŸ¥ç¼“å­˜ç›®å½•å¤§å°
            if self.cache_dir.exists():
                cache_files = list(self.cache_dir.glob("*.cache"))
                verification['cache_size'] = len(cache_files)
            
            # æ£€æŸ¥å¤‡ä»½æ–‡ä»¶
            if self.backup_dir.exists():
                backup_files = list(self.backup_dir.glob("backup_*.json"))
                verification['backup_files'] = len(backup_files)
            
            # æ€»ä½“æ¸…ç†çŠ¶æ€
            verification['overall_clean'] = (
                verification['temp_dir_clean'] and 
                verification['cache_size'] <= 50  # å…è®¸ä¸€å®šæ•°é‡çš„ç¼“å­˜æ–‡ä»¶
            )
            
            logger.info("âœ… èµ„æºæ¸…ç†éªŒè¯å®Œæˆ")
            return verification
            
        except Exception as e:
            logger.error(f"âŒ èµ„æºæ¸…ç†éªŒè¯å¤±è´¥: {e}")
            verification['error'] = str(e)
            return verification

    def cleanup(self):
        """å®Œæ•´çš„æ¸…ç†å·¥ä½œ"""
        try:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            temp_dir = Path(self.config.temp_dir)
            if temp_dir.exists():
                temp_files_cleaned = 0
                for file in temp_dir.iterdir():
                    if file.is_file():
                        try:
                            file.unlink()
                            temp_files_cleaned += 1
                            logger.debug(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file}")
                        except Exception as e:
                            logger.debug(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {file} - {e}")
                
                if temp_files_cleaned > 0:
                    logger.info(f"âœ… æ¸…ç†äº† {temp_files_cleaned} ä¸ªä¸´æ—¶æ–‡ä»¶")
            
            # æ¸…ç†è¿‡æœŸç¼“å­˜
            if self.config.cache_enabled and self.cache_dir.exists():
                current_time = time.time()
                cache_files_cleaned = 0
                for cache_file in self.cache_dir.iterdir():
                    if cache_file.is_file() and cache_file.suffix == '.cache':
                        try:
                            with open(cache_file, 'r', encoding='utf-8') as f:
                                cache_data = json.load(f)
                            
                            cache_time = cache_data.get('timestamp', 0)
                            if current_time - cache_time > self.config.cache_expiry:
                                cache_file.unlink()
                                cache_files_cleaned += 1
                                logger.debug(f"åˆ é™¤è¿‡æœŸç¼“å­˜: {cache_file}")
                        except Exception as e:
                            logger.debug(f"å¤„ç†ç¼“å­˜æ–‡ä»¶å¤±è´¥: {cache_file} - {e}")
                
                if cache_files_cleaned > 0:
                    logger.info(f"âœ… æ¸…ç†äº† {cache_files_cleaned} ä¸ªè¿‡æœŸç¼“å­˜æ–‡ä»¶")
        
        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

    def create_demo_template(self) -> bool:
        """åˆ›å»ºç¤ºä¾‹æ¨¡æ¿æ–‡ä»¶"""
        demo_content = """# IPTVé¢‘é“æ¨¡æ¿æ–‡ä»¶
# æ ¼å¼: åˆ†ç±»åç§°,#genre#
#        é¢‘é“åç§°1
#        é¢‘é“åç§°2

å¤®è§†é¢‘é“,#genre#
CCTV-1 ç»¼åˆ
CCTV-2 è´¢ç»
CCTV-3 ç»¼è‰º
CCTV-4 ä¸­æ–‡å›½é™…
CCTV-5 ä½“è‚²
CCTV-6 ç”µå½±
CCTV-7 å›½é˜²å†›äº‹
CCTV-8 ç”µè§†å‰§
CCTV-9 çºªå½•
CCTV-10 ç§‘æ•™
CCTV-11 æˆæ›²
CCTV-12 ç¤¾ä¼šä¸æ³•
CCTV-13 æ–°é—»
CCTV-14 å°‘å„¿
CCTV-15 éŸ³ä¹

å«è§†é¢‘é“,#genre#
æ¹–å—å«è§†
æµ™æ±Ÿå«è§†
æ±Ÿè‹å«è§†
ä¸œæ–¹å«è§†
åŒ—äº¬å«è§†
å¤©æ´¥å«è§†
å±±ä¸œå«è§†
å¹¿ä¸œå«è§†
æ·±åœ³å«è§†

åœ°æ–¹é¢‘é“,#genre#
åŒ—äº¬æ–°é—»
ä¸Šæµ·æ–°é—»
å¹¿å·ç»¼åˆ
é‡åº†å«è§†
æˆéƒ½æ–°é—»

é«˜æ¸…é¢‘é“,#genre#
CCTV-1 HD
CCTV-5+ HD
æ¹–å—å«è§† HD
æµ™æ±Ÿå«è§† HD
"""
        try:
            with open(self.config.template_file, 'w', encoding='utf-8') as f:
                f.write(demo_content)
            logger.info(f"âœ… å·²åˆ›å»ºç¤ºä¾‹æ¨¡æ¿æ–‡ä»¶: {Path(self.config.template_file).absolute()}")
            return True
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def run(self):
        """å®Œæ•´çš„ä¸»è¿è¡Œå‡½æ•°"""
        print("=" * 60)
        print("ğŸ¬ IPTVæ™ºèƒ½ç®¡ç†å·¥å…· - å®Œæ•´ä¼ä¸šçº§ç‰ˆæœ¬ v8.0")
        print("=" * 60)
        
        # æ£€æŸ¥ä¾èµ–
        if not self.check_dependencies():
            print("âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return
        
        # éªŒè¯é…ç½®
        if not self.validate_config():
            print("âŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return
        
        # ä¿å­˜åˆå§‹é…ç½®
        if not self.save_config():
            print("âš ï¸  é…ç½®ä¿å­˜å¤±è´¥ï¼Œä½†ç¨‹åºå°†ç»§ç»­è¿è¡Œ")
        
        # æ£€æŸ¥æ¢å¤ç‚¹
        can_resume, checkpoint_data = self.can_resume_from_checkpoint()
        if can_resume:
            print(f"ğŸ” å‘ç°æ£€æŸ¥ç‚¹: {checkpoint_data['stage']}")
            response = input("æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤? (y/N): ")
            if response.lower() == 'y':
                print("â© ä»æ£€æŸ¥ç‚¹æ¢å¤åŠŸèƒ½å¾…å®ç°ï¼Œå¼€å§‹æ–°çš„å¤„ç†æµç¨‹...")
            else:
                print("ğŸ”„ å¼€å§‹æ–°çš„å¤„ç†æµç¨‹...")
        
        # æ£€æŸ¥æ¨¡æ¿æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºç¤ºä¾‹
        template_path = Path(self.config.template_file)
        if not template_path.exists():
            print("ğŸ“ æœªæ‰¾åˆ°æ¨¡æ¿æ–‡ä»¶ï¼Œåˆ›å»ºç¤ºä¾‹æ¨¡æ¿...")
            if self.create_demo_template():
                print(f"\nğŸ’¡ æ¨¡æ¿æ–‡ä»¶å·²åˆ›å»ºï¼Œè¯·ç¼–è¾‘ä»¥ä¸‹æ–‡ä»¶åé‡æ–°è¿è¡Œç¨‹åº:")
                print(f"   ğŸ“„ {template_path.absolute()}")
                print("\næ¨¡æ¿å†…å®¹åŒ…å«å¸¸è§çš„å¤®è§†ã€å«è§†ç­‰é¢‘é“")
                input("æŒ‰å›è½¦é”®é€€å‡º...")
            return
        
        start_time = time.time()
        
        try:
            # 1. åŠ è½½æ¨¡æ¿
            print("\nğŸ“‹ æ­¥éª¤ 1/7: åŠ è½½é¢‘é“æ¨¡æ¿")
            self.save_checkpoint("loading_template")
            template_categories = self.load_template()
            if not template_categories:
                return
            
            # 2. è·å–æ‰€æœ‰æºæ•°æ®
            print("\nğŸŒ æ­¥éª¤ 2/7: è·å–æºæ•°æ®")
            self.save_checkpoint("fetching_sources")
            content = self.fetch_all_streams()
            if not content:
                print("âŒ æœªèƒ½è·å–ä»»ä½•æºæ•°æ®")
                return
            
            # å¤‡ä»½åŸå§‹æ•°æ®
            self.backup_data("raw_content", content)
            
            # 3. æ•´ç†æºæ•°æ®
            print("\nğŸ”§ æ­¥éª¤ 3/7: æ•´ç†æºæ•°æ®")
            self.save_checkpoint("organizing_streams")
            sources_df = self.organize_streams(content)
            if sources_df.empty:
                print("âŒ æœªèƒ½è§£æå‡ºæœ‰æ•ˆçš„æµæ•°æ®")
                return
            
            # å¤‡ä»½æ•´ç†åçš„æ•°æ®
            self.backup_data("organized_streams", sources_df)
            
            # 4. è·å–æ‰€æœ‰æ¨¡æ¿é¢‘é“
            all_template_channels = []
            for channels in template_categories.values():
                all_template_channels.extend(channels)
            
            # 5. è¿‡æ»¤å’ŒåŒ¹é…é¢‘é“
            print("\nğŸ¯ æ­¥éª¤ 4/7: é¢‘é“åŒ¹é…")
            self.save_checkpoint("matching_channels")
            filtered_df = self.filter_and_sort_sources(sources_df, all_template_channels)
            if filtered_df.empty:
                print("âŒ æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æ¨¡æ¿é¢‘é“")
                return
            
            # å¤‡ä»½åŒ¹é…ç»“æœ
            self.backup_data("matched_channels", filtered_df)
            
            # 6. æµ‹é€Ÿ
            print("\nâš¡ æ­¥éª¤ 5/7: æºæµ‹é€Ÿ")
            self.save_checkpoint("speed_testing")
            speed_tested_df = self.speed_test_sources(filtered_df)
            if speed_tested_df.empty:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„æºé€šè¿‡æµ‹é€Ÿ")
                return
            
            # 7. ç”Ÿæˆæœ€ç»ˆæ•°æ®
            print("\nğŸ¨ æ­¥éª¤ 6/7: ç”Ÿæˆæ’­æ”¾åˆ—è¡¨")
            self.save_checkpoint("generating_final_data")
            final_data = self.generate_final_data(speed_tested_df, template_categories)
            
            # 8. ä¿å­˜æ–‡ä»¶
            print("\nğŸ’¾ æ­¥éª¤ 7/7: ä¿å­˜æ–‡ä»¶")
            self.save_checkpoint("saving_files")
            if not self.save_output_files(final_data):
                print("âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥")
                return
            
            # 9. éªŒè¯è¾“å‡ºæ–‡ä»¶
            print("\nğŸ” éªŒè¯è¾“å‡ºæ–‡ä»¶...")
            validation_result = self.validate_output_files()
            
            # 10. ç”Ÿæˆå®Œæ•´æ€§æŠ¥å‘Š
            integrity_report = self.generate_integrity_report(final_data)
            
            # 11. æ‰“å°ç»Ÿè®¡å’ŒæŠ¥å‘Š
            self.print_statistics(final_data)
            
            # æ‰“å°éªŒè¯ç»“æœ
            print("\nğŸ“Š æ–‡ä»¶éªŒè¯ç»“æœ:")
            if validation_result['txt_file']['valid']:
                print(f"  âœ… TXTæ–‡ä»¶: {validation_result['txt_file']['categories']}åˆ†ç±», {validation_result['txt_file']['sources']}ä¸ªæº")
            else:
                print(f"  âŒ TXTæ–‡ä»¶: æ— æ•ˆæˆ–ä¸ºç©º")
            
            if validation_result['m3u_file']['valid']:
                print(f"  âœ… M3Uæ–‡ä»¶: {validation_result['m3u_file']['channels']}ä¸ªé¢‘é“, {validation_result['m3u_file']['sources']}ä¸ªæº")
            else:
                print(f"  âŒ M3Uæ–‡ä»¶: æ— æ•ˆæˆ–æ ¼å¼é”™è¯¯")
            
            # æ‰“å°å®Œæ•´æ€§æŠ¥å‘Šæ‘˜è¦
            print(f"\nğŸ“ˆ æ•°æ®å®Œæ•´æ€§: {integrity_report['overall_status'].upper()}")
            print(f"ğŸ“º é¢‘é“è¦†ç›–ç‡: {integrity_report['channel_coverage']['coverage_rate']}%")
            print(f"ğŸ”— å¹³å‡æºæ•°: {integrity_report['channel_coverage']['avg_sources_per_channel']:.2f}")
            
            if integrity_report['recommendations']:
                print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                for recommendation in integrity_report['recommendations']:
                    print(f"  â€¢ {recommendation}")
            
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            print("\nğŸ‰ å¤„ç†å®Œæˆ!")
            print(f"â° æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
            print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶ä½ç½®:")
            print(f"   ğŸ“„ {Path(self.config.output_txt).absolute()}")
            print(f"   ğŸ“„ {Path(self.config.output_m3u).absolute()}")
            
            # æ¸…ç†æ£€æŸ¥ç‚¹ï¼ˆæˆåŠŸå®Œæˆï¼‰
            if self.checkpoint_file.exists():
                self.checkpoint_file.unlink()
            
            # éªŒè¯èµ„æºæ¸…ç†
            cleanup_verification = self.verify_cleanup()
            if not cleanup_verification['overall_clean']:
                print(f"âš ï¸  èµ„æºæ¸…ç†è­¦å‘Š: è¿˜æœ‰{cleanup_verification['temp_files_remaining']}ä¸ªä¸´æ—¶æ–‡ä»¶æœªæ¸…ç†")
                
        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
            print("ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œä¸‹æ¬¡å¯ä»¥æ¢å¤å¤„ç†")
        except Exception as e:
            print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            print("ğŸ’¾ é”™è¯¯æ£€æŸ¥ç‚¹å·²ä¿å­˜")
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self.cleanup()

def main():
    """ä¸»å‡½æ•°"""
    try:
        # å°è¯•åŠ è½½ç°æœ‰é…ç½®
        manager = IPTVManager()
        config = manager.load_config()
        if config:
            manager = IPTVManager(config)
            print("âœ… ä½¿ç”¨å·²ä¿å­˜çš„é…ç½®")
        else:
            print("â„¹ï¸  ä½¿ç”¨é»˜è®¤é…ç½®")
        
        manager.run()
    except Exception as e:
        print(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
