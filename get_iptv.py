#!/usr/bin/env python3
"""
IPTVæ™ºèƒ½ç®¡ç†å·¥å…· - å®Œæ•´ä¼ä¸šçº§ç‰ˆæœ¬ï¼ˆä¿®å¤ä¼˜åŒ–ç‰ˆï¼‰
åŠŸèƒ½ï¼šå¤šæºæŠ“å–ã€é¢‘é“åŒ¹é…ã€é€Ÿåº¦æµ‹è¯•ã€æ’­æ”¾åˆ—è¡¨ç”Ÿæˆã€é…ç½®ç®¡ç†ã€æ•°æ®éªŒè¯
ç‰ˆæœ¬ï¼šv8.3 (å®Œæ•´ä¼ä¸šçº§ç‰ˆæœ¬ - ä¿®å¤ä¼˜åŒ–ç‰ˆ)
"""

import requests
import pandas as pd
import re
import os
import time
import subprocess
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import hashlib
import json
from datetime import datetime
import shutil

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    handlers=[
        logging.FileHandler('iptv_manager.log', encoding='utf-8', mode='w'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger('IPTVManager')

@dataclass
class AppConfig:
    """åº”ç”¨é…ç½®ç±»"""
    source_urls: List[str]
    request_timeout: int = 15
    max_sources_per_channel: int = 8
    speed_test_timeout: int = 10  # æµ‹é€Ÿè¶…æ—¶å¢åŠ åˆ°10ç§’
    similarity_threshold: int = 50
    max_workers: int = 6
    template_file: str = "demo.txt"
    output_txt: str = "iptv.txt"
    output_m3u: str = "iptv.m3u"
    temp_dir: str = "temp"
    cache_enabled: bool = True
    cache_expiry: int = 3600  # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'AppConfig':
        """ä»å­—å…¸åˆ›å»ºé…ç½®"""
        return cls(**data)

class ProgressBar:
    """è‡ªå®šä¹‰è¿›åº¦æ¡å®ç°"""
    
    def __init__(self, total: int, desc: str = "Processing", unit: str = "it"):
        self.total = total
        self.desc = desc
        self.unit = unit
        self.completed = 0
        self.start_time = time.time()
        self.last_update_time = self.start_time
        self.update_interval = 0.1  # æ›´æ–°é—´éš”ï¼ˆç§’ï¼‰
    
    def update(self, n: int = 1) -> None:
        """æ›´æ–°è¿›åº¦"""
        self.completed += n
        current_time = time.time()
        
        # æ§åˆ¶æ›´æ–°é¢‘ç‡
        if current_time - self.last_update_time < self.update_interval and self.completed < self.total:
            return
            
        self.last_update_time = current_time
        self._display()
    
    def _display(self) -> None:
        """æ˜¾ç¤ºè¿›åº¦æ¡"""
        if self.total == 0:
            return
            
        elapsed = time.time() - self.start_time
        percent = min(100, (self.completed / self.total) * 100)
        
        # è®¡ç®—ETA
        if self.completed > 0:
            eta = (elapsed / self.completed) * (self.total - self.completed)
            eta_str = f"ETA: {eta:.1f}s"
        else:
            eta_str = "ETA: è®¡ç®—ä¸­..."
        
        # è¿›åº¦æ¡
        bar_length = 50
        filled_length = int(bar_length * percent / 100)
        bar = 'â–ˆ' * filled_length + ' ' * (bar_length - filled_length)
        
        # é€Ÿåº¦
        speed = self.completed / elapsed if elapsed > 0 else 0
        speed_str = f"{speed:.2f} {self.unit}/s"
        
        # æ˜¾ç¤º
        display_text = f"\r{self.desc}: [{bar}] {percent:.1f}% ({self.completed}/{self.total}) {speed_str} {eta_str}"
        print(display_text, end="", flush=True)
    
    def close(self) -> None:
        """å®Œæˆè¿›åº¦æ¡"""
        if self.total > 0:
            self.completed = self.total
            self._display()
        print()  # æ¢è¡Œ

class IPTVManager:
    """IPTVæ™ºèƒ½ç®¡ç†å·¥å…·ä¸»ç±»"""
    
    def __init__(self, config: Optional[AppConfig] = None):
        """åˆå§‹åŒ–IPTVç®¡ç†å™¨"""
        # ä½¿ç”¨é»˜è®¤é…ç½®æˆ–ä¼ å…¥é…ç½®
        self.config = config or AppConfig(
            source_urls=[
    "https://raw.githubusercontent.com/zwc456baby/iptv_alive/master/live.txt",
    "https://raw.githubusercontent.com/iptv-org/iptv/gh-pages/countries/cn.m3u",
    "https://ghfast.top/raw.githubusercontent.com/Supprise0901/TVBox_live/main/live.txt",
    "https://gh-proxy.com/https://raw.githubusercontent.com/wwb521/live/main/tv.m3u",
    "https://gh-proxy.com/https://raw.githubusercontent.com/zeee-u/lzh06/main/fl.m3u",
    "https://raw.githubusercontent.com/Guovin/iptv-database/master/result.txt",  
    "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/cn.m3u",
    "https://raw.githubusercontent.com/suxuang/myIPTV/main/ipv4.m3u",
    "https://raw.githubusercontent.com/vbskycn/iptv/master/tv/iptv4.txt",
    "http://47.120.41.246:8899/zb.txt",
    "https://live.zbds.top/tv/iptv4.txt",
            ]
        )
        
        # åˆå§‹åŒ–ä¼šè¯å’Œç›®å½•
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self._setup_directories()
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self._compile_patterns()
        
        # çŠ¶æ€å˜é‡
        self.ffmpeg_available = False
        self.processed_count = 0
        self.total_count = 0
        self.cache_dir = Path("cache")
        self.backup_dir = Path("backups")
        self.checkpoint_file = Path("checkpoint.json")
        self.current_stage = "not_started"
        
        # ç¼“å­˜é™åˆ¶
        self.max_cache_size = 100  # æœ€å¤§ç¼“å­˜æ–‡ä»¶æ•°
        self.max_cache_size_mb = 100  # æœ€å¤§ç¼“å­˜å¤§å°(MB)
        
        # åˆ›å»ºç›®å½•
        self.cache_dir.mkdir(exist_ok=True)
        self.backup_dir.mkdir(exist_ok=True)

    def _setup_directories(self) -> None:
        """è®¾ç½®å¿…è¦çš„ç›®å½•"""
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_path = Path(self.config.temp_dir)
            temp_path.mkdir(exist_ok=True)
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = Path(".").absolute()
            logger.info(f"å·¥ä½œç›®å½•: {output_dir}")
            
        except Exception as e:
            logger.error(f"ç›®å½•è®¾ç½®å¤±è´¥: {e}")
            raise

    def _compile_patterns(self) -> None:
        """ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        self.patterns = {
            'ipv4': re.compile(r'^https?://(\d{1,3}\.){3}\d{1,3}'),
            'ipv6': re.compile(r'^https?://\[([a-fA-F0-9:]+)\]'),
            'extinf': re.compile(r'#EXTINF:.*?tvg-name="([^"]+)".*?,(.+)'),
            'category': re.compile(r'^(.*?),#genre#$'),
            'url': re.compile(r'https?://[^\s,]+'),
            'channel_name': re.compile(r'[#EXTINF:].*?,(.+)$'),
            'clean_name': re.compile(r'[^\w\u4e00-\u9fa5\s-]'),
            'tvg_name': re.compile(r'tvg-name="([^"]*)"'),
            'tvg_id': re.compile(r'tvg-id="([^"]*)"'),
            'group_title': re.compile(r'group-title="([^"]*)"'),
            'extinf_content': re.compile(r',\s*(.+)$')
        }

    def save_config(self, config_path: str = "iptv_config.json") -> bool:
        """ä¿å­˜é…ç½®åˆ°JSONæ–‡ä»¶"""
        try:
            config_dict = self.config.to_dict()
            
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config_dict, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {Path(config_path).absolute()}")
            return True
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜é…ç½®å¤±è´¥: {e}")
            return False

    def load_config(self, config_path: str = "iptv_config.json") -> Optional[AppConfig]:
        """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            if not Path(config_path).exists():
                logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
                return None
            
            with open(config_path, 'r', encoding='utf-8') as f:
                config_dict = json.load(f)
            
            # éªŒè¯å¿…éœ€çš„é…ç½®é¡¹
            required_keys = ['source_urls']
            for key in required_keys:
                if key not in config_dict:
                    logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘å¿…éœ€çš„é”®: {key}")
                    return None
            
            # åˆ›å»ºé…ç½®å¯¹è±¡
            config = AppConfig.from_dict(config_dict)
            
            logger.info(f"âœ… é…ç½®å·²ä»æ–‡ä»¶åŠ è½½: {config_path}")
            return config
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é…ç½®å¤±è´¥: {e}")
            return None

    def validate_config(self) -> bool:
        """éªŒè¯é…ç½®çš„å®Œæ•´æ€§"""
        try:
            config = self.config
            
            # éªŒè¯URLs
            if not config.source_urls:
                logger.error("âŒ é…ç½®é”™è¯¯: æ²¡æœ‰æºURL")
                return False
                
            for url in config.source_urls:
                if not self.validate_url(url):
                    logger.error(f"âŒ é…ç½®é”™è¯¯: æ— æ•ˆçš„æºURL - {url}")
                    return False
            
            # éªŒè¯æ•°å€¼å‚æ•°
            if config.request_timeout <= 0:
                logger.error("âŒ é…ç½®é”™è¯¯: è¯·æ±‚è¶…æ—¶å¿…é¡»å¤§äº0")
                return False
                
            if config.speed_test_timeout <= 0:
                logger.error("âŒ é…ç½®é”™è¯¯: æµ‹é€Ÿè¶…æ—¶å¿…é¡»å¤§äº0")
                return False
                
            if config.similarity_threshold < 0 or config.similarity_threshold > 100:
                logger.error("âŒ é…ç½®é”™è¯¯: ç›¸ä¼¼åº¦é˜ˆå€¼å¿…é¡»åœ¨0-100ä¹‹é—´")
                return False
                
            if config.max_sources_per_channel <= 0:
                logger.error("âŒ é…ç½®é”™è¯¯: æœ€å¤§æºæ•°å¿…é¡»å¤§äº0")
                return False
            
            if config.max_workers <= 0:
                logger.error("âŒ é…ç½®é”™è¯¯: å·¥ä½œçº¿ç¨‹æ•°å¿…é¡»å¤§äº0")
                return False
            
            # éªŒè¯æ–‡ä»¶è·¯å¾„
            template_path = Path(config.template_file)
            if not template_path.exists():
                logger.warning(f"âš ï¸ æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")
                # è¿™é‡Œä¸è¿”å›Falseï¼Œå› ä¸ºç¨‹åºä¼šåˆ›å»ºç¤ºä¾‹æ¨¡æ¿
            
            # éªŒè¯ç›®å½•æƒé™
            try:
                temp_dir = Path(config.temp_dir)
                temp_dir.mkdir(exist_ok=True)
                test_file = temp_dir / "test_write"
                test_file.touch()
                test_file.unlink()
            except Exception as e:
                logger.error(f"âŒ é…ç½®é”™è¯¯: æ— æ³•å†™å…¥ä¸´æ—¶ç›®å½• - {e}")
                return False
                
            logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {e}")
            return False

    def check_dependencies(self) -> bool:
        """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
        try:
            # æ£€æŸ¥åŸºç¡€ä¾èµ–
            import requests
            import pandas as pd
            logger.info("âœ… åŸºç¡€ä¾èµ–æ£€æŸ¥é€šè¿‡")
            
            # æ£€æŸ¥FFmpeg
            self.ffmpeg_available = self._check_ffmpeg()
            
            return True
            
        except ImportError as e:
            logger.error(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
            print("è¯·è¿è¡Œ: pip install requests pandas")
            return False

    def _check_ffmpeg(self) -> bool:
        """æ£€æŸ¥FFmpegæ˜¯å¦å¯ç”¨"""
        try:
            result = subprocess.run(
                ['ffmpeg', '-version'], 
                capture_output=True, 
                timeout=5,
                check=False
            )
            if result.returncode == 0:
                logger.info("âœ… FFmpegå¯ç”¨")
                return True
            else:
                logger.warning("âš ï¸ FFmpegæœªå®‰è£…æˆ–ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨HTTPæµ‹é€Ÿ")
                return False
        except (subprocess.SubprocessError, FileNotFoundError, subprocess.TimeoutExpired):
            logger.warning("âš ï¸ FFmpegæœªå®‰è£…ï¼Œå°†ä½¿ç”¨HTTPæµ‹é€Ÿ")
            return False

    def get_cache_key(self, url: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(url.encode('utf-8')).hexdigest()

    def get_cached_content(self, url: str) -> Optional[str]:
        """è·å–ç¼“å­˜å†…å®¹"""
        if not self.config.cache_enabled:
            return None
            
        cache_file = self.cache_dir / f"{self.get_cache_key(url)}.cache"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                cache_time = cache_data.get('timestamp', 0)
                if time.time() - cache_time < self.config.cache_expiry:
                    logger.debug(f"ä½¿ç”¨ç¼“å­˜: {url}")
                    return cache_data.get('content')
                else:
                    logger.debug(f"ç¼“å­˜å·²è¿‡æœŸ: {url}")
            except Exception as e:
                logger.debug(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
                
        return None

    def set_cached_content(self, url: str, content: str) -> None:
        """è®¾ç½®ç¼“å­˜å†…å®¹ï¼ˆåŒ…å«å¤§å°é™åˆ¶ï¼‰"""
        if not self.config.cache_enabled:
            return
            
        try:
            # æ£€æŸ¥ç¼“å­˜å¤§å°å¹¶æ¸…ç†
            self._cleanup_cache_if_needed()
            
            cache_file = self.cache_dir / f"{self.get_cache_key(url)}.cache"
            cache_data = {
                'timestamp': time.time(),
                'content': content,
                'url': url,
                'size': len(content.encode('utf-8'))
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False)
        except Exception as e:
            logger.debug(f"å†™å…¥ç¼“å­˜å¤±è´¥: {e}")

    def _cleanup_cache_if_needed(self) -> None:
        """æ¸…ç†ç¼“å­˜å¦‚æœè¶…è¿‡é™åˆ¶"""
        try:
            if not self.cache_dir.exists():
                return
                
            cache_files = list(self.cache_dir.glob("*.cache"))
            
            # æ£€æŸ¥æ–‡ä»¶æ•°é‡é™åˆ¶
            if len(cache_files) >= self.max_cache_size:
                self._cleanup_cache_by_count(cache_files)
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶
            total_size = sum(f.stat().st_size for f in cache_files if f.is_file())
            if total_size > self.max_cache_size_mb * 1024 * 1024:
                self._cleanup_cache_by_size(cache_files)
                
        except Exception as e:
            logger.debug(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

    def _cleanup_cache_by_count(self, cache_files: List[Path]) -> None:
        """æŒ‰æ–‡ä»¶æ•°é‡æ¸…ç†ç¼“å­˜"""
        # æŒ‰æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„
        files_with_time = []
        for cache_file in cache_files:
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                files_with_time.append((cache_file, cache_data.get('timestamp', 0)))
            except:
                files_with_time.append((cache_file, 0))
        
        # æŒ‰æ—¶é—´æ’åº
        files_with_time.sort(key=lambda x: x[1])
        
        # åˆ é™¤è¶…è¿‡é™åˆ¶çš„æ–‡ä»¶
        files_to_remove = len(files_with_time) - self.max_cache_size
        for i in range(files_to_remove):
            try:
                files_with_time[i][0].unlink()
                logger.debug(f"æ¸…ç†ç¼“å­˜æ–‡ä»¶: {files_with_time[i][0]}")
            except:
                pass

    def _cleanup_cache_by_size(self, cache_files: List[Path]) -> None:
        """æŒ‰æ–‡ä»¶å¤§å°æ¸…ç†ç¼“å­˜"""
        # æŒ‰æ–‡ä»¶å¤§å°å’Œæ—¶é—´æ’åº
        files_with_info = []
        for cache_file in cache_files:
            try:
                stat = cache_file.stat()
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                files_with_info.append((cache_file, stat.st_size, cache_data.get('timestamp', 0)))
            except:
                files_with_info.append((cache_file, 0, 0))
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆå…ˆåˆ é™¤æœ€æ—§çš„ï¼‰
        files_with_info.sort(key=lambda x: x[2])
        
        # è®¡ç®—éœ€è¦åˆ é™¤çš„å¤§å°
        total_size = sum(size for _, size, _ in files_with_info)
        target_size = self.max_cache_size_mb * 1024 * 1024 * 0.8  # æ¸…ç†åˆ°80%
        
        current_size = total_size
        for file_path, size, _ in files_with_info:
            if current_size <= target_size:
                break
            try:
                file_path.unlink()
                current_size -= size
                logger.debug(f"æ¸…ç†å¤§ç¼“å­˜æ–‡ä»¶: {file_path} ({size} bytes)")
            except:
                pass

    def backup_data(self, stage: str, data: Any) -> bool:
        """å¤‡ä»½å¤„ç†é˜¶æ®µçš„æ•°æ®"""
        try:
            # æ¸…ç†æ—§çš„å¤‡ä»½æ–‡ä»¶
            self._cleanup_old_backups()
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = self.backup_dir / f"backup_{stage}_{timestamp}.json"
            
            if isinstance(data, pd.DataFrame):
                # å¤‡ä»½DataFrame
                data.to_json(backup_file, orient='records', force_ascii=False, indent=2)
            elif isinstance(data, dict):
                # å¤‡ä»½å­—å…¸
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
            elif isinstance(data, str):
                # å¤‡ä»½å­—ç¬¦ä¸²
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump({'content': data}, f, ensure_ascii=False, indent=2)
            else:
                # å¤‡ä»½å…¶ä»–æ•°æ®ç±»å‹
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump({'data': str(data)}, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"âœ… æ•°æ®å¤‡ä»½å®Œæˆ: {stage} -> {backup_file}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¤‡ä»½å¤±è´¥: {e}")
            return False

    def _cleanup_old_backups(self) -> None:
        """æ¸…ç†æ—§çš„å¤‡ä»½æ–‡ä»¶"""
        try:
            if not self.backup_dir.exists():
                return
                
            backup_files = list(self.backup_dir.glob("backup_*.json"))
            if len(backup_files) <= 50:  # ä¿ç•™æœ€å¤š50ä¸ªå¤‡ä»½æ–‡ä»¶
                return
                
            # æŒ‰æ—¶é—´æ’åº
            backup_files.sort(key=lambda x: x.stat().st_mtime)
            
            # åˆ é™¤æœ€æ—§çš„å¤‡ä»½æ–‡ä»¶
            files_to_remove = len(backup_files) - 50
            for i in range(files_to_remove):
                try:
                    backup_files[i].unlink()
                    logger.debug(f"æ¸…ç†å¤‡ä»½æ–‡ä»¶: {backup_files[i]}")
                except Exception as e:
                    logger.debug(f"åˆ é™¤å¤‡ä»½æ–‡ä»¶å¤±è´¥: {backup_files[i]} - {e}")
                    
        except Exception as e:
            logger.debug(f"å¤‡ä»½æ–‡ä»¶æ¸…ç†å¤±è´¥: {e}")

    def save_checkpoint(self, stage: str, data: Any = None) -> bool:
        """ä¿å­˜å¤„ç†æ£€æŸ¥ç‚¹"""
        try:
            checkpoint_data = {
                'stage': stage,
                'timestamp': time.time(),
                'config': self.config.to_dict()
            }
            
            if data is not None:
                # ä¿å­˜å…³é”®æ•°æ®æ‘˜è¦
                if isinstance(data, pd.DataFrame):
                    checkpoint_data['data_summary'] = {
                        'rows': len(data),
                        'columns': list(data.columns),
                        'sample_channels': data['program_name'].head(5).tolist() if 'program_name' in data.columns else []
                    }
                elif isinstance(data, dict):
                    checkpoint_data['data_summary'] = {
                        'categories': len(data),
                        'total_channels': sum(len(channels) for channels in data.values()) if data else 0
                    }
            
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            
            self.current_stage = stage
            logger.info(f"âœ… æ£€æŸ¥ç‚¹ä¿å­˜: {stage}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")
            return False

    def can_resume_from_checkpoint(self) -> Tuple[bool, Optional[Dict]]:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥ä»æ£€æŸ¥ç‚¹æ¢å¤"""
        try:
            if not self.checkpoint_file.exists():
                return False, None
            
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
            
            # éªŒè¯æ£€æŸ¥ç‚¹æ•°æ®çš„å®Œæ•´æ€§
            required_keys = ['stage', 'timestamp']
            for key in required_keys:
                if key not in checkpoint_data:
                    return False, None
            
            # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…ï¼ˆ24å°æ—¶å†…ï¼‰
            if time.time() - checkpoint_data['timestamp'] > 86400:
                logger.warning("âš ï¸ æ£€æŸ¥ç‚¹å·²è¿‡æœŸï¼ˆè¶…è¿‡24å°æ—¶ï¼‰")
                return False, None
            
            return True, checkpoint_data
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ç‚¹è¯»å–å¤±è´¥: {e}")
            return False, None

    def resume_from_checkpoint(self, checkpoint_data: Dict) -> bool:
        """ä»æ£€æŸ¥ç‚¹æ¢å¤å¤„ç†"""
        try:
            stage = checkpoint_data['stage']
            logger.info(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤: {stage}")
            
            if stage == "loading_template":
                return self._resume_from_loading_template(checkpoint_data)
            elif stage == "fetching_sources":
                return self._resume_from_fetching_sources(checkpoint_data)
            elif stage == "organizing_streams":
                return self._resume_from_organizing_streams(checkpoint_data)
            elif stage == "matching_channels":
                return self._resume_from_matching_channels(checkpoint_data)
            elif stage == "speed_testing":
                return self._resume_from_speed_testing(checkpoint_data)
            elif stage == "generating_final_data":
                return self._resume_from_generating_final_data(checkpoint_data)
            elif stage == "saving_files":
                return self._resume_from_saving_files(checkpoint_data)
            else:
                logger.error(f"âŒ æœªçŸ¥çš„æ£€æŸ¥ç‚¹é˜¶æ®µ: {stage}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ä»æ£€æŸ¥ç‚¹æ¢å¤å¤±è´¥: {e}")
            return False

    def _resume_from_loading_template(self, checkpoint_data: Dict) -> bool:
        """ä»åŠ è½½æ¨¡æ¿é˜¶æ®µæ¢å¤"""
        print("\nğŸ“‹ ä»æ£€æŸ¥ç‚¹æ¢å¤: åŠ è½½é¢‘é“æ¨¡æ¿")
        template_categories = self.load_template()
        if not template_categories:
            return False
        
        # ç»§ç»­åç»­æµç¨‹
        return self._continue_after_template_loading(template_categories)

    def _resume_from_fetching_sources(self, checkpoint_data: Dict) -> bool:
        """ä»è·å–æºæ•°æ®é˜¶æ®µæ¢å¤"""
        print("\nğŸŒ ä»æ£€æŸ¥ç‚¹æ¢å¤: è·å–æºæ•°æ®")
        content = self.fetch_all_streams()
        if not content:
            return False
        
        self.backup_data("raw_content", content)
        return self._continue_after_fetching_sources(content)

    def _resume_from_organizing_streams(self, checkpoint_data: Dict) -> bool:
        """ä»æ•´ç†æ•°æ®é˜¶æ®µæ¢å¤"""
        print("\nğŸ”§ ä»æ£€æŸ¥ç‚¹æ¢å¤: æ•´ç†æºæ•°æ®")
        # è¿™é‡Œéœ€è¦é‡æ–°è·å–å†…å®¹æˆ–ä»å¤‡ä»½æ¢å¤
        # ç®€åŒ–å®ç°ï¼šé‡æ–°å¼€å§‹
        return False

    def _resume_from_matching_channels(self, checkpoint_data: Dict) -> bool:
        """ä»é¢‘é“åŒ¹é…é˜¶æ®µæ¢å¤"""
        print("\nğŸ¯ ä»æ£€æŸ¥ç‚¹æ¢å¤: é¢‘é“åŒ¹é…")
        # è¿™é‡Œéœ€è¦é‡æ–°è·å–æ•°æ®æˆ–ä»å¤‡ä»½æ¢å¤
        # ç®€åŒ–å®ç°ï¼šé‡æ–°å¼€å§‹
        return False

    def _resume_from_speed_testing(self, checkpoint_data: Dict) -> bool:
        """ä»æµ‹é€Ÿé˜¶æ®µæ¢å¤"""
        print("\nâš¡ ä»æ£€æŸ¥ç‚¹æ¢å¤: æºæµ‹é€Ÿ")
        # è¿™é‡Œéœ€è¦é‡æ–°è·å–æ•°æ®æˆ–ä»å¤‡ä»½æ¢å¤
        # ç®€åŒ–å®ç°ï¼šé‡æ–°å¼€å§‹
        return False

    def _resume_from_generating_final_data(self, checkpoint_data: Dict) -> bool:
        """ä»ç”Ÿæˆæ•°æ®é˜¶æ®µæ¢å¤"""
        print("\nğŸ¨ ä»æ£€æŸ¥ç‚¹æ¢å¤: ç”Ÿæˆæ’­æ”¾åˆ—è¡¨")
        # è¿™é‡Œéœ€è¦é‡æ–°è·å–æ•°æ®æˆ–ä»å¤‡ä»½æ¢å¤
        # ç®€åŒ–å®ç°ï¼šé‡æ–°å¼€å§‹
        return False

    def _resume_from_saving_files(self, checkpoint_data: Dict) -> bool:
        """ä»ä¿å­˜æ–‡ä»¶é˜¶æ®µæ¢å¤"""
        print("\nğŸ’¾ ä»æ£€æŸ¥ç‚¹æ¢å¤: ä¿å­˜æ–‡ä»¶")
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        txt_path = Path(self.config.output_txt)
        m3u_path = Path(self.config.output_m3u)
        
        if txt_path.exists() and m3u_path.exists():
            print("âœ… è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ¢å¤å®Œæˆ")
            return True
        else:
            print("âŒ è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ")
            return False

    def _continue_after_template_loading(self, template_categories: Dict) -> bool:
        """æ¨¡æ¿åŠ è½½åçš„ç»§ç»­å¤„ç†"""
        # è·å–æ‰€æœ‰æ¨¡æ¿é¢‘é“
        all_template_channels = []
        for channels in template_categories.values():
            all_template_channels.extend(channels)
        
        # ç»§ç»­è·å–æºæ•°æ®
        print("\nğŸŒ ç»§ç»­: è·å–æºæ•°æ®")
        content = self.fetch_all_streams()
        if not content:
            return False
        
        self.backup_data("raw_content", content)
        return self._continue_after_fetching_sources(content, template_categories, all_template_channels)

    def _continue_after_fetching_sources(self, content: str, template_categories: Dict = None, all_template_channels: List[str] = None) -> bool:
        """è·å–æºæ•°æ®åçš„ç»§ç»­å¤„ç†"""
        # æ•´ç†æºæ•°æ®
        print("\nğŸ”§ ç»§ç»­: æ•´ç†æºæ•°æ®")
        sources_df = self.organize_streams(content)
        if sources_df.empty:
            return False
        
        self.backup_data("organized_streams", sources_df)
        
        # å¦‚æœæ²¡æœ‰ä¼ å…¥æ¨¡æ¿æ•°æ®ï¼Œéœ€è¦é‡æ–°åŠ è½½
        if template_categories is None or all_template_channels is None:
            template_categories = self.load_template()
            if not template_categories:
                return False
            all_template_channels = []
            for channels in template_categories.values():
                all_template_channels.extend(channels)
        
        return self._continue_processing(sources_df, template_categories, all_template_channels)

    def _continue_processing(self, sources_df: pd.DataFrame, template_categories: Dict, all_template_channels: List[str]) -> bool:
        """ç»§ç»­å¤„ç†æµç¨‹"""
        # é¢‘é“åŒ¹é…
        print("\nğŸ¯ ç»§ç»­: é¢‘é“åŒ¹é…")
        filtered_df = self.filter_and_sort_sources(sources_df, all_template_channels)
        if filtered_df.empty:
            return False
        
        self.backup_data("matched_channels", filtered_df)
        
        # æµ‹é€Ÿ
        print("\nâš¡ ç»§ç»­: æºæµ‹é€Ÿ")
        speed_tested_df = self.speed_test_sources(filtered_df)
        if speed_tested_df.empty:
            return False
        
        # ç”Ÿæˆæœ€ç»ˆæ•°æ®
        print("\nğŸ¨ ç»§ç»­: ç”Ÿæˆæ’­æ”¾åˆ—è¡¨")
        final_data = self.generate_final_data(speed_tested_df, template_categories)
        
        # ä¿å­˜æ–‡ä»¶
        print("\nğŸ’¾ ç»§ç»­: ä¿å­˜æ–‡ä»¶")
        if not self.save_output_files(final_data):
            return False
        
        return True

    def validate_url(self, url: str) -> bool:
        """éªŒè¯URLæ ¼å¼æ˜¯å¦æ­£ç¡®"""
        if not url or not isinstance(url, str):
            return False
            
        try:
            result = urlparse(url)
            valid_scheme = result.scheme in ['http', 'https']
            valid_netloc = bool(result.netloc)
            valid_domain = '.' in result.netloc or 'localhost' in result.netloc or '[' in result.netloc
            
            return all([valid_scheme, valid_netloc, valid_domain])
        except Exception as e:
            logger.debug(f"URLè§£æå¤±è´¥: {url} - {e}")
            return False

    def fetch_streams_from_url(self, url: str, retries: int = 3) -> Optional[str]:
        """æ”¹è¿›çš„URLæ•°æ®è·å–ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶"""
        if not self.validate_url(url):
            logger.error(f"âŒ æ— æ•ˆçš„URL: {url}")
            return None
            
        # æ£€æŸ¥ç¼“å­˜
        cached_content = self.get_cached_content(url)
        if cached_content:
            return cached_content
            
        logger.info(f"ğŸ“¡ æ­£åœ¨çˆ¬å–æº: {url}")
        
        for attempt in range(retries):
            try:
                response = self.session.get(
                    url, 
                    timeout=self.config.request_timeout,
                    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                )
                response.encoding = 'utf-8'
                
                if response.status_code == 200:
                    content = response.text
                    content_length = len(content)
                    logger.info(f"âœ… æˆåŠŸè·å–æ•°æ®: {url} ({content_length} å­—ç¬¦)")
                    
                    # ç¼“å­˜å†…å®¹
                    self.set_cached_content(url, content)
                    return content
                    
                elif response.status_code == 429:  # Too Many Requests
                    wait_time = (attempt + 1) * 10  # é€’å¢ç­‰å¾…æ—¶é—´
                    logger.warning(f"âš ï¸ è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                    
                elif response.status_code in [403, 404]:
                    logger.error(f"âŒ è®¿é—®è¢«æ‹’ç»æˆ–èµ„æºä¸å­˜åœ¨: {url} (çŠ¶æ€ç : {response.status_code})")
                    return None
                    
                else:
                    logger.warning(f"âš ï¸ è·å–æ•°æ®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}ï¼Œå°è¯• {attempt + 1}/{retries}")
                    
            except requests.exceptions.Timeout:
                logger.warning(f"âš ï¸ è¯·æ±‚è¶…æ—¶ï¼Œå°è¯• {attempt + 1}/{retries}: {url}")
                
            except requests.exceptions.ConnectionError:
                logger.warning(f"âš ï¸ è¿æ¥é”™è¯¯ï¼Œå°è¯• {attempt + 1}/{retries}: {url}")
                
            except requests.exceptions.RequestException as e:
                logger.warning(f"âš ï¸ è¯·æ±‚å¼‚å¸¸ï¼Œå°è¯• {attempt + 1}/{retries}: {e}")
                
            except Exception as e:
                logger.error(f"âŒ æœªçŸ¥é”™è¯¯: {e} - {url}")
                break
                
            # ç­‰å¾…åé‡è¯•
            if attempt < retries - 1:
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        
        logger.error(f"âŒ æ‰€æœ‰é‡è¯•å¤±è´¥: {url}")
        return None

    def fetch_all_streams(self) -> str:
        """è·å–æ‰€æœ‰æºçš„æµæ•°æ®ï¼ˆä½¿ç”¨è‡ªå®šä¹‰è¿›åº¦æ¡ï¼‰"""
        logger.info("ğŸš€ å¼€å§‹æ™ºèƒ½å¤šæºæŠ“å–...")
        
        if not self.config.source_urls:
            logger.error("âŒ æ²¡æœ‰é…ç½®æºURL")
            return ""
        
        all_streams = []
        successful_sources = 0
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress = ProgressBar(total=len(self.config.source_urls), desc="ğŸŒ æŠ“å–æºæ•°æ®", unit="source")
        
        def process_future(future, url: str) -> bool:
            """å¤„ç†å•ä¸ªfutureç»“æœ"""
            nonlocal successful_sources
            try:
                content = future.result()
                if content:
                    all_streams.append(content)
                    successful_sources += 1
                    return True
            except Exception as e:
                logger.error(f"å¤„ç† {url} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
        
        with ThreadPoolExecutor(max_workers=min(5, len(self.config.source_urls))) as executor:
            # åˆ›å»ºfutureåˆ°URLçš„æ˜ å°„
            future_to_url = {}
            for url in self.config.source_urls:
                future = executor.submit(self.fetch_streams_from_url, url)
                future_to_url[future] = url
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                process_future(future, url)
                progress.update(1)
        
        progress.close()
        
        logger.info(f"âœ… æˆåŠŸè·å– {successful_sources}/{len(self.config.source_urls)} ä¸ªæºçš„æ•°æ®")
        return "\n".join(all_streams) if all_streams else ""

    def _extract_program_name(self, extinf_line: str) -> str:
        """å®Œæ•´çš„EXTINFè¡Œè§£æï¼ˆä¿®å¤è¯­æ³•é”™è¯¯ï¼‰"""
        if not extinf_line.startswith('#EXTINF'):
            return "æœªçŸ¥é¢‘é“"
        
        try:
            # æ–¹æ³•1: ä»tvg-nameå±æ€§æå–ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            tvg_match = self.patterns['tvg_name'].search(extinf_line)
            if tvg_match and tvg_match.group(1).strip():
                name = tvg_match.group(1).strip()
                if name and name != "æœªçŸ¥é¢‘é“":
                    return name
            
            # æ–¹æ³•2: ä»é€—å·åçš„å†…å®¹æå–
            content_match = self.patterns['extinf_content'].search(extinf_line)
            if content_match and content_match.group(1).strip():
                name = content_match.group(1).strip()  # ä¿®å¤è¯­æ³•é”™è¯¯
                # æ¸…ç†å¯èƒ½çš„é¢å¤–ä¿¡æ¯
                name = re.sub(r'\[.*?\]|\(.*?\)', '', name).strip()
                if name and name != "æœªçŸ¥é¢‘é“":
                    return name
            
            # æ–¹æ³•3: å°è¯•å…¶ä»–å±æ€§
            for attr_pattern in [self.patterns['tvg_id'], self.patterns['group_title']]:
                attr_match = attr_pattern.search(extinf_line)
                if attr_match and attr_match.group(1).strip():
                    name = attr_match.group(1).strip()
                    if name and name != "æœªçŸ¥é¢‘é“":
                        return name
                        
        except Exception as e:
            logger.debug(f"EXTINFè§£æé”™è¯¯: {extinf_line} - {e}")
        
        return "æœªçŸ¥é¢‘é“"

    def parse_m3u(self, content: str) -> List[Dict[str, str]]:
        """è§£æM3Uæ ¼å¼å†…å®¹"""
        if not content or not isinstance(content, str):
            return []
            
        streams = []
        lines = content.splitlines()
        current_program = None
        current_group = "é»˜è®¤åˆ†ç»„"
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            if not line:
                i += 1
                continue
                
            if line.startswith("#EXTINF"):
                # æå–é¢‘é“ä¿¡æ¯
                current_program = self._extract_program_name(line)
                
                # æå–åˆ†ç»„ä¿¡æ¯
                group_match = self.patterns['group_title'].search(line)
                if group_match:
                    current_group = group_match.group(1).strip()
                else:
                    current_group = "é»˜è®¤åˆ†ç»„"
                    
                # æŸ¥æ‰¾ä¸‹ä¸€è¡Œçš„URL
                if i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    if self.validate_url(next_line):
                        streams.append({
                            "program_name": current_program,
                            "stream_url": next_line,
                            "group": current_group,
                            "original_name": current_program
                        })
                        i += 1  # è·³è¿‡URLè¡Œ
            elif line.startswith(('http://', 'https://')):
                # ç‹¬ç«‹çš„URLè¡Œï¼ˆæ²¡æœ‰EXTINFä¿¡æ¯ï¼‰
                if self.validate_url(line):
                    streams.append({
                        "program_name": "æœªçŸ¥é¢‘é“",
                        "stream_url": line,
                        "group": "é»˜è®¤åˆ†ç»„",
                        "original_name": "æœªçŸ¥é¢‘é“"
                    })
            
            i += 1
        
        return streams

    def parse_txt(self, content: str) -> List[Dict[str, str]]:
        """è§£æTXTæ ¼å¼å†…å®¹"""
        if not content or not isinstance(content, str):
            return []
            
        streams = []
        
        for line in content.splitlines():
            line = line.strip()
            if not line or line.startswith('#') or '#genre#' in line:
                continue
            
            # æ”¯æŒå¤šç§åˆ†éš”ç¬¦æ ¼å¼
            if ',' in line:
                parts = line.split(',', 1)
                if len(parts) == 2:
                    program_name = parts[0].strip()
                    url_part = parts[1].strip()
                    
                    # ä»ç¬¬äºŒéƒ¨åˆ†æå–URL
                    url_match = self.patterns['url'].search(url_part)
                    if url_match:
                        stream_url = url_match.group()
                        if self.validate_url(stream_url):
                            streams.append({
                                "program_name": program_name,
                                "stream_url": stream_url,
                                "group": "é»˜è®¤åˆ†ç»„",
                                "original_name": program_name
                            })
            else:
                # å°è¯•ä»æ•´è¡Œæå–URL
                url_match = self.patterns['url'].search(line)
                if url_match:
                    stream_url = url_match.group()
                    program_name = line.replace(stream_url, '').strip()
                    if not program_name:
                        program_name = "æœªçŸ¥é¢‘é“"
                    
                    if self.validate_url(stream_url):
                        streams.append({
                            "program_name": program_name,
                            "stream_url": stream_url,
                            "group": "é»˜è®¤åˆ†ç»„",
                            "original_name": program_name
                        })
        
        return streams

    def organize_streams(self, content: str) -> pd.DataFrame:
        """æ•´ç†æµæ•°æ®ï¼Œå»é™¤é‡å¤å’Œæ— æ•ˆæ•°æ®"""
        if not content:
            logger.error("âŒ æ²¡æœ‰å†…å®¹å¯å¤„ç†")
            return pd.DataFrame()
            
        logger.info("ğŸ” è§£ææµæ•°æ®...")
        
        try:
            # è‡ªåŠ¨æ£€æµ‹æ ¼å¼å¹¶è§£æ
            if content.startswith("#EXTM3U"):
                streams = self.parse_m3u(content)
            else:
                streams = self.parse_txt(content)
            
            if not streams:
                logger.error("âŒ æœªèƒ½è§£æå‡ºä»»ä½•æµæ•°æ®")
                return pd.DataFrame()
                
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(streams)
            
            # æ•°æ®æ¸…ç†
            initial_count = len(df)
            
            # ç§»é™¤ç©ºå€¼
            df = df.dropna()
            
            # è¿‡æ»¤æ— æ•ˆçš„èŠ‚ç›®åç§°å’ŒURL
            df = df[df['program_name'].str.len() > 0]
            df = df[df['stream_url'].str.startswith(('http://', 'https://'))]
            
            # åº”ç”¨URLéªŒè¯
            df['url_valid'] = df['stream_url'].apply(self.validate_url)
            df = df[df['url_valid']].drop('url_valid', axis=1)
            
            # ç¡®ä¿original_nameåˆ—å­˜åœ¨
            if 'original_name' not in df.columns:
                df['original_name'] = df['program_name']
            
            # å»é‡ï¼ˆåŸºäºèŠ‚ç›®åç§°å’ŒURLï¼‰
            df = df.drop_duplicates(subset=['program_name', 'stream_url'])
            
            final_count = len(df)
            logger.info(f"ğŸ“Š æ•°æ®æ¸…ç†: {initial_count} -> {final_count} ä¸ªæµ")
            
            if final_count == 0:
                logger.warning("âš ï¸ æ•°æ®æ¸…ç†åæ²¡æœ‰æœ‰æ•ˆçš„æµæ•°æ®")
                
            return df
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¤„ç†é”™è¯¯: {e}")
            return pd.DataFrame()

    def load_template(self) -> Optional[Dict[str, List[str]]]:
        """åŠ è½½é¢‘é“æ¨¡æ¿æ–‡ä»¶"""
        template_file = Path(self.config.template_file)
        
        if not template_file.exists():
            logger.error(f"âŒ æ¨¡æ¿æ–‡ä»¶ {template_file} ä¸å­˜åœ¨")
            return None
            
        logger.info(f"ğŸ“‹ åŠ è½½æ¨¡æ¿æ–‡ä»¶: {template_file}")
        categories = {}
        current_category = None
        
        try:
            with open(template_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                        
                    # æ£€æµ‹åˆ†ç±»è¡Œ
                    category_match = self.patterns['category'].match(line)
                    if category_match:
                        current_category = category_match.group(1).strip()
                        categories[current_category] = []
                        logger.debug(f"æ‰¾åˆ°åˆ†ç±»: {current_category}")
                    
                    # æ£€æµ‹é¢‘é“è¡Œ
                    elif current_category and line and not line.startswith('#'):
                        # æå–é¢‘é“åç§°ï¼ˆå»é™¤å¯èƒ½çš„åˆ†éš”ç¬¦å’Œæ³¨é‡Šï¼‰
                        channel_name = line.split(',')[0].strip() if ',' in line else line.strip()
                        if channel_name:
                            categories[current_category].append(channel_name)
                            logger.debug(f"æ·»åŠ é¢‘é“: {channel_name} -> {current_category}")
        
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
            return None
        
        if not categories:
            logger.error("âŒ æ¨¡æ¿æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„é¢‘é“åˆ†ç±»")
            return None
            
        # ç»Ÿè®¡ä¿¡æ¯
        total_channels = sum(len(channels) for channels in categories.values())
        logger.info(f"ğŸ“ æ¨¡æ¿åˆ†ç±»: {list(categories.keys())}")
        logger.info(f"ğŸ“º æ¨¡æ¿é¢‘é“æ€»æ•°: {total_channels}")
        
        return categories

    def clean_channel_name(self, name: str) -> str:
        """æ”¹è¿›çš„é¢‘é“åç§°æ¸…ç†"""
        if not name:
            return ""
        
        try:
            # ä¿ç•™å…³é”®ä¿¡æ¯ï¼šä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ç©ºæ ¼ã€æ¨ªæ 
            cleaned = re.sub(r'[^\w\u4e00-\u9fa5\s-]', '', name.lower())
            # åˆå¹¶å¤šä¸ªç©ºæ ¼
            cleaned = re.sub(r'\s+', ' ', cleaned).strip()
            # ç§»é™¤å¸¸è§çš„æ— æ„ä¹‰åç¼€
            cleaned = re.sub(r'\s+(hd|fhd|4k|ç›´æ’­|é¢‘é“|tv|television)$', '', cleaned)
            return cleaned
        except Exception as e:
            logger.debug(f"é¢‘é“åç§°æ¸…ç†é”™è¯¯: {name} - {e}")
            return name.lower() if name else ""

    def similarity_score(self, str1: str, str2: str) -> int:
        """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-100ï¼‰"""
        if not str1 or not str2 or not isinstance(str1, str) or not isinstance(str2, str):
            return 0
            
        try:
            # æ¸…ç†å­—ç¬¦ä¸²
            clean_str1 = self.clean_channel_name(str1)
            clean_str2 = self.clean_channel_name(str2)
            
            if not clean_str1 or not clean_str2:
                return 0
            
            # å®Œå…¨åŒ¹é…
            if clean_str1 == clean_str2:
                return 100
            
            # åŒ…å«å…³ç³»ï¼ˆåŒå‘ï¼‰
            if clean_str1 in clean_str2:
                return 90
            if clean_str2 in clean_str1:
                return 85
            
            # ä½¿ç”¨é›†åˆè®¡ç®—Jaccardç›¸ä¼¼åº¦
            set1 = set(clean_str1)
            set2 = set(clean_str2)
            
            intersection = len(set1 & set2)
            union = len(set1 | set2)
            
            if union > 0:
                jaccard_similarity = intersection / union
                return int(jaccard_similarity * 80)
                
        except Exception as e:
            logger.debug(f"ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {str1}, {str2} - {e}")
        
        return 0

    def filter_and_sort_sources(self, sources_df: pd.DataFrame, template_channels: List[str]) -> pd.DataFrame:
        """å®Œæ•´çš„é¢‘é“åŒ¹é…å’Œæºç­›é€‰å®ç°"""
        logger.info("ğŸ¯ å¼€å§‹é¢‘é“åŒ¹é…å’Œæºç­›é€‰...")
        
        # ä¸¥æ ¼çš„ç©ºå€¼æ£€æŸ¥
        if sources_df is None or sources_df.empty:
            logger.error("âŒ æºæ•°æ®ä¸ºç©ºæˆ–Noneï¼Œæ— æ³•è¿›è¡ŒåŒ¹é…")
            return pd.DataFrame()
        
        if template_channels is None or not template_channels:
            logger.error("âŒ æ¨¡æ¿é¢‘é“åˆ—è¡¨ä¸ºç©ºæˆ–None")
            return pd.DataFrame()
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['program_name', 'stream_url']
        missing_columns = [col for col in required_columns if col not in sources_df.columns]
        if missing_columns:
            logger.error(f"âŒ æºæ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            return pd.DataFrame()
        
        # åˆ›å»ºåŒ¹é…ç»“æœåˆ—è¡¨
        matched_results = []
        
        logger.info(f"å¼€å§‹åŒ¹é… {len(template_channels)} ä¸ªæ¨¡æ¿é¢‘é“...")
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress = ProgressBar(total=len(template_channels), desc="ğŸ” é¢‘é“åŒ¹é…", unit="channel")
        
        for template_channel in template_channels:
            best_match_row = None
            best_score = 0
            
            # ä¸ºæ¯ä¸ªæ¨¡æ¿é¢‘é“æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„æº
            for _, source_row in sources_df.iterrows():
                source_channel = source_row['program_name']
                score = self.similarity_score(template_channel, source_channel)
                
                if score > best_score and score >= self.config.similarity_threshold:
                    best_score = score
                    best_match_row = source_row.copy()
                    best_match_row['template_channel'] = template_channel
                    best_match_row['match_score'] = score
            
            if best_match_row is not None:
                matched_results.append(best_match_row)
            
            progress.update(1)
        
        progress.close()
        
        # è½¬æ¢ä¸ºDataFrameå¹¶æ•´åˆæ•°æ®
        if matched_results:
            result_df = pd.DataFrame(matched_results)
            
            # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
            required_columns = ['program_name', 'stream_url', 'template_channel', 'match_score']
            for col in required_columns:
                if col not in result_df.columns:
                    logger.warning(f"ç¼ºå¤±åˆ—: {col}ï¼Œä½¿ç”¨é»˜è®¤å€¼å¡«å……")
                    if col == 'template_channel':
                        result_df[col] = result_df.get('program_name', 'æœªçŸ¥é¢‘é“')
                    elif col == 'match_score':
                        result_df[col] = 0
            
            # é‡å‘½ååˆ—ä»¥æ˜ç¡®å«ä¹‰
            column_mapping = {
                'program_name': 'original_name',
                'template_channel': 'program_name'
            }
            result_df = result_df.rename(columns=column_mapping)
            
            # ç¡®ä¿original_nameåˆ—å­˜åœ¨
            if 'original_name' not in result_df.columns:
                result_df['original_name'] = result_df.get('program_name', 'æœªçŸ¥é¢‘é“')
            
            # é‡æ–°æ’åˆ—åˆ—é¡ºåº
            preferred_order = ['program_name', 'original_name', 'stream_url', 'match_score', 'group']
            available_columns = [col for col in preferred_order if col in result_df.columns]
            other_columns = [col for col in result_df.columns if col not in preferred_order]
            result_df = result_df[available_columns + other_columns]
            
            # æ˜¾ç¤ºåŒ¹é…ç»“æœç»Ÿè®¡
            unique_matched_channels = result_df['program_name'].nunique()
            logger.info(f"âœ… é¢‘é“åŒ¹é…å®Œæˆ: {len(matched_results)} ä¸ªæµåŒ¹é…åˆ° {unique_matched_channels} ä¸ªæ¨¡æ¿é¢‘é“")
            
            # æ˜¾ç¤ºæœ€ä½³åŒ¹é…ç»“æœ
            if not result_df.empty:
                top_matches = result_df.nlargest(10, 'match_score')[['program_name', 'original_name', 'match_score']]
                print("\nğŸ“Š æœ€ä½³åŒ¹é…ç»“æœï¼ˆå‰10ä¸ªï¼‰:")
                for _, match in top_matches.iterrows():
                    print(f"  âœ… {match['program_name']:<20} <- {match['original_name']:<30} (åˆ†æ•°: {match['match_score']:2d})")
            
            return result_df
        else:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çš„é¢‘é“")
            return pd.DataFrame()

    def speed_test_ffmpeg(self, stream_url: str) -> Tuple[bool, float]:
        """ä½¿ç”¨FFmpegè¿›è¡Œæµåª’ä½“æµ‹é€Ÿï¼ˆå¢å¼ºå¼‚å¸¸å¤„ç†ï¼‰"""
        if not self.ffmpeg_available or not stream_url:
            return False, float('inf')
            
        temp_file = Path(self.config.temp_dir) / f'test_{abs(hash(stream_url))}.ts'
        
        try:
            cmd = [
                'ffmpeg',
                '-y',  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                '-timeout', '10000000',  # 10ç§’è¶…æ—¶ï¼ˆå¾®ç§’ï¼‰
                '-i', stream_url,
                '-t', '10',  # æµ‹è¯•10ç§’ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»“æœ
                '-c', 'copy',
                '-f', 'mpegts',
                '-max_muxing_queue_size', '1024',
                str(temp_file)
            ]
            
            start_time = time.time()
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                timeout=15,  # æ€»è¶…æ—¶15ç§’ï¼ˆåŒ…æ‹¬å¯åŠ¨æ—¶é—´ï¼‰
                check=False
            )
            end_time = time.time()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file.exists():
                try:
                    temp_file.unlink()
                except:
                    pass
            
            if result.returncode == 0:
                speed = end_time - start_time
                logger.debug(f"âœ… FFmpegæµ‹é€ŸæˆåŠŸ: {stream_url} - è€—æ—¶: {speed:.2f}ç§’")
                return True, speed
            else:
                # åˆ†æé”™è¯¯åŸå› 
                error_output = result.stderr.lower() if result.stderr else ""
                if "connection refused" in error_output:
                    logger.debug(f"âŒ FFmpegæµ‹é€Ÿå¤±è´¥: è¿æ¥è¢«æ‹’ç» - {stream_url}")
                elif "timeout" in error_output:
                    logger.debug(f"â° FFmpegæµ‹é€Ÿå¤±è´¥: è¶…æ—¶ - {stream_url}")
                elif "404" in error_output:
                    logger.debug(f"âŒ FFmpegæµ‹é€Ÿå¤±è´¥: èµ„æºä¸å­˜åœ¨ - {stream_url}")
                else:
                    logger.debug(f"âŒ FFmpegæµ‹é€Ÿå¤±è´¥ï¼Œè¿”å›ç : {result.returncode} - {stream_url}")
                return False, float('inf')
                
        except subprocess.TimeoutExpired:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file.exists():
                try:
                    temp_file.unlink()
                except:
                    pass
            logger.debug(f"â° FFmpegæµ‹é€Ÿè¶…æ—¶: {stream_url}")
            return False, float('inf')
        except FileNotFoundError:
            logger.debug(f"âŒ FFmpegæµ‹é€Ÿå¤±è´¥: FFmpegæœªæ‰¾åˆ° - {stream_url}")
            return False, float('inf')
        except PermissionError:
            logger.debug(f"âŒ FFmpegæµ‹é€Ÿå¤±è´¥: æƒé™ä¸è¶³ - {stream_url}")
            return False, float('inf')
        except Exception as e:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file.exists():
                try:
                    temp_file.unlink()
                except:
                    pass
            logger.debug(f"âŒ FFmpegæµ‹é€Ÿå¼‚å¸¸: {stream_url} - {e}")
            return False, float('inf')

    def speed_test_simple(self, stream_url: str) -> Tuple[bool, float]:
        """ç®€å•çš„HTTPæµ‹é€Ÿï¼ˆä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´ï¼‰"""
        if not stream_url:
            return False, float('inf')
            
        try:
            start_time = time.time()
            response = self.session.head(
                stream_url, 
                timeout=self.config.speed_test_timeout,  # ä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´
                allow_redirects=True
            )
            end_time = time.time()
            
            if response.status_code in [200, 302, 301, 307]:
                speed = end_time - start_time
                logger.debug(f"âœ… HTTPæµ‹é€ŸæˆåŠŸ: {stream_url} - è€—æ—¶: {speed:.2f}ç§’")
                return True, speed
            else:
                logger.debug(f"âŒ HTTPæµ‹é€Ÿå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code} - {stream_url}")
                return False, float('inf')
        except Exception as e:
            logger.debug(f"âŒ HTTPæµ‹é€Ÿå¼‚å¸¸: {stream_url} - {e}")
            return False, float('inf')

    def speed_test_sources(self, sources_df: pd.DataFrame) -> pd.DataFrame:
        """å®Œæ•´çš„æµ‹é€Ÿå®ç°"""
        logger.info("â±ï¸  å¼€å§‹æ™ºèƒ½æµ‹é€Ÿï¼ˆFFmpeg: 10ç§’æµ‹è¯•ï¼Œ10ç§’è¶…æ—¶ï¼‰...")
        
        if sources_df is None or sources_df.empty:
            logger.error("âŒ æ²¡æœ‰éœ€è¦æµ‹é€Ÿçš„æº")
            return pd.DataFrame()
            
        results = []
        total_sources = len(sources_df)
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress = ProgressBar(total=total_sources, desc="âš¡ æµ‹é€Ÿè¿›åº¦", unit="source")
        
        def test_single_source(row) -> Dict[str, Any]:
            """æµ‹è¯•å•ä¸ªæºçš„è¾…åŠ©å‡½æ•°"""
            try:
                program_name = row['program_name']
                stream_url = row['stream_url']
                
                # æ ¹æ®URLç±»å‹é€‰æ‹©æµ‹é€Ÿæ–¹æ³•
                if any(ext in stream_url.lower() for ext in ['.m3u8', '.ts', '.flv', '.mp4']):
                    if self.ffmpeg_available:
                        accessible, speed = self.speed_test_ffmpeg(stream_url)
                    else:
                        accessible, speed = self.speed_test_simple(stream_url)
                else:
                    accessible, speed = self.speed_test_simple(stream_url)
                
                return {
                    'program_name': program_name,
                    'stream_url': stream_url,
                    'accessible': accessible,
                    'speed': speed,
                    'original_name': row.get('original_name', ''),
                    'match_score': row.get('match_score', 0)
                }
            except Exception as e:
                logger.error(f"æµ‹é€Ÿå•ä¸ªæºæ—¶å‡ºé”™: {e}")
                return {
                    'program_name': row.get('program_name', 'æœªçŸ¥'),
                    'stream_url': row.get('stream_url', ''),
                    'accessible': False,
                    'speed': float('inf'),
                    'original_name': row.get('original_name', ''),
                    'match_score': row.get('match_score', 0)
                }
        
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            futures = [executor.submit(test_single_source, row) for _, row in sources_df.iterrows()]
            
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=20)  # å•ä¸ªä»»åŠ¡è¶…æ—¶20ç§’
                    results.append(result)
                    
                    # æ˜¾ç¤ºæµ‹é€ŸçŠ¶æ€
                    status = "âœ…" if result['accessible'] else "âŒ"
                    speed_display = f"{result['speed']:.2f}s" if result['accessible'] else "è¶…æ—¶"
                    
                    progress.update(1)
                except Exception as e:
                    logger.error(f"æµ‹é€Ÿå¼‚å¸¸: {e}")
                    progress.update(1)
        
        progress.close()
        
        # è½¬æ¢ä¸ºDataFrameå¹¶æ•´åˆç»“æœ
        try:
            result_df = pd.DataFrame(results)
            if result_df.empty:
                return pd.DataFrame()
            
            # è¿‡æ»¤å¯è®¿é—®çš„æºå¹¶æŒ‰é€Ÿåº¦æ’åº
            accessible_df = result_df[result_df['accessible']].copy()
            accessible_df = accessible_df.sort_values(['program_name', 'speed'])
            
            accessible_count = len(accessible_df)
            
            # ç»Ÿè®¡æµ‹é€Ÿç»“æœ
            fast_sources = len(accessible_df[accessible_df['speed'] < 3])  # 3ç§’å†…å“åº”
            medium_sources = len(accessible_df[(accessible_df['speed'] >= 3) & (accessible_df['speed'] < 8)])  # 3-8ç§’
            slow_sources = len(accessible_df[accessible_df['speed'] >= 8])  # 8ç§’ä»¥ä¸Š
            
            logger.info(f"ğŸ“Š æµ‹é€Ÿå®Œæˆ: {accessible_count}/{total_sources} ä¸ªæºå¯ç”¨")
            logger.info(f"ğŸš€ å¿«é€Ÿæº(<3s): {fast_sources}ä¸ª, ğŸ¢ ä¸­é€Ÿæº(3-8s): {medium_sources}ä¸ª, ğŸŒ æ…¢é€Ÿæº(>8s): {slow_sources}ä¸ª")
            
            if accessible_count == 0:
                logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„æºé€šè¿‡æµ‹é€Ÿ")
                
            return accessible_df
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æµ‹é€Ÿç»“æœæ—¶å‡ºé”™: {e}")
            return pd.DataFrame()

    def generate_final_data(self, speed_tested_df: pd.DataFrame, template_categories: Dict[str, List[str]]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæ•°æ®ï¼ˆä½¿ç”¨è‡ªå®šä¹‰è¿›åº¦æ¡ï¼‰"""
        logger.info("ğŸ¨ ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶...")
        
        final_data = {}
        total_sources = 0
        
        if speed_tested_df is None or speed_tested_df.empty:
            logger.error("âŒ æµ‹é€Ÿæ•°æ®ä¸ºç©º")
            return final_data
        
        if not template_categories:
            logger.error("âŒ æ¨¡æ¿åˆ†ç±»ä¸ºç©º")
            return final_data
        
        # è®¡ç®—æ€»é¢‘é“æ•°
        total_channels = sum(len(channels) for channels in template_categories.values())
        
        if total_channels == 0:
            logger.error("âŒ æ¨¡æ¿ä¸­æ²¡æœ‰é¢‘é“")
            return final_data
        
        logger.info(f"ä¸º {len(template_categories)} ä¸ªåˆ†ç±»ç”Ÿæˆæœ€ç»ˆæ•°æ®...")
        
        # åˆ›å»ºè¿›åº¦æ¡
        progress = ProgressBar(total=total_channels, desc="ğŸ“¦ ç”Ÿæˆæ•°æ®", unit="channel")
        
        for category, channels in template_categories.items():
            final_data[category] = {}
            
            for channel in channels:
                # è·å–è¯¥é¢‘é“çš„æ‰€æœ‰æº
                channel_sources = speed_tested_df[speed_tested_df['program_name'] == channel]
                
                if not channel_sources.empty:
                    # æŒ‰é€Ÿåº¦æ’åºå¹¶å–å‰Nä¸ª
                    sorted_sources = channel_sources.head(self.config.max_sources_per_channel)
                    final_data[category][channel] = sorted_sources[['stream_url', 'speed']].to_dict('records')
                    source_count = len(sorted_sources)
                    total_sources += source_count
                else:
                    final_data[category][channel] = []
                
                progress.update(1)
        
        progress.close()
        
        logger.info(f"ğŸ“¦ æ€»å…±æ”¶é›†åˆ° {total_sources} ä¸ªæœ‰æ•ˆæº")
        return final_data

    def save_output_files(self, final_data: Dict[str, Any]) -> bool:
        """ä¿å­˜è¾“å‡ºæ–‡ä»¶ï¼ˆä½¿ç”¨è‡ªå®šä¹‰è¿›åº¦æ¡ï¼‰"""
        logger.info("ğŸ’¾ ä¿å­˜æ–‡ä»¶...")
        
        if not final_data:
            logger.error("âŒ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return False
        
        # è®¡ç®—æ€»è¡Œæ•°ç”¨äºè¿›åº¦æ¡
        total_lines = 0
        for category, channels in final_data.items():
            total_lines += 1  # åˆ†ç±»è¡Œ
            for channel, sources in channels.items():
                total_lines += len(sources)  # é¢‘é“è¡Œ
            total_lines += 1  # ç©ºè¡Œ
        
        success_count = 0
        
        # ä¿å­˜TXTæ ¼å¼
        try:
            print("ä¿å­˜TXTæ–‡ä»¶...")
            progress = ProgressBar(total=total_lines, desc="ğŸ“„ ä¿å­˜TXT", unit="line")
            
            with open(self.config.output_txt, 'w', encoding='utf-8') as f:
                for category, channels in final_data.items():
                    f.write(f"{category},#genre#\n")
                    progress.update(1)
                    
                    for channel, sources in channels.items():
                        for source in sources:
                            f.write(f"{channel},{source['stream_url']}\n")
                            progress.update(1)
                    
                    f.write("\n")
                    progress.update(1)
            
            progress.close()
            success_count += 1
            logger.info(f"âœ… TXTæ–‡ä»¶å·²ä¿å­˜: {Path(self.config.output_txt).absolute()}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜TXTæ–‡ä»¶å¤±è´¥: {e}")
            return False
        
        # ä¿å­˜M3Uæ ¼å¼
        try:
            print("ä¿å­˜M3Uæ–‡ä»¶...")
            progress = ProgressBar(total=total_lines, desc="ğŸ“„ ä¿å­˜M3U", unit="line")
            
            with open(self.config.output_m3u, 'w', encoding='utf-8') as f:
                f.write("#EXTM3U\n")
                progress.update(1)
                
                for category, channels in final_data.items():
                    for channel, sources in channels.items():
                        for source in sources:
                            f.write(f'#EXTINF:-1 tvg-name="{channel}" group-title="{category}",{channel}\n')
                            f.write(f"{source['stream_url']}\n")
                            progress.update(2)
            
            progress.close()
            success_count += 1
            logger.info(f"âœ… M3Uæ–‡ä»¶å·²ä¿å­˜: {Path(self.config.output_m3u).absolute()}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜M3Uæ–‡ä»¶å¤±è´¥: {e}")
            return False
            
        return success_count == 2  # ä¸¤ä¸ªæ–‡ä»¶éƒ½ä¿å­˜æˆåŠŸ

    def validate_output_files(self) -> Dict[str, Any]:
        """å®Œæ•´çš„æ•°æ®éªŒè¯"""
        validation_result = {
            'txt_file': {'exists': False, 'categories': 0, 'sources': 0, 'valid': False, 'errors': [], 'warnings': []},
            'm3u_file': {'exists': False, 'channels': 0, 'sources': 0, 'valid': False, 'errors': [], 'warnings': []},
            'content_validation': {'valid_urls': 0, 'invalid_urls': 0, 'duplicate_channels': 0, 'empty_categories': 0},
            'overall_valid': False
        }
        
        try:
            # éªŒè¯TXTæ–‡ä»¶
            txt_path = Path(self.config.output_txt)
            if txt_path.exists():
                validation_result['txt_file']['exists'] = True
                with open(txt_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    lines = content.strip().split('\n')
                    
                    # ç»Ÿè®¡å’ŒéªŒè¯
                    categories = [line for line in lines if line.endswith(',#genre#')]
                    sources = [line for line in lines if line and not line.endswith(',#genre#') and ',' in line]
                    
                    validation_result['txt_file']['categories'] = len(categories)
                    validation_result['txt_file']['sources'] = len(sources)
                    
                    # æ£€æŸ¥ç©ºåˆ†ç±»
                    current_category = None
                    category_sources = {}
                    for line in lines:
                        if line.endswith(',#genre#'):
                            current_category = line.replace(',#genre#', '').strip()
                            category_sources[current_category] = 0
                        elif line and ',' in line and current_category:
                            category_sources[current_category] = category_sources.get(current_category, 0) + 1
                    
                    empty_categories = [cat for cat, count in category_sources.items() if count == 0]
                    validation_result['content_validation']['empty_categories'] = len(empty_categories)
                    if empty_categories:
                        validation_result['txt_file']['warnings'].extend(
                            [f"ç©ºåˆ†ç±»: {category}" for category in empty_categories]
                        )
                    
                    # å†…å®¹éªŒè¯
                    for line in sources:
                        if ',' in line:
                            channel, url = line.split(',', 1)
                            if not self.validate_url(url.strip()):
                                validation_result['content_validation']['invalid_urls'] += 1
                                validation_result['txt_file']['errors'].append(f"æ— æ•ˆURL: {url}")
                            else:
                                validation_result['content_validation']['valid_urls'] += 1
                    
                    validation_result['txt_file']['valid'] = (
                        len(sources) > 0 and 
                        len(categories) > 0 and
                        validation_result['content_validation']['invalid_urls'] == 0
                    )
            
            # éªŒè¯M3Uæ–‡ä»¶
            m3u_path = Path(self.config.output_m3u)
            if m3u_path.exists():
                validation_result['m3u_file']['exists'] = True
                with open(m3u_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    lines = content.strip().split('\n')
                    
                    extinf_lines = [line for line in lines if line.startswith('#EXTINF')]
                    url_lines = [line for line in lines if line.startswith(('http://', 'https://'))]
                    
                    validation_result['m3u_file']['channels'] = len(extinf_lines)
                    validation_result['m3u_file']['sources'] = len(url_lines)
                    
                    # æ£€æŸ¥EXTINFå’ŒURLè¡Œæ•°åŒ¹é…
                    if len(extinf_lines) != len(url_lines):
                        validation_result['m3u_file']['errors'].append(
                            f"EXTINFè¡Œæ•°({len(extinf_lines)})ä¸URLè¡Œæ•°({len(url_lines)})ä¸åŒ¹é…"
                        )
                    
                    # éªŒè¯EXTINFæ ¼å¼
                    for i, extinf_line in enumerate(extinf_lines):
                        if 'tvg-name=' not in extinf_line:
                            validation_result['m3u_file']['warnings'].append(
                                f"ç¬¬{i+1}ä¸ªEXTINFç¼ºå°‘tvg-nameå±æ€§"
                            )
                        if 'group-title=' not in extinf_line:
                            validation_result['m3u_file']['warnings'].append(
                                f"ç¬¬{i+1}ä¸ªEXTINFç¼ºå°‘group-titleå±æ€§"
                            )
                    
                    # éªŒè¯URLæ ¼å¼
                    for url_line in url_lines:
                        if not self.validate_url(url_line.strip()):
                            validation_result['content_validation']['invalid_urls'] += 1
                            validation_result['m3u_file']['errors'].append(f"æ— æ•ˆURL: {url_line}")
                    
                    validation_result['m3u_file']['valid'] = (
                        len(extinf_lines) == len(url_lines) and 
                        len(url_lines) > 0 and
                        content.startswith('#EXTM3U') and
                        validation_result['content_validation']['invalid_urls'] == 0
                    )
            
            # æ£€æŸ¥é‡å¤é¢‘é“
            if validation_result['txt_file']['valid']:
                channels = {}
                with open(txt_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip() and not line.endswith(',#genre#') and ',' in line:
                            channel = line.split(',')[0].strip()
                            channels[channel] = channels.get(channel, 0) + 1
                
                duplicate_channels = {k: v for k, v in channels.items() if v > 1}
                validation_result['content_validation']['duplicate_channels'] = len(duplicate_channels)
                if duplicate_channels:
                    validation_result['txt_file']['warnings'].extend(
                        [f"é‡å¤é¢‘é“: {channel} (å‡ºç°{count}æ¬¡)" for channel, count in duplicate_channels.items()]
                    )
            
            # æ€»ä½“éªŒè¯
            validation_result['overall_valid'] = (
                validation_result['txt_file']['valid'] and 
                validation_result['m3u_file']['valid'] and
                validation_result['content_validation']['invalid_urls'] == 0
            )
            
            logger.info("âœ… è¾“å‡ºæ–‡ä»¶éªŒè¯å®Œæˆ")
            return validation_result
            
        except Exception as e:
            logger.error(f"âŒ è¾“å‡ºæ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
            validation_result['error'] = str(e)
            return validation_result

    def generate_integrity_report(self, final_data: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®å®Œæ•´æ€§æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'overall_status': 'unknown',
            'categories_analysis': {},
            'channel_coverage': {},
            'data_quality': {},
            'recommendations': []
        }
        
        try:
            if not final_data:
                report['overall_status'] = 'empty'
                report['recommendations'].append('æœ€ç»ˆæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æºæ•°æ®å’Œæ¨¡æ¿åŒ¹é…')
                return report
            
            total_categories = len(final_data)
            total_channels = 0
            total_sources = 0
            channels_with_sources = 0
            
            # åˆ†ææ¯ä¸ªåˆ†ç±»
            for category, channels in final_data.items():
                category_channels = 0
                category_sources = 0
                category_channels_with_sources = 0
                
                for channel, sources in channels.items():
                    total_channels += 1
                    category_channels += 1
                    
                    if sources:
                        total_sources += len(sources)
                        category_sources += len(sources)
                        channels_with_sources += 1
                        category_channels_with_sources += 1
                
                # åˆ†ç±»åˆ†æ
                report['categories_analysis'][category] = {
                    'channels_total': category_channels,
                    'channels_with_sources': category_channels_with_sources,
                    'sources_total': category_sources,
                    'coverage_rate': round(category_channels_with_sources / category_channels * 100, 2) if category_channels > 0 else 0
                }
            
            # æ€»ä½“è¦†ç›–ç‡
            coverage_rate = round(channels_with_sources / total_channels * 100, 2) if total_channels > 0 else 0
            avg_sources_per_channel = round(total_sources / channels_with_sources, 2) if channels_with_sources > 0 else 0
            
            report['channel_coverage'] = {
                'total_categories': total_categories,
                'total_channels': total_channels,
                'channels_with_sources': channels_with_sources,
                'coverage_rate': coverage_rate,
                'total_sources': total_sources,
                'avg_sources_per_channel': avg_sources_per_channel
            }
            
            # æ•°æ®è´¨é‡è¯„ä¼°
            if coverage_rate >= 80:
                report['overall_status'] = 'excellent'
            elif coverage_rate >= 60:
                report['overall_status'] = 'good'
            elif coverage_rate >= 40:
                report['overall_status'] = 'fair'
            else:
                report['overall_status'] = 'poor'
            
            # æ•°æ®è´¨é‡æŒ‡æ ‡
            report['data_quality'] = {
                'coverage_score': coverage_rate,
                'source_diversity_score': min(100, avg_sources_per_channel * 20),  # æ¯ä¸ªé¢‘é“5ä¸ªæºå¾—100åˆ†
                'category_balance_score': min(100, (total_categories / 10) * 100)  # 10ä¸ªåˆ†ç±»å¾—100åˆ†
            }
            
            # ç”Ÿæˆå»ºè®®
            if coverage_rate < 50:
                report['recommendations'].append('é¢‘é“è¦†ç›–ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ æºURLæˆ–è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼')
            if avg_sources_per_channel < 2:
                report['recommendations'].append('å¹³å‡æºæ•°é‡è¾ƒå°‘ï¼Œå»ºè®®å¢åŠ æºURLæˆ–è°ƒæ•´æœ€å¤§æºæ•°é™åˆ¶')
            if total_categories < 3:
                report['recommendations'].append('åˆ†ç±»æ•°é‡è¾ƒå°‘ï¼Œå»ºè®®å®Œå–„æ¨¡æ¿æ–‡ä»¶')
            
            if not report['recommendations']:
                report['recommendations'].append('æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ç‰¹æ®Šè°ƒæ•´')
            
            logger.info("âœ… å®Œæ•´æ€§æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            return report
            
        except Exception as e:
            logger.error(f"âŒ å®Œæ•´æ€§æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            report['overall_status'] = 'error'
            report['error'] = str(e)
            return report

    def print_statistics(self, final_data: Dict[str, Any]):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*50)
        print("ğŸ“ˆ ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š")
        print("="*50)
        
        if not final_data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ç»Ÿè®¡")
            return
        
        total_channels = 0
        total_sources = 0
        categories_with_sources = 0
        
        for category, channels in final_data.items():
            category_channels = 0
            category_sources = 0
            
            for channel, sources in channels.items():
                if sources:
                    category_channels += 1
                    category_sources += len(sources)
            
            if category_channels > 0:
                categories_with_sources += 1
                print(f"  ğŸ“º {category}: {category_channels}é¢‘é“, {category_sources}æº")
                total_channels += category_channels
                total_sources += category_sources
        
        print("-"*50)
        print(f"ğŸ“Š æ€»è®¡: {total_channels}é¢‘é“, {total_sources}æº")
        print(f"ğŸ“ æœ‰æ•ˆåˆ†ç±»: {categories_with_sources}/{len(final_data)}")
        
        # ç»Ÿè®¡æ— æºçš„é¢‘é“
        no_source_channels = []
        for category, channels in final_data.items():
            for channel, sources in channels.items():
                if not sources:
                    no_source_channels.append(f"{category}-{channel}")
        
        if no_source_channels:
            print(f"âš ï¸  æ— æºé¢‘é“: {len(no_source_channels)}ä¸ª")
            if len(no_source_channels) <= 10:
                for channel in no_source_channels[:10]:
                    print(f"    âŒ {channel}")
            if len(no_source_channels) > 10:
                print(f"    ... è¿˜æœ‰ {len(no_source_channels) - 10} ä¸ªæ— æºé¢‘é“")

    def verify_cleanup(self) -> Dict[str, Any]:
        """éªŒè¯èµ„æºæ˜¯å¦å®Œå…¨æ¸…ç†"""
        verification = {
            'temp_dir_clean': False,
            'cache_size': 0,
            'backup_files': 0,
            'overall_clean': False
        }
        
        try:
            # æ£€æŸ¥ä¸´æ—¶ç›®å½•
            temp_dir = Path(self.config.temp_dir)
            if temp_dir.exists():
                temp_files = list(temp_dir.iterdir())
                verification['temp_dir_clean'] = len(temp_files) == 0
                verification['temp_files_remaining'] = len(temp_files)
            else:
                verification['temp_dir_clean'] = True
            
            # æ£€æŸ¥ç¼“å­˜ç›®å½•å¤§å°
            if self.cache_dir.exists():
                cache_files = list(self.cache_dir.glob("*.cache"))
                verification['cache_size'] = len(cache_files)
            
            # æ£€æŸ¥å¤‡ä»½æ–‡ä»¶
            if self.backup_dir.exists():
                backup_files = list(self.backup_dir.glob("backup_*.json"))
                verification['backup_files'] = len(backup_files)
            
            # æ€»ä½“æ¸…ç†çŠ¶æ€
            verification['overall_clean'] = (
                verification['temp_dir_clean'] and 
                verification['cache_size'] <= 50  # å…è®¸ä¸€å®šæ•°é‡çš„ç¼“å­˜æ–‡ä»¶
            )
            
            logger.info("âœ… èµ„æºæ¸…ç†éªŒè¯å®Œæˆ")
            return verification
            
        except Exception as e:
            logger.error(f"âŒ èµ„æºæ¸…ç†éªŒè¯å¤±è´¥: {e}")
            verification['error'] = str(e)
            return verification

    def cleanup(self):
        """å®Œæ•´çš„æ¸…ç†å·¥ä½œ"""
        try:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            temp_dir = Path(self.config.temp_dir)
            if temp_dir.exists():
                temp_files_cleaned = 0
                for file in temp_dir.iterdir():
                    if file.is_file():
                        try:
                            file.unlink()
                            temp_files_cleaned += 1
                            logger.debug(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file}")
                        except Exception as e:
                            logger.debug(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {file} - {e}")
                
                if temp_files_cleaned > 0:
                    logger.info(f"âœ… æ¸…ç†äº† {temp_files_cleaned} ä¸ªä¸´æ—¶æ–‡ä»¶")
            
            # æ¸…ç†è¿‡æœŸç¼“å­˜
            if self.config.cache_enabled and self.cache_dir.exists():
                current_time = time.time()
                cache_files_cleaned = 0
                for cache_file in self.cache_dir.iterdir():
                    if cache_file.is_file() and cache_file.suffix == '.cache':
                        try:
                            with open(cache_file, 'r', encoding='utf-8') as f:
                                cache_data = json.load(f)
                            
                            cache_time = cache_data.get('timestamp', 0)
                            if current_time - cache_time > self.config.cache_expiry:
                                cache_file.unlink()
                                cache_files_cleaned += 1
                                logger.debug(f"åˆ é™¤è¿‡æœŸç¼“å­˜: {cache_file}")
                        except Exception as e:
                            logger.debug(f"å¤„ç†ç¼“å­˜æ–‡ä»¶å¤±è´¥: {cache_file} - {e}")
                
                if cache_files_cleaned > 0:
                    logger.info(f"âœ… æ¸…ç†äº† {cache_files_cleaned} ä¸ªè¿‡æœŸç¼“å­˜æ–‡ä»¶")
        
        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

    def create_demo_template(self) -> bool:
        """åˆ›å»ºç¤ºä¾‹æ¨¡æ¿æ–‡ä»¶"""
        demo_content = """# IPTVé¢‘é“æ¨¡æ¿æ–‡ä»¶
# æ ¼å¼: åˆ†ç±»åç§°,#genre#
#        é¢‘é“åç§°1
#        é¢‘é“åç§°2

å¤®è§†é¢‘é“,#genre#
CCTV-1 ç»¼åˆ
CCTV-2 è´¢ç»
CCTV-3 ç»¼è‰º
CCTV-4 ä¸­æ–‡å›½é™…
CCTV-5 ä½“è‚²
CCTV-6 ç”µå½±
CCTV-7 å›½é˜²å†›äº‹
CCTV-8 ç”µè§†å‰§
CCTV-9 çºªå½•
CCTV-10 ç§‘æ•™
CCTV-11 æˆæ›²
CCTV-12 ç¤¾ä¼šä¸æ³•
CCTV-13 æ–°é—»
CCTV-14 å°‘å„¿
CCTV-15 éŸ³ä¹

å«è§†é¢‘é“,#genre#
æ¹–å—å«è§†
æµ™æ±Ÿå«è§†
æ±Ÿè‹å«è§†
ä¸œæ–¹å«è§†
åŒ—äº¬å«è§†
å¤©æ´¥å«è§†
å±±ä¸œå«è§†
å¹¿ä¸œå«è§†
æ·±åœ³å«è§†

åœ°æ–¹é¢‘é“,#genre#
åŒ—äº¬æ–°é—»
ä¸Šæµ·æ–°é—»
å¹¿å·ç»¼åˆ
é‡åº†å«è§†
æˆéƒ½æ–°é—»

é«˜æ¸…é¢‘é“,#genre#
CCTV-1 HD
CCTV-5+ HD
æ¹–å—å«è§† HD
æµ™æ±Ÿå«è§† HD
"""
        try:
            with open(self.config.template_file, 'w', encoding='utf-8') as f:
                f.write(demo_content)
            logger.info(f"âœ… å·²åˆ›å»ºç¤ºä¾‹æ¨¡æ¿æ–‡ä»¶: {Path(self.config.template_file).absolute()}")
            return True
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def run(self):
        """å®Œæ•´çš„ä¸»è¿è¡Œå‡½æ•°"""
        print("=" * 60)
        print("ğŸ¬ IPTVæ™ºèƒ½ç®¡ç†å·¥å…· - å®Œæ•´ä¼ä¸šçº§ç‰ˆæœ¬ v8.3 (ä¿®å¤ä¼˜åŒ–ç‰ˆ)")
        print("=" * 60)
        
        # æ£€æŸ¥ä¾èµ–
        if not self.check_dependencies():
            print("âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return
        
        # éªŒè¯é…ç½®
        if not self.validate_config():
            print("âŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return
        
        # ä¿å­˜åˆå§‹é…ç½®
        if not self.save_config():
            print("âš ï¸  é…ç½®ä¿å­˜å¤±è´¥ï¼Œä½†ç¨‹åºå°†ç»§ç»­è¿è¡Œ")
        
        # æ£€æŸ¥æ¢å¤ç‚¹
        can_resume, checkpoint_data = self.can_resume_from_checkpoint()
        if can_resume:
            print(f"ğŸ” å‘ç°æ£€æŸ¥ç‚¹: {checkpoint_data['stage']}")
            response = input("æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤? (y/N): ")
            if response.lower() == 'y':
                if self.resume_from_checkpoint(checkpoint_data):
                    print("âœ… ä»æ£€æŸ¥ç‚¹æ¢å¤æˆåŠŸ")
                    return
                else:
                    print("âŒ ä»æ£€æŸ¥ç‚¹æ¢å¤å¤±è´¥ï¼Œå¼€å§‹æ–°çš„å¤„ç†æµç¨‹...")
            else:
                print("ğŸ”„ å¼€å§‹æ–°çš„å¤„ç†æµç¨‹...")
        
        # æ£€æŸ¥æ¨¡æ¿æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºç¤ºä¾‹
        template_path = Path(self.config.template_file)
        if not template_path.exists():
            print("ğŸ“ æœªæ‰¾åˆ°æ¨¡æ¿æ–‡ä»¶ï¼Œåˆ›å»ºç¤ºä¾‹æ¨¡æ¿...")
            if self.create_demo_template():
                print(f"\nğŸ’¡ æ¨¡æ¿æ–‡ä»¶å·²åˆ›å»ºï¼Œè¯·ç¼–è¾‘ä»¥ä¸‹æ–‡ä»¶åé‡æ–°è¿è¡Œç¨‹åº:")
                print(f"   ğŸ“„ {template_path.absolute()}")
                print("\næ¨¡æ¿å†…å®¹åŒ…å«å¸¸è§çš„å¤®è§†ã€å«è§†ç­‰é¢‘é“")
                input("æŒ‰å›è½¦é”®é€€å‡º...")
            return
        
        start_time = time.time()
        
        try:
            # 1. åŠ è½½æ¨¡æ¿
            print("\nğŸ“‹ æ­¥éª¤ 1/7: åŠ è½½é¢‘é“æ¨¡æ¿")
            self.save_checkpoint("loading_template")
            template_categories = self.load_template()
            if not template_categories:
                return
            
            # 2. è·å–æ‰€æœ‰æºæ•°æ®
            print("\nğŸŒ æ­¥éª¤ 2/7: è·å–æºæ•°æ®")
            self.save_checkpoint("fetching_sources")
            content = self.fetch_all_streams()
            if not content:
                print("âŒ æœªèƒ½è·å–ä»»ä½•æºæ•°æ®")
                return
            
            # å¤‡ä»½åŸå§‹æ•°æ®
            self.backup_data("raw_content", content)
            
            # 3. æ•´ç†æºæ•°æ®
            print("\nğŸ”§ æ­¥éª¤ 3/7: æ•´ç†æºæ•°æ®")
            self.save_checkpoint("organizing_streams")
            sources_df = self.organize_streams(content)
            if sources_df.empty:
                print("âŒ æœªèƒ½è§£æå‡ºæœ‰æ•ˆçš„æµæ•°æ®")
                return
            
            # å¤‡ä»½æ•´ç†åçš„æ•°æ®
            self.backup_data("organized_streams", sources_df)
            
            # 4. è·å–æ‰€æœ‰æ¨¡æ¿é¢‘é“
            all_template_channels = []
            for channels in template_categories.values():
                all_template_channels.extend(channels)
            
            # 5. è¿‡æ»¤å’ŒåŒ¹é…é¢‘é“
            print("\nğŸ¯ æ­¥éª¤ 4/7: é¢‘é“åŒ¹é…")
            self.save_checkpoint("matching_channels")
            filtered_df = self.filter_and_sort_sources(sources_df, all_template_channels)
            if filtered_df.empty:
                print("âŒ æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æ¨¡æ¿é¢‘é“")
                return
            
            # å¤‡ä»½åŒ¹é…ç»“æœ
            self.backup_data("matched_channels", filtered_df)
            
            # 6. æµ‹é€Ÿ
            print("\nâš¡ æ­¥éª¤ 5/7: æºæµ‹é€Ÿï¼ˆFFmpeg: 10ç§’æµ‹è¯•ï¼Œ10ç§’è¶…æ—¶ï¼‰")
            self.save_checkpoint("speed_testing")
            speed_tested_df = self.speed_test_sources(filtered_df)
            if speed_tested_df.empty:
                print("âŒ æ²¡æœ‰å¯ç”¨çš„æºé€šè¿‡æµ‹é€Ÿ")
                return
            
            # 7. ç”Ÿæˆæœ€ç»ˆæ•°æ®
            print("\nğŸ¨ æ­¥éª¤ 6/7: ç”Ÿæˆæ’­æ”¾åˆ—è¡¨")
            self.save_checkpoint("generating_final_data")
            final_data = self.generate_final_data(speed_tested_df, template_categories)
            
            # 8. ä¿å­˜æ–‡ä»¶
            print("\nğŸ’¾ æ­¥éª¤ 7/7: ä¿å­˜æ–‡ä»¶")
            self.save_checkpoint("saving_files")
            if not self.save_output_files(final_data):
                print("âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥")
                return
            
            # 9. éªŒè¯è¾“å‡ºæ–‡ä»¶
            print("\nğŸ” éªŒè¯è¾“å‡ºæ–‡ä»¶...")
            validation_result = self.validate_output_files()
            
            # 10. ç”Ÿæˆå®Œæ•´æ€§æŠ¥å‘Š
            integrity_report = self.generate_integrity_report(final_data)
            
            # 11. æ‰“å°ç»Ÿè®¡å’ŒæŠ¥å‘Š
            self.print_statistics(final_data)
            
            # æ‰“å°éªŒè¯ç»“æœ
            print("\nğŸ“Š æ–‡ä»¶éªŒè¯ç»“æœ:")
            if validation_result['txt_file']['valid']:
                print(f"  âœ… TXTæ–‡ä»¶: {validation_result['txt_file']['categories']}åˆ†ç±», {validation_result['txt_file']['sources']}ä¸ªæº")
            else:
                print(f"  âŒ TXTæ–‡ä»¶: æ— æ•ˆæˆ–ä¸ºç©º")
            
            if validation_result['m3u_file']['valid']:
                print(f"  âœ… M3Uæ–‡ä»¶: {validation_result['m3u_file']['channels']}ä¸ªé¢‘é“, {validation_result['m3u_file']['sources']}ä¸ªæº")
            else:
                print(f"  âŒ M3Uæ–‡ä»¶: æ— æ•ˆæˆ–æ ¼å¼é”™è¯¯")
            
            # æ‰“å°å®Œæ•´æ€§æŠ¥å‘Šæ‘˜è¦
            print(f"\nğŸ“ˆ æ•°æ®å®Œæ•´æ€§: {integrity_report['overall_status'].upper()}")
            print(f"ğŸ“º é¢‘é“è¦†ç›–ç‡: {integrity_report['channel_coverage']['coverage_rate']}%")
            print(f"ğŸ”— å¹³å‡æºæ•°: {integrity_report['channel_coverage']['avg_sources_per_channel']:.2f}")
            
            if integrity_report['recommendations']:
                print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                for recommendation in integrity_report['recommendations']:
                    print(f"  â€¢ {recommendation}")
            
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            print("\nğŸ‰ å¤„ç†å®Œæˆ!")
            print(f"â° æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
            print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶ä½ç½®:")
            print(f"   ğŸ“„ {Path(self.config.output_txt).absolute()}")
            print(f"   ğŸ“„ {Path(self.config.output_m3u).absolute()}")
            
            # æ¸…ç†æ£€æŸ¥ç‚¹ï¼ˆæˆåŠŸå®Œæˆï¼‰
            if self.checkpoint_file.exists():
                self.checkpoint_file.unlink()
            
            # éªŒè¯èµ„æºæ¸…ç†
            cleanup_verification = self.verify_cleanup()
            if not cleanup_verification['overall_clean']:
                print(f"âš ï¸  èµ„æºæ¸…ç†è­¦å‘Š: è¿˜æœ‰{cleanup_verification['temp_files_remaining']}ä¸ªä¸´æ—¶æ–‡ä»¶æœªæ¸…ç†")
                
        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
            print("ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œä¸‹æ¬¡å¯ä»¥æ¢å¤å¤„ç†")
        except Exception as e:
            print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            print("ğŸ’¾ é”™è¯¯æ£€æŸ¥ç‚¹å·²ä¿å­˜")
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self.cleanup()

def main():
    """ä¸»å‡½æ•°"""
    try:
        # å…ˆå°è¯•åŠ è½½é…ç½®
        config = None
        config_path = "iptv_config.json"
        
        if Path(config_path).exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config_dict = json.load(f)
                config = AppConfig.from_dict(config_dict)
                print("âœ… ä½¿ç”¨å·²ä¿å­˜çš„é…ç½®")
            except Exception as e:
                print(f"âš ï¸  é…ç½®åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        # åˆ›å»ºç®¡ç†å™¨å®ä¾‹
        manager = IPTVManager(config)
        manager.run()
        
    except Exception as e:
        print(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
