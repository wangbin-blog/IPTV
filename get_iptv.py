import requests
import pandas as pd
import re
import os
import time
import logging
import json
import stat
import platform
from itertools import cycle
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# ======================== æ ¸å¿ƒé…ç½®åŒº =========================
# åŸºç¡€åŠŸèƒ½é…ç½®
SOURCE_URLS = [
    "https://raw.githubusercontent.com/zwc456baby/iptv_alive/master/live.txt",
    "https://live.zbds.top/tv/iptv6.txt",
    "https://live.zbds.top/tv/iptv4.txt",
]
DEFAULT_TEMPLATE = "demo.txt"  # é»˜è®¤åˆ†ç±»æ¨¡æ¿
BACKUP_TEMPLATE = "demo_backup.txt"  # å¤‡ä»½æ¨¡æ¿
MAX_INTERFACES_PER_CHANNEL = 5  # å•é¢‘é“æœ€å¤šä¿ç•™æ¥å£æ•°
SPEED_TEST_TIMEOUT = 8  # æµ‹é€Ÿè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
MAX_SPEED_TEST_WORKERS = 15  # æµ‹é€Ÿå¹¶å‘çº¿ç¨‹æ•°
MAX_FETCH_WORKERS = 5  # æºæŠ“å–å¹¶å‘çº¿ç¨‹æ•°

# è¾“å‡ºé…ç½®ï¼ˆå›ºå®šæ–‡ä»¶åï¼‰
TXT_OUTPUT = "iptv.txt"
M3U_OUTPUT = "iptv.m3u"
CATEGORY_MARKER = "#genre#"  # æ¨¡æ¿ä¸­åˆ†ç±»æ ‡è®°
CACHE_FILE = ".iptv_valid_cache.json"  # æºæœ‰æ•ˆæ€§ç¼“å­˜æ–‡ä»¶
CACHE_EXPIRE = 3600  # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
MAX_CACHE_SIZE = 100  # æœ€å¤§ç¼“å­˜æ•°é‡ï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰

# æŠ“å–ä¼˜åŒ–é…ç½®
MAX_REDIRECTS = 3  # æœ€å¤§é‡å®šå‘æ¬¡æ•°
REQ_INTERVAL = [0.2, 0.3, 0.4, 0.5]  # æŠ“å–è¯·æ±‚é—´éš”ï¼ˆéšæœºå¾ªç¯ï¼‰
MIN_CONTENT_LEN = 100  # æœ‰æ•ˆæºå†…å®¹æœ€å°é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰
TEST_URL = "https://www.baidu.com"  # ç½‘ç»œæ£€æµ‹URL

# ç³»ç»Ÿå…¼å®¹æ€§é…ç½®
SYSTEM = platform.system()
IS_WINDOWS = SYSTEM == "Windows"
IS_LINUX = SYSTEM == "Linux"
IS_MAC = SYSTEM == "Darwin"

# é¢œè‰²è¾“å‡ºé…ç½®ï¼ˆWindowsç»ˆç«¯å…¼å®¹ï¼‰
if IS_WINDOWS:
    COLOR_GREEN = ""
    COLOR_RED = ""
    COLOR_YELLOW = ""
    COLOR_BLUE = ""
    COLOR_RESET = ""
else:
    COLOR_GREEN = "\033[92m"
    COLOR_RED = "\033[91m"
    COLOR_YELLOW = "\033[93m"
    COLOR_BLUE = "\033[94m"
    COLOR_RESET = "\033[0m"

# çº¿ç¨‹å®‰å…¨é”
CACHE_LOCK = Lock()  # ç¼“å­˜æ“ä½œé”
PRINT_LOCK = Lock()  # æ§åˆ¶å°è¾“å‡ºé”
# =========================================================================

# æ­£åˆ™è¡¨è¾¾å¼å®šä¹‰
IPV4_PAT = re.compile(r'^https?://(\d{1,3}\.){3}\d{1,3}')
IPV6_PAT = re.compile(r'^https?://\[([a-fA-F0-9:]+)\]')
URL_PAT = re.compile(r'^https?://')
SPACE_CLEAN_PAT = re.compile(r'^\s+|\s+$|\s+(?=\s)')  # ä¼˜åŒ–çš„ç©ºæ ¼æ¸…ç†æ­£åˆ™

# æ—¥å¿—åˆå§‹åŒ–ï¼ˆåˆ†çº§è®°å½•ï¼Œä¾¿äºæ’æŸ¥ï¼‰
logging.basicConfig(
    filename="iptv_tool.log",
    level=logging.INFO,
    encoding="utf-8",
    format="%(asctime)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s"
)


def print_sep(title: str = "", length: int = 70) -> None:
    """æ‰“å°å¸¦æ ‡é¢˜çš„åˆ†éš”çº¿ï¼Œçº¿ç¨‹å®‰å…¨"""
    with PRINT_LOCK:
        sep = "=" * length
        if title:
            print(f"\n{sep}\nğŸ“Œ {COLOR_BLUE}{title}{COLOR_RESET}\n{sep}")
        else:
            print(sep)


def safe_print(msg: str) -> None:
    """çº¿ç¨‹å®‰å…¨çš„æ§åˆ¶å°è¾“å‡º"""
    with PRINT_LOCK:
        print(msg)


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºæ ¼ã€æ¢è¡Œç¬¦"""
    return SPACE_CLEAN_PAT.sub("", str(text).strip())


def check_network() -> bool:
    """æ£€æµ‹ç½‘ç»œè¿æ¥çŠ¶æ€ï¼Œé€‚é…å¤šç³»ç»Ÿ"""
    safe_print(f"{COLOR_BLUE}ğŸ” æ­£åœ¨æ£€æµ‹ç½‘ç»œè¿æ¥...{COLOR_RESET}")
    try:
        timeout = 3 if not IS_WINDOWS else 5
        resp = requests.get(TEST_URL, timeout=timeout)
        if resp.status_code == 200:
            safe_print(f"{COLOR_GREEN}âœ… ç½‘ç»œè¿æ¥æ­£å¸¸ï¼ˆ{SYSTEM}ç³»ç»Ÿï¼‰{COLOR_RESET}")
            return True
        else:
            safe_print(f"{COLOR_RED}âŒ ç½‘ç»œæ£€æµ‹å¤±è´¥ï¼šHTTPçŠ¶æ€ç  {resp.status_code}{COLOR_RESET}")
            return False
    except Exception as e:
        safe_print(f"{COLOR_RED}âŒ ç½‘ç»œè¿æ¥å¼‚å¸¸ï¼š{str(e)}{COLOR_RESET}")
        return False


def set_file_permissions(file_path: str) -> None:
    """è®¾ç½®æ–‡ä»¶æƒé™ï¼Œé€‚é…å¤šç³»ç»Ÿ"""
    if IS_WINDOWS:
        return
    try:
        os.chmod(file_path, stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH)
        logging.info(f"è®¾ç½®æ–‡ä»¶æƒé™ï¼š{file_path}")
    except Exception as e:
        safe_print(f"{COLOR_YELLOW}âš ï¸ è®¾ç½®æ–‡ä»¶æƒé™å¤±è´¥ï¼š{str(e)}{COLOR_RESET}")
        logging.warning(f"æ–‡ä»¶æƒé™è®¾ç½®å¤±è´¥ï¼š{file_path} - {str(e)}")


def generate_default_template() -> bool:
    """ç”Ÿæˆé»˜è®¤åˆ†ç±»æ¨¡æ¿ï¼ˆå½“æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨æ—¶ï¼‰"""
    default_cats = [
        {"name": "å¤®è§†é¢‘é“", "channels": ["CCTV1", "CCTV2", "CCTV3", "CCTV5", "CCTV6", "CCTV8", "CCTV13"]},
        {"name": "å«è§†é¢‘é“", "channels": ["æ¹–å—å«è§†", "æµ™æ±Ÿå«è§†", "ä¸œæ–¹å«è§†", "æ±Ÿè‹å«è§†", "åŒ—äº¬å«è§†", "å®‰å¾½å«è§†"]},
        {"name": "åœ°æ–¹é¢‘é“", "channels": ["å¹¿ä¸œå«è§†", "å±±ä¸œå«è§†", "å››å·å«è§†", "æ¹–åŒ—å«è§†", "æ²³å—å«è§†"]}
    ]
    try:
        with open(DEFAULT_TEMPLATE, 'w', encoding='utf-8') as f:
            f.write(f"# IPTVåˆ†ç±»æ¨¡æ¿ï¼ˆè‡ªåŠ¨ç”Ÿæˆäº {time.strftime('%Y-%m-%d %H:%M:%S')}ï¼‰\n")
            f.write(f"# ç³»ç»Ÿï¼š{SYSTEM} | æ ¼å¼ï¼š{CATEGORY_MARKER} åˆ†ç±»å æ¢è¡Œ é¢‘é“å\n\n")
            for cat in default_cats:
                f.write(f"{CATEGORY_MARKER} {cat['name']}\n")
                for ch in cat["channels"]:
                    f.write(f"{ch}\n")
                f.write("\n")
        set_file_permissions(DEFAULT_TEMPLATE)
        safe_print(f"{COLOR_GREEN}âœ… é»˜è®¤æ¨¡æ¿ç”ŸæˆæˆåŠŸï¼š{os.path.abspath(DEFAULT_TEMPLATE)}{COLOR_RESET}")
        return True
    except Exception as e:
        safe_print(f"{COLOR_RED}âŒ ç”Ÿæˆé»˜è®¤æ¨¡æ¿å¤±è´¥ï¼š{str(e)}{COLOR_RESET}")
        logging.error(f"æ¨¡æ¿ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
        return False


def read_template(template_path: str = DEFAULT_TEMPLATE) -> tuple[list[dict], list[str]] | tuple[None, None]:
    """è¯»å–åˆ†ç±»æ¨¡æ¿ï¼Œæ”¯æŒæŒ‡å®šè·¯å¾„ã€è‡ªåŠ¨å¤‡ä»½å’Œç”Ÿæˆ"""
    if not os.path.exists(template_path):
        safe_print(f"{COLOR_YELLOW}âš ï¸ æ¨¡æ¿ã€Œ{template_path}ã€ä¸å­˜åœ¨ï¼Œç”Ÿæˆé»˜è®¤æ¨¡æ¿...{COLOR_RESET}")
        if not generate_default_template():
            return None, None

    # è‡ªåŠ¨å¤‡ä»½æ¨¡æ¿
    try:
        with open(template_path, 'r', encoding='utf-8') as f_src, open(BACKUP_TEMPLATE, 'w', encoding='utf-8') as f_dst:
            f_dst.write(f"# æ¨¡æ¿å¤‡ä»½ï¼ˆ{time.strftime('%Y-%m-%d %H:%M:%S')}ï¼‰\n# æºæ¨¡æ¿ï¼š{template_path}\n")
            f_dst.write(f_src.read())
        set_file_permissions(BACKUP_TEMPLATE)
    except Exception as e:
        safe_print(f"{COLOR_YELLOW}âš ï¸ æ¨¡æ¿å¤‡ä»½å¤±è´¥ï¼š{str(e)}ï¼Œä¸å½±å“æµç¨‹{COLOR_RESET}")
        logging.warning(f"æ¨¡æ¿å¤‡ä»½å¤±è´¥ï¼š{str(e)}")

    categories = []
    current_cat = None
    all_channels = []

    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                if line.startswith("#") and not line.startswith(CATEGORY_MARKER):
                    continue

                # å¤„ç†åˆ†ç±»è¡Œ
                if line.startswith(CATEGORY_MARKER):
                    cat_name = clean_text(line.lstrip(CATEGORY_MARKER))
                    if not cat_name:
                        safe_print(f"{COLOR_YELLOW}âš ï¸ ç¬¬{line_num}è¡Œï¼šåˆ†ç±»åä¸ºç©ºï¼Œå¿½ç•¥{COLOR_RESET}")
                        current_cat = None
                        continue
                    existing = next((c for c in categories if c["name"] == cat_name), None)
                    if existing:
                        current_cat = cat_name
                    else:
                        categories.append({"name": cat_name, "channels": []})
                        current_cat = cat_name
                    continue

                # å¤„ç†é¢‘é“è¡Œ
                if current_cat is None:
                    safe_print(f"{COLOR_YELLOW}âš ï¸ ç¬¬{line_num}è¡Œï¼šé¢‘é“æœªåˆ†ç±»ï¼Œå½’å…¥ã€Œæœªåˆ†ç±»ã€{COLOR_RESET}")
                    if not any(c["name"] == "æœªåˆ†ç±»" for c in categories):
                        categories.append({"name": "æœªåˆ†ç±»", "channels": []})
                    current_cat = "æœªåˆ†ç±»"

                ch_name = clean_text(line.split(",")[0])
                if not ch_name:
                    safe_print(f"{COLOR_YELLOW}âš ï¸ ç¬¬{line_num}è¡Œï¼šé¢‘é“åä¸ºç©ºï¼Œå¿½ç•¥{COLOR_RESET}")
                    continue
                if ch_name not in all_channels:
                    all_channels.append(ch_name)
                    for cat in categories:
                        if cat["name"] == current_cat:
                            cat["channels"].append(ch_name)
                            break

    except Exception as e:
        safe_print(f"{COLOR_RED}âŒ è¯»å–æ¨¡æ¿å¤±è´¥ï¼š{str(e)}{COLOR_RESET}")
        logging.error(f"æ¨¡æ¿è¯»å–å¤±è´¥ï¼š{str(e)}")
        return None, None

    # è¾“å‡ºæ¨¡æ¿ç»Ÿè®¡
    total_ch = sum(len(c["channels"]) for c in categories)
    safe_print(f"{COLOR_GREEN}âœ… æ¨¡æ¿è¯»å–å®Œæˆ | åˆ†ç±»æ•°ï¼š{len(categories)} | æ€»é¢‘é“æ•°ï¼š{total_ch} | è·¯å¾„ï¼š{os.path.abspath(template_path)}{COLOR_RESET}")
    safe_print("  " + "-" * 70)
    for i, cat in enumerate(categories, 1):
        safe_print(f"  {i:2d}. {cat['name']:<25} é¢‘é“æ•°ï¼š{len(cat['channels']):2d}")
    safe_print("  " + "-" * 70)
    return categories, all_channels


def load_valid_cache() -> dict:
    """åŠ è½½ç¼“å­˜ï¼Œè‡ªåŠ¨æ¸…ç†è¿‡æœŸ/è¶…é‡é¡¹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    with CACHE_LOCK:
        if not os.path.exists(CACHE_FILE):
            return {}
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                cache = json.load(f)
            # è¿‡æ»¤è¿‡æœŸç¼“å­˜
            current_time = time.time()
            valid_cache = {
                url: info for url, info in cache.items()
                if current_time - info["timestamp"] < CACHE_EXPIRE
            }
            # æ¸…ç†è¶…é‡ç¼“å­˜
            if len(valid_cache) > MAX_CACHE_SIZE:
                sorted_cache = sorted(valid_cache.items(), key=lambda x: x[1]["timestamp"], reverse=True)
                valid_cache = dict(sorted_cache[:MAX_CACHE_SIZE])
                safe_print(f"{COLOR_YELLOW}âš ï¸ ç¼“å­˜è¶…é‡ï¼Œä¿ç•™æœ€æ–°{MAX_CACHE_SIZE}ä¸ª{COLOR_RESET}")
            logging.info(f"åŠ è½½ç¼“å­˜ï¼š{len(valid_cache)}ä¸ªæœ‰æ•ˆé¡¹")
            return valid_cache
        except Exception as e:
            safe_print(f"{COLOR_YELLOW}âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥ï¼š{str(e)}ï¼Œé‡æ–°ç”Ÿæˆ{COLOR_RESET}")
            logging.error(f"ç¼“å­˜åŠ è½½å¤±è´¥ï¼š{str(e)}")
            return {}


def save_valid_cache(cache: dict) -> bool:
    """ä¿å­˜ç¼“å­˜ï¼Œè‡ªåŠ¨æ§åˆ¶å¤§å°ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    with CACHE_LOCK:
        if len(cache) > MAX_CACHE_SIZE:
            sorted_cache = sorted(cache.items(), key=lambda x: x[1]["timestamp"], reverse=True)
            cache = dict(sorted_cache[:MAX_CACHE_SIZE])
        try:
            with open(CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(cache, f, ensure_ascii=False, indent=2)
            set_file_permissions(CACHE_FILE)
            logging.info(f"ä¿å­˜ç¼“å­˜ï¼š{len(cache)}ä¸ªé¡¹")
            return True
        except Exception as e:
            safe_print(f"{COLOR_YELLOW}âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥ï¼š{str(e)}ï¼Œä¸å½±å“æµç¨‹{COLOR_RESET}")
            logging.error(f"ç¼“å­˜ä¿å­˜å¤±è´¥ï¼š{str(e)}")
            return False


def is_valid_url(url: str) -> bool:
    """éªŒè¯URLæ˜¯å¦ä¸ºHTTP/HTTPSæ ¼å¼"""
    return URL_PAT.match(url) is not None


def fetch_single(url: str, cache: dict) -> str | None:
    """æŠ“å–å•ä¸ªæºï¼Œç»“åˆç¼“å­˜ä¼˜åŒ–ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    # æ£€æŸ¥ç¼“å­˜
    current_time = time.time()
    with CACHE_LOCK:
        if url in cache:
            cache_info = cache[url]
            if current_time - cache_info["timestamp"] < CACHE_EXPIRE:
                if cache_info["valid"]:
                    safe_print(f"{COLOR_BLUE}ğŸ” ç¼“å­˜å‘½ä¸­ï¼š{url[:50]}{'...' if len(url)>50 else ''}ï¼ˆæœ‰æ•ˆï¼‰{COLOR_RESET}")
                    return cache_info["content"]
                else:
                    safe_print(f"{COLOR_YELLOW}ğŸ” ç¼“å­˜å‘½ä¸­ï¼š{url[:50]}{'...' if len(url)>50 else ''}ï¼ˆå¤±æ•ˆï¼Œè·³è¿‡ï¼‰{COLOR_RESET}")
                    return None

    safe_print(f"{COLOR_BLUE}ğŸ” æŠ“å–ï¼š{url[:50]}{'...' if len(url)>50 else ''}{COLOR_RESET}")
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/124.0.0.0 Safari/537.36",
            "Accept": "*/*",
            "Connection": "keep-alive"
        }
        # é€‚é…ç³»ç»Ÿè¶…æ—¶
        connect_timeout = 5 if not IS_WINDOWS else 8
        read_timeout = SPEED_TEST_TIMEOUT if not IS_WINDOWS else SPEED_TEST_TIMEOUT + 2
        resp = requests.get(
            url,
            timeout=(connect_timeout, read_timeout),
            headers=headers,
            allow_redirects=True,
            max_redirects=MAX_REDIRECTS,
            stream=False
        )
        resp.raise_for_status()

        # å¤„ç†ç¼–ç 
        if not resp.encoding or resp.encoding.lower() == "iso-8859-1":
            resp.encoding = resp.apparent_encoding
        content = resp.text

        # æ ¡éªŒå†…å®¹æœ‰æ•ˆæ€§
        if len(content) < MIN_CONTENT_LEN:
            safe_print(f"{COLOR_YELLOW}âš ï¸ å†…å®¹è¿‡çŸ­ï¼ˆ{len(content)}å­—ç¬¦ï¼‰ï¼Œæ— æ•ˆ{COLOR_RESET}")
            with CACHE_LOCK:
                cache[url] = {"valid": False, "timestamp": current_time}
            return None
        if not (URL_PAT.search(content) or "#EXTM3U" in content[:100]):
            safe_print(f"{COLOR_YELLOW}âš ï¸ æ— ç›´æ’­æºä¿¡æ¯ï¼Œæ— æ•ˆ{COLOR_RESET}")
            with CACHE_LOCK:
                cache[url] = {"valid": False, "timestamp": current_time}
            return None

        # ç¼“å­˜æœ‰æ•ˆå†…å®¹
        safe_print(f"{COLOR_GREEN}âœ… æŠ“å–æˆåŠŸ | é•¿åº¦ï¼š{len(content):,}å­—ç¬¦{COLOR_RESET}")
        with CACHE_LOCK:
            cache[url] = {"valid": True, "content": content, "timestamp": current_time}
        return content

    except requests.exceptions.ConnectTimeout:
        msg = "è¿æ¥è¶…æ—¶"
        safe_print(f"{COLOR_RED}âŒ æŠ“å–å¤±è´¥ï¼š{msg}{COLOR_RESET}")
        with CACHE_LOCK:
            cache[url] = {"valid": False, "timestamp": current_time}
        return None
    except requests.exceptions.ReadTimeout:
        msg = f"è¯»å–è¶…æ—¶ï¼ˆ>{read_timeout}ç§’ï¼‰"
        safe_print(f"{COLOR_RED}âŒ æŠ“å–å¤±è´¥ï¼š{msg}{COLOR_RESET}")
        with CACHE_LOCK:
            cache[url] = {"valid": False, "timestamp": current_time}
        return None
    except requests.exceptions.TooManyRedirects:
        msg = f"é‡å®šå‘è¶…{MAX_REDIRECTS}æ¬¡"
        safe_print(f"{COLOR_RED}âŒ æŠ“å–å¤±è´¥ï¼š{msg}{COLOR_RESET}")
        with CACHE_LOCK:
            cache[url] = {"valid": False, "timestamp": current_time}
        return None
    except requests.exceptions.ConnectionError:
        msg = "ç½‘ç»œè¿æ¥å¤±è´¥"
        safe_print(f"{COLOR_RED}âŒ æŠ“å–å¤±è´¥ï¼š{msg}{COLOR_RESET}")
        with CACHE_LOCK:
            cache[url] = {"valid": False, "timestamp": current_time}
        return None
    except requests.exceptions.HTTPError as e:
        msg = f"HTTPé”™è¯¯ï¼šçŠ¶æ€ç  {e.response.status_code}"
        safe_print(f"{COLOR_RED}âŒ æŠ“å–å¤±è´¥ï¼š{msg}{COLOR_RESET}")
        with CACHE_LOCK:
            cache[url] = {"valid": False, "timestamp": current_time}
        return None
    except Exception as e:
        msg = f"æœªçŸ¥é”™è¯¯ï¼š{str(e)}"
        safe_print(f"{COLOR_RED}âŒ æŠ“å–å¤±è´¥ï¼š{msg}{COLOR_RESET}")
        with CACHE_LOCK:
            cache[url] = {"valid": False, "timestamp": current_time}
        return None


def batch_fetch(url_list: list) -> str:
    """æ‰¹é‡æŠ“å–ç›´æ’­æºï¼Œç»“åˆç¼“å­˜ä¼˜åŒ–å’Œè¿›åº¦æ˜¾ç¤º"""
    # åŠ è½½ç¼“å­˜
    cache = load_valid_cache()

    # æ­¥éª¤1ï¼šè¿‡æ»¤æ— æ•ˆURL
    valid_urls = [u for u in url_list if is_valid_url(u)]
    invalid_cnt = len(url_list) - len(valid_urls)
    if invalid_cnt > 0:
        safe_print(f"{COLOR_YELLOW}âš ï¸ è¿‡æ»¤æ— æ•ˆURLï¼š{invalid_cnt} ä¸ªï¼ˆéHTTP/HTTPSæ ¼å¼ï¼‰{COLOR_RESET}")
        logging.warning(f"è¿‡æ»¤æ— æ•ˆURLæ•°é‡ï¼š{invalid_cnt}")
    if not valid_urls:
        safe_print(f"{COLOR_RED}âŒ æ— æœ‰æ•ˆURLå¯æŠ“å–{COLOR_RESET}")
        return ""

    # æ­¥éª¤2ï¼šå¹¶å‘æŠ“å–ï¼ˆå¸¦éšæœºé—´éš”ï¼‰
    combined = []
    interval_cycle = cycle(REQ_INTERVAL)
    total = len(valid_urls)
    print_sep("æ‰¹é‡æŠ“å–é…ç½®")
    safe_print(f"æ€»URLï¼š{total} | å¹¶å‘æ•°ï¼š{MAX_FETCH_WORKERS} | é—´éš”ï¼š{min(REQ_INTERVAL)}-{max(REQ_INTERVAL)}ç§’ | ç¼“å­˜é¡¹ï¼š{len(cache)}")
    print_sep(length=70)

    with ThreadPoolExecutor(max_workers=MAX_FETCH_WORKERS) as executor:
        futures = {}
        for url in valid_urls:
            time.sleep(next(interval_cycle))  # éšæœºé—´éš”é¿å…åçˆ¬
            futures[executor.submit(fetch_single, url, cache)] = url

        # å¤„ç†æŠ“å–ç»“æœï¼ˆæ˜¾ç¤ºè¿›åº¦ç™¾åˆ†æ¯”ï¼‰
        completed = 0
        for future in as_completed(futures):
            completed += 1
            progress = (completed / total) * 100
            url = futures[future]
            content = future.result()
            if content:
                combined.append(content)
            safe_print(f"{COLOR_YELLOW}ğŸ“Š æŠ“å–è¿›åº¦ï¼š{completed}/{total} ({progress:.1f}%){COLOR_RESET}")
            print_sep(length=70)

    # ä¿å­˜ç¼“å­˜
    save_valid_cache(cache)

    # è¾“å‡ºæŠ“å–ç»Ÿè®¡
    success_cnt = len(combined)
    safe_print(f"\n{COLOR_GREEN}ğŸ“Š æŠ“å–ç»Ÿè®¡ | æˆåŠŸï¼š{success_cnt} ä¸ª | å¤±è´¥ï¼š{total-success_cnt} ä¸ª | è¿‡æ»¤ï¼š{invalid_cnt} ä¸ª | ç¼“å­˜å‘½ä¸­ï¼š{len(cache)}ä¸ª{COLOR_RESET}")
    logging.info(f"æ‰¹é‡æŠ“å–å®Œæˆ | æ€»ï¼š{total} | æˆåŠŸï¼š{success_cnt} | å¤±è´¥ï¼š{total-success_cnt} | ç¼“å­˜æ›´æ–°ï¼š{len(cache)}ä¸ª")
    return "\n".join(combined)


def parse_m3u(content: str) -> list[dict]:
    """è§£æM3Uæ ¼å¼ç›´æ’­æºï¼Œæå–é¢‘é“åå’Œæ’­æ”¾åœ°å€"""
    streams = []
    current_ch = None
    line_cnt = 0

    for line in content.splitlines():
        line_cnt += 1
        line = line.strip()
        # è§£æé¢‘é“åï¼ˆä¼˜å…ˆtvg-nameï¼Œå…¶æ¬¡ä»æè¿°æå–ï¼‰
        if line.startswith("#EXTINF"):
            tvg_match = re.search(r'tvg-name=(["\']?)([^"\']+)\1', line)
            if tvg_match:
                current_ch = clean_text(tvg_match.group(2))
            else:
                desc_match = re.search(r',([^,]+)$', line)
                if desc_match:
                    current_ch = clean_text(desc_match.group(1))
            continue
        # è§£ææ’­æ”¾åœ°å€
        if URL_PAT.match(line) and current_ch:
            streams.append({"name": current_ch, "url": line})
            current_ch = None  # é‡ç½®é¿å…é‡å¤åŒ¹é…

    safe_print(f"{COLOR_GREEN}ğŸ“Š M3Uè§£æç»“æœ | æ€»è¡Œæ•°ï¼š{line_cnt:,} | æå–æœ‰æ•ˆæºï¼š{len(streams)} ä¸ª{COLOR_RESET}")
    return streams


def parse_txt(content: str) -> list[dict]:
    """è§£æTXTæ ¼å¼ç›´æ’­æºï¼ˆæ ¼å¼ï¼šé¢‘é“å,URLï¼‰"""
    streams = []
    line_cnt = 0
    valid_cnt = 0

    for line in content.splitlines():
        line_cnt += 1
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        # å…¼å®¹ç©ºæ ¼åˆ†éš”ï¼Œç»Ÿä¸€è½¬ä¸ºé€—å·åˆ†éš”
        line = line.replace(" ", ",")
        parts = [p.strip() for p in line.split(",") if p.strip()]
        # éœ€åŒ…å«é¢‘é“åå’ŒURLï¼ˆè‡³å°‘2éƒ¨åˆ†ï¼Œæœ€åä¸€éƒ¨åˆ†ä¸ºURLï¼‰
        if len(parts) >= 2 and URL_PAT.match(parts[-1]):
            ch_name = clean_text(",".join(parts[:-1]))
            streams.append({"name": ch_name, "url": parts[-1]})
            valid_cnt += 1
        else:
            safe_print(f"{COLOR_YELLOW}âš ï¸ ç¬¬{line_cnt}è¡Œï¼šæ ¼å¼æ— æ•ˆï¼ˆéœ€ä¸ºã€Œé¢‘é“å,URLã€ï¼‰ï¼Œå¿½ç•¥{COLOR_RESET}")

    safe_print(f"{COLOR_GREEN}ğŸ“Š TXTè§£æç»“æœ | æ€»è¡Œæ•°ï¼š{line_cnt:,} | æœ‰æ•ˆè¡Œï¼š{valid_cnt} | æå–æœ‰æ•ˆæºï¼š{len(streams)} ä¸ª{COLOR_RESET}")
    return streams


def test_latency(url: str) -> int | None:
    """æµ‹è¯•å•ä¸ªç›´æ’­æºå»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰ï¼Œä¼˜å…ˆHEADè¯·æ±‚ï¼Œå¤±è´¥é™çº§ä¸ºGET"""
    start = time.time()
    try:
        # å…ˆå°è¯•HEADè¯·æ±‚ï¼ˆè½»é‡ï¼‰ï¼Œå¤±è´¥åˆ™ç”¨GETè¯·æ±‚éªŒè¯
        for method in [requests.head, requests.get]:
            with method(
                url,
                timeout=SPEED_TEST_TIMEOUT,
                allow_redirects=True,
                stream=(method == requests.get)
            ) as resp:
                if resp.status_code in [200, 206]:
                    if method == requests.get:
                        resp.iter_content(1).__next__()  # è¯»å–1å­—èŠ‚ç¡®è®¤å¯ç”¨æ€§
                    return int((time.time() - start) * 1000)
    except Exception:
        return None  # å¼‚å¸¸ç›´æ¥è¿”å›Noneï¼Œä¸åœ¨æ­¤æ‰“å°æ—¥å¿—


def batch_test(streams: list[dict]) -> pd.DataFrame:
    """æ‰¹é‡æµ‹è¯•ç›´æ’­æºå»¶è¿Ÿï¼Œè¿”å›æŒ‰å»¶è¿Ÿæ’åºçš„DataFrame"""
    if not streams:
        safe_print(f"{COLOR_RED}âŒ æ— ç›´æ’­æºå¯æµ‹é€Ÿ{COLOR_RESET}")
        return pd.DataFrame(columns=["name", "url", "latency"])

    stream_df = pd.DataFrame(streams)
    total = len(stream_df)
    valid = []
    print_sep("æ‰¹é‡æµ‹é€Ÿé…ç½®")
    safe_print(f"æ€»æºæ•°ï¼š{total} | å¹¶å‘æ•°ï¼š{MAX_SPEED_TEST_WORKERS} | è¶…æ—¶ï¼š{SPEED_TEST_TIMEOUT}ç§’")
    print_sep(length=100)

    with ThreadPoolExecutor(max_workers=MAX_SPEED_TEST_WORKERS) as executor:
        futures = {
            executor.submit(test_latency, row["url"]): (row["name"], row["url"])
            for _, row in stream_df.iterrows()
        }

        # å¤„ç†æµ‹é€Ÿç»“æœï¼ˆæ˜¾ç¤ºè¿›åº¦ï¼‰
        completed = 0
        for idx, future in enumerate(as_completed(futures), 1):
            completed += 1
            progress = (completed / total) * 100
            ch_name, url = futures[future]
            latency = future.result()
            display_url = url[:70] + "..." if len(url) > 70 else url

            if latency is not None:
                valid.append({"name": ch_name, "url": url, "latency": latency})
                safe_print(f"{COLOR_GREEN}âœ… [{idx:3d}/{total} ({progress:.1f}%)] é¢‘é“ï¼š{ch_name:<20} URLï¼š{display_url:<75} å»¶è¿Ÿï¼š{latency:4d}ms{COLOR_RESET}")
            else:
                safe_print(f"{COLOR_RED}âŒ [{idx:3d}/{total} ({progress:.1f}%)] é¢‘é“ï¼š{ch_name:<20} URLï¼š{display_url:<75} çŠ¶æ€ï¼šæ— æ•ˆ{COLOR_RESET}")

    # è½¬æ¢ä¸ºDataFrameå¹¶æŒ‰å»¶è¿Ÿæ’åº
    latency_df = pd.DataFrame(valid)
    if not latency_df.empty:
        latency_df = latency_df.sort_values("latency").reset_index(drop=True)

    # è¾“å‡ºæµ‹é€Ÿç»Ÿè®¡
    print_sep(length=100)
    safe_print(f"ğŸ æµ‹é€Ÿå®Œæˆ | æœ‰æ•ˆæºï¼š{len(latency_df)} ä¸ª | æ— æ•ˆæºï¼š{total - len(latency_df)} ä¸ª")
    if len(latency_df) > 0:
        avg_lat = int(latency_df["latency"].mean())
        safe_print(f"ğŸ“Š å»¶è¿Ÿç»Ÿè®¡ | æœ€å¿«ï¼š{latency_df['latency'].min()}ms | æœ€æ…¢ï¼š{latency_df['latency'].max()}ms | å¹³å‡ï¼š{avg_lat}ms")
    logging.info(f"æ‰¹é‡æµ‹é€Ÿå®Œæˆ | æ€»ï¼š{total} | æœ‰æ•ˆï¼š{len(latency_df)} | å¹³å‡å»¶è¿Ÿï¼š{avg_lat if len(latency_df) > 0 else 0}ms")
    return latency_df


def organize_streams(raw_content: str, categories: list[dict], all_channels: list) -> list[dict]:
    """æŒ‰åˆ†ç±»æ¨¡æ¿æ•´ç†ç›´æ’­æºï¼šè¿‡æ»¤åŒ¹é…ã€æµ‹é€Ÿæ’åºã€é™åˆ¶æ¥å£æ•°"""
    print_sep("å¼€å§‹æ•´ç†ç›´æ’­æºï¼ˆ4ä¸ªæ­¥éª¤ï¼‰")

    # æ­¥éª¤1ï¼šè‡ªåŠ¨è¯†åˆ«æ ¼å¼å¹¶è§£æ
    if raw_content.startswith("#EXTM3U") or "#EXTINF" in raw_content[:100]:
        safe_print("1. è¯†åˆ«æ ¼å¼ï¼šM3U")
        parsed_streams = parse_m3u(raw_content)
    else:
        safe_print("1. è¯†åˆ«æ ¼å¼ï¼šTXTï¼ˆé»˜è®¤ï¼‰")
        parsed_streams = parse_txt(raw_content)

    if not parsed_streams:
        safe_print(f"{COLOR_RED}âŒ è§£æåæ— æœ‰æ•ˆç›´æ’­æºï¼Œæ•´ç†ç»ˆæ­¢{COLOR_RESET}")
        return []

    # æ­¥éª¤2ï¼šæŒ‰æ¨¡æ¿é¢‘é“è¿‡æ»¤ï¼ˆä»…ä¿ç•™æ¨¡æ¿ä¸­å­˜åœ¨çš„é¢‘é“ï¼‰
    safe_print(f"2. æŒ‰æ¨¡æ¿è¿‡æ»¤ | è§£ææºæ•°ï¼š{len(parsed_streams)} | æ¨¡æ¿é¢‘é“æ•°ï¼š{len(all_channels)}")
    matched_streams = []
    for stream in parsed_streams:
        # æ¨¡ç³ŠåŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
        if any(clean_text(stream["name"]).lower() == clean_text(ch).lower() for ch in all_channels):
            matched_streams.append(stream)

    if not matched_streams:
        safe_print(f"{COLOR_RED}âŒ æ— ç›´æ’­æºåŒ¹é…æ¨¡æ¿é¢‘é“ï¼Œæ•´ç†ç»ˆæ­¢{COLOR_RESET}")
        return []
    safe_print(f"   åŒ¹é…æˆåŠŸï¼š{len(matched_streams)} ä¸ªæº")

    # æ­¥éª¤3ï¼šæ‰¹é‡æµ‹é€Ÿå¹¶æŒ‰å»¶è¿Ÿæ’åº
    safe_print("3. å¼€å§‹æ‰¹é‡æµ‹é€Ÿï¼ˆæŒ‰å»¶è¿Ÿå‡åºæ’åºï¼‰")
    sorted_df = batch_test(matched_streams)
    if sorted_df.empty:
        safe_print(f"{COLOR_RED}âŒ æµ‹é€Ÿåæ— æœ‰æ•ˆæºï¼Œæ•´ç†ç»ˆæ­¢{COLOR_RESET}")
        return []

    # æ­¥éª¤4ï¼šæŒ‰åˆ†ç±»åˆ†ç»„å¹¶é™åˆ¶å•é¢‘é“æ¥å£æ•°
    safe_print("4. æŒ‰åˆ†ç±»åˆ†ç»„å¹¶é™åˆ¶æ¥å£æ•°")
    organized = []
    for cat in categories:
        cat_streams = []
        for _, row in sorted_df.iterrows():
            # åŒ¹é…åˆ†ç±»ä¸‹çš„é¢‘é“
            if clean_text(row["name"]).lower() in [clean_text(ch).lower() for ch in cat["channels"]]:
                cat_streams.append({
                    "category": cat["name"],
                    "name": row["name"],
                    "url": row["url"],
                    "latency": row["latency"]
                })
        # æŒ‰é¢‘é“å»é‡å¹¶é™åˆ¶æ¯ä¸ªé¢‘é“çš„æ¥å£æ•°
        ch_count = {}
        filtered_cat = []
        for s in cat_streams:
            ch_key = clean_text(s["name"]).lower()
            if ch_count.get(ch_key, 0) < MAX_INTERFACES_PER_CHANNEL:
                filtered_cat.append(s)
                ch_count[ch_key] = ch_count.get(ch_key, 0) + 1
        organized.extend(filtered_cat)

    safe_print(f"{COLOR_GREEN}âœ… æ•´ç†å®Œæˆ | æœ€ç»ˆæœ‰æ•ˆæºæ•°ï¼š{len(organized)}{COLOR_RESET}")
    return organized


def save_results(organized_streams: list[dict]) -> bool:
    """ä¿å­˜æ•´ç†åçš„ç›´æ’­æºåˆ°iptv.txtå’Œiptv.m3u"""
    if not organized_streams:
        safe_print(f"{COLOR_RED}âŒ æ— æœ‰æ•ˆæºå¯ä¿å­˜{COLOR_RESET}")
        return False

    # 1. ä¿å­˜TXTæ–‡ä»¶ï¼ˆæŒ‰åˆ†ç±»åˆ†ç»„ï¼‰
    try:
        with open(TXT_OUTPUT, 'w', encoding='utf-8') as f:
            f.write(f"# IPTVç›´æ’­æºåˆ—è¡¨ï¼ˆç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S')}ï¼‰\n")
            f.write(f"# ç³»ç»Ÿï¼š{SYSTEM} | æ€»æºæ•°ï¼š{len(organized_streams)} | å•é¢‘é“æœ€å¤§æ¥å£æ•°ï¼š{MAX_INTERFACES_PER_CHANNEL}\n\n")
            
            current_cat = None
            for s in organized_streams:
                if s["category"] != current_cat:
                    current_cat = s["category"]
                    f.write(f"# {CATEGORY_MARKER} {current_cat}\n")
                f.write(f"{s['name']},{s['url']},å»¶è¿Ÿï¼š{s['latency']}ms\n")
        set_file_permissions(TXT_OUTPUT)
        safe_print(f"{COLOR_GREEN}âœ… TXTæ–‡ä»¶ä¿å­˜æˆåŠŸï¼š{os.path.abspath(TXT_OUTPUT)}{COLOR_RESET}")
    except Exception as e:
        safe_print(f"{COLOR_RED}âŒ TXTæ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}{COLOR_RESET}")
        logging.error(f"TXTä¿å­˜å¤±è´¥ï¼š{str(e)}")
        return False

    # 2. ä¿å­˜M3Uæ–‡ä»¶ï¼ˆæ”¯æŒæ’­æ”¾å™¨è¯†åˆ«ï¼‰
    try:
        with open(M3U_OUTPUT, 'w', encoding='utf-8') as f:
            f.write("#EXTM3U x-tvg-url=\"http://epg.51zmt.top:8000/e.xml\"\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S')} | ç³»ç»Ÿï¼š{SYSTEM} | æ€»æºæ•°ï¼š{len(organized_streams)}\n")
            
            for s in organized_streams:
                f.write(f"#EXTINF:-1 tvg-name=\"{s['name']}\" group-title=\"{s['category']}\",{s['name']}\n")
                f.write(f"{s['url']}\n")
        set_file_permissions(M3U_OUTPUT)
        safe_print(f"{COLOR_GREEN}âœ… M3Uæ–‡ä»¶ä¿å­˜æˆåŠŸï¼š{os.path.abspath(M3U_OUTPUT)}{COLOR_RESET}")
    except Exception as e:
        safe_print(f"{COLOR_RED}âŒ M3Uæ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}{COLOR_RESET}")
        logging.error(f"M3Uä¿å­˜å¤±è´¥ï¼š{str(e)}")
        return False

    logging.info(f"ç»“æœä¿å­˜å®Œæˆ | TXTï¼š{len(organized_streams)} ä¸ªæº | M3Uï¼š{len(organized_streams)} ä¸ªæº")
    return True


if __name__ == "__main__":
    print_sep("IPTVç›´æ’­æºåˆ†ç±»æ•´ç†å·¥å…·ï¼ˆç»ˆæå®Œç¾ç‰ˆï¼‰", length=70)
    start_time = time.time()

    try:
        # æ­¥éª¤1ï¼šæ£€æµ‹ç½‘ç»œ
        if not check_network():
            raise Exception("ç½‘ç»œè¿æ¥å¼‚å¸¸ï¼Œæ— æ³•ç»§ç»­")

        # æ­¥éª¤2ï¼šè¯»å–åˆ†ç±»æ¨¡æ¿
        print_sep("æ­¥éª¤1/4ï¼šè¯»å–åˆ†ç±»æ¨¡æ¿")
        categories, all_channels = read_template()
        if not categories or not all_channels:
            raise Exception("æ¨¡æ¿è¯»å–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")

        # æ­¥éª¤3ï¼šæ‰¹é‡æŠ“å–ç›´æ’­æº
        print_sep("æ­¥éª¤2/4ï¼šæ‰¹é‡æŠ“å–ç›´æ’­æº")
        raw_content = batch_fetch(SOURCE_URLS)
        if not raw_content.strip():
            raise Exception("æœªæŠ“å–åˆ°æœ‰æ•ˆç›´æ’­æºå†…å®¹")

        # æ­¥éª¤4ï¼šæŒ‰æ¨¡æ¿æ•´ç†ç›´æ’­æºï¼ˆè§£æâ†’è¿‡æ»¤â†’æµ‹é€Ÿâ†’åˆ†ç»„ï¼‰
        print_sep("æ­¥éª¤3/4ï¼šæ•´ç†ç›´æ’­æº")
        organized_streams = organize_streams(raw_content, categories, all_channels)
        if not organized_streams:
            raise Exception("ç›´æ’­æºæ•´ç†åæ— æœ‰æ•ˆæ•°æ®")

        # æ­¥éª¤5ï¼šä¿å­˜ç»“æœåˆ°å›ºå®šæ–‡ä»¶
        print_sep("æ­¥éª¤4/4ï¼šä¿å­˜ç»“æœæ–‡ä»¶")
        save_success = save_results(organized_streams)
        if not save_success:
            raise Exception("ç»“æœæ–‡ä»¶ä¿å­˜å¤±è´¥")

        # æµç¨‹å®Œæˆ
        total_time = round(time.time() - start_time, 2)
        print_sep("å·¥å…·æ‰§è¡Œå®Œæˆ", length=70)
        safe_print(f"{COLOR_GREEN}ğŸ‰ æ‰€æœ‰æµç¨‹æˆåŠŸå®Œæˆï¼{COLOR_RESET}")
        safe_print(f"â±ï¸  æ€»è€—æ—¶ï¼š{total_time} ç§’")
        safe_print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š")
        safe_print(f"   - {os.path.abspath(TXT_OUTPUT)}")
        safe_print(f"   - {os.path.abspath(M3U_OUTPUT)}")
        safe_print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶ï¼š{os.path.abspath('iptv_tool.log')}")

    except Exception as e:
        print_sep("å·¥å…·æ‰§è¡Œå¤±è´¥", length=70)
        safe_print(f"{COLOR_RED}âŒ å¤±è´¥åŸå› ï¼š{str(e)}{COLOR_RESET}")
        logging.error(f"æ•´ä½“æµç¨‹å¤±è´¥ï¼š{str(e)}")
        exit(1)
