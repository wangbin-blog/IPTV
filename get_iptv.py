import requests
import pandas as pd
import re
import os
import time
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

# ======================== æ ¸å¿ƒé…ç½®åŒºï¼ˆå¯æŒ‰éœ€ä¿®æ”¹ï¼‰========================
SOURCE_URLS = [
    "https://raw.githubusercontent.com/zwc456baby/iptv_alive/master/live.txt",
    "https://live.zbds.top/tv/iptv6.txt",
    "https://live.zbds.top/tv/iptv4.txt",
]
CATEGORY_TEMPLATE_PATH = "demo.txt"  # åˆ†ç±»æ¨¡æ¿è·¯å¾„
MAX_INTERFACES_PER_CHANNEL = 8  # å•é¢‘é“æœ€å¤§æ¥å£æ•°
SPEED_TEST_TIMEOUT = 10  # æµ‹é€Ÿè¶…æ—¶ï¼ˆç§’ï¼‰
MAX_SPEED_TEST_WORKERS = 15  # æµ‹é€Ÿå¹¶å‘æ•°
MAX_FETCH_WORKERS = 5  # æŠ“å–å¹¶å‘æ•°ï¼ˆé¿å…è¯·æ±‚è¿‡è½½ï¼‰
OUTPUT_FILE_PREFIX = "iptv"  # è¾“å‡ºæ–‡ä»¶å‰ç¼€
OUTPUT_DIR = "iptv_results"  # è¾“å‡ºç›®å½•ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
CATEGORY_MARKER = "#genre#"  # æ¨¡æ¿åˆ†ç±»æ ‡è®°ï¼ˆå¦‚"å¤®è§†é¢‘é“,#genre#"ï¼‰
LOG_FILE = "iptv_tool.log"  # æ—¥å¿—æ–‡ä»¶è·¯å¾„
# =========================================================================

# åˆå§‹åŒ–é…ç½®ï¼ˆè¾“å‡ºç›®å½•ã€æ—¥å¿—ï¼‰
os.makedirs(OUTPUT_DIR, exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(os.path.join(OUTPUT_DIR, LOG_FILE), encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ­£åˆ™è¡¨è¾¾å¼ï¼ˆä¼˜åŒ–é€‚é… HTTPSï¼‰
IPV4_PATTERN = re.compile(r'^https?://(\d{1,3}\.){3}\d{1,3}')
IPV6_PATTERN = re.compile(r'^https?://\[([a-fA-F0-9:]+)\]')
URL_PATTERN = re.compile(r'^https?://')
SPACE_CLEAN_PATTERN = re.compile(r'\s+')


def print_separator(title: str = "", length: int = 70) -> None:
    """æ‰“å°åˆ†éš”çº¿ï¼Œä¼˜åŒ–æ—¥å¿—å¯è¯»æ€§"""
    sep = "=" * length
    if title:
        logger.info(f"\n{sep}")
        logger.info(f"ğŸ“Œ {title}")
        logger.info(sep)
    else:
        logger.info(sep)


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºæ ¼ã€æ¢è¡Œç¬¦"""
    return SPACE_CLEAN_PATTERN.sub("", str(text).strip())


def read_category_template(template_path: str) -> tuple[list[dict], list[str]] | tuple[None, None]:
    """è¯»å–åˆ†ç±»æ¨¡æ¿ï¼Œè¿”å›(åˆ†ç±»ç»“æ„, å»é‡é¢‘é“åˆ—è¡¨)æˆ–(None, None)"""
    if not os.path.exists(template_path):
        logger.error(f"æ¨¡æ¿æ–‡ä»¶ã€Œ{template_path}ã€ä¸å­˜åœ¨ï¼")
        logger.info(f"æ¨¡æ¿æ ¼å¼ç¤ºä¾‹ï¼š\n  {CATEGORY_MARKER} å¤®è§†é¢‘é“\n  CCTV1\n  CCTV2\n  {CATEGORY_MARKER} å«è§†é¢‘é“\n  æ¹–å—å«è§†")
        return None, None

    categories = []
    current_category = None
    all_channels = []

    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                # è·³è¿‡éåˆ†ç±»æ ‡è®°çš„æ³¨é‡Šè¡Œ
                if line.startswith("#") and not line.startswith(CATEGORY_MARKER):
                    continue

                # å¤„ç†åˆ†ç±»è¡Œ
                if line.startswith(CATEGORY_MARKER):
                    cat_name = clean_text(line.lstrip(CATEGORY_MARKER))
                    if not cat_name:
                        logger.warning(f"ç¬¬{line_num}è¡Œï¼šåˆ†ç±»åæ— æ•ˆï¼Œå¿½ç•¥")
                        current_category = None
                        continue
                    # åˆå¹¶é‡å¤åˆ†ç±»
                    existing = next((c for c in categories if c["category"] == cat_name), None)
                    if existing:
                        current_category = cat_name
                    else:
                        categories.append({"category": cat_name, "channels": []})
                        current_category = cat_name
                    continue

                # å¤„ç†é¢‘é“è¡Œ
                if current_category is None:
                    logger.warning(f"ç¬¬{line_num}è¡Œï¼šé¢‘é“æœªåˆ†ç±»ï¼Œå½’å…¥ã€Œæœªåˆ†ç±»ã€")
                    if not any(c["category"] == "æœªåˆ†ç±»" for c in categories):
                        categories.append({"category": "æœªåˆ†ç±»", "channels": []})
                    current_category = "æœªåˆ†ç±»"

                ch_name = clean_text(line.split(",")[0])
                if not ch_name:
                    logger.warning(f"ç¬¬{line_num}è¡Œï¼šé¢‘é“åæ— æ•ˆï¼Œå¿½ç•¥")
                    continue
                # å»é‡å¹¶æ·»åŠ åˆ°åˆ†ç±»
                if ch_name not in all_channels:
                    all_channels.append(ch_name)
                    for cat in categories:
                        if cat["category"] == current_category:
                            cat["channels"].append(ch_name)
                            break
    except Exception as e:
        logger.error(f"è¯»å–æ¨¡æ¿å¤±è´¥ï¼š{str(e)}")
        return None, None

    if not categories:
        logger.warning("æ¨¡æ¿æ— æœ‰æ•ˆåˆ†ç±»/é¢‘é“")
        return None, None

    # è¾“å‡ºæ¨¡æ¿ç»Ÿè®¡
    total_ch = sum(len(cat["channels"]) for cat in categories)
    logger.info(f"âœ… æ¨¡æ¿è¯»å–å®Œæˆ | åˆ†ç±»æ•°ï¼š{len(categories)} | é¢‘é“æ•°ï¼š{total_ch}")
    logger.info("  " + "-" * 50)
    for idx, cat in enumerate(categories, 1):
        logger.info(f"  {idx:2d}. åˆ†ç±»ï¼š{cat['category']:<20} é¢‘é“æ•°ï¼š{len(cat['channels']):2d}")
    logger.info("  " + "-" * 50)
    return categories, all_channels


def fetch_single_source(url: str) -> str | None:
    """æŠ“å–å•ä¸ªURLçš„ç›´æ’­æºå†…å®¹ï¼ˆä¼˜åŒ–è¶…æ—¶å’Œç¼–ç å¤„ç†ï¼‰"""
    logger.info(f"\nğŸ” æŠ“å–ï¼š{url}")
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/124.0.0.0 Safari/537.36",
            "Accept": "*/*",
            "Connection": "keep-alive"
        }
        # å¢åŠ è¿æ¥è¶…æ—¶å’Œè¯»å–è¶…æ—¶åŒºåˆ†
        resp = requests.get(
            url, 
            timeout=(5, SPEED_TEST_TIMEOUT),  # è¿æ¥è¶…æ—¶5sï¼Œè¯»å–è¶…æ—¶æŒ‰é…ç½®
            headers=headers, 
            allow_redirects=True,
            stream=False
        )
        resp.raise_for_status()
        # æ™ºèƒ½ç¼–ç å¤„ç†ï¼ˆè§£å†³ä¸­æ–‡ä¹±ç ï¼‰
        if not resp.encoding or resp.encoding.lower() == "iso-8859-1":
            resp.encoding = resp.apparent_encoding
        logger.info(f"âœ… æˆåŠŸ | é•¿åº¦ï¼š{len(resp.text):,} å­—ç¬¦")
        return resp.text
    except requests.exceptions.ConnectTimeout:
        logger.error(f"âŒ å¤±è´¥ï¼šè¿æ¥è¶…æ—¶ï¼ˆè¶…è¿‡5ç§’ï¼‰")
    except requests.exceptions.ReadTimeout:
        logger.error(f"âŒ å¤±è´¥ï¼šè¯»å–è¶…æ—¶ï¼ˆè¶…è¿‡{SPEED_TEST_TIMEOUT}ç§’ï¼‰")
    except requests.exceptions.ConnectionError:
        logger.error(f"âŒ å¤±è´¥ï¼šç½‘ç»œé”™è¯¯ï¼ˆæ— æ³•è¿æ¥ï¼‰")
    except requests.exceptions.HTTPError as e:
        logger.error(f"âŒ å¤±è´¥ï¼šHTTP {e.response.status_code}")
    except Exception as e:
        logger.error(f"âŒ å¤±è´¥ï¼š{str(e)[:50]}")
    return None


def batch_fetch_sources(url_list: list) -> str:
    """æ‰¹é‡æŠ“å–å¤šä¸ªURLçš„ç›´æ’­æºï¼ˆæ”¹ä¸ºå¹¶å‘æŠ“å–ï¼Œæå‡æ•ˆç‡ï¼‰"""
    if not url_list:
        logger.warning("URLåˆ—è¡¨ä¸ºç©º")
        return ""

    total = len(url_list)
    combined_content = []
    logger.info(f"ğŸ“¥ æ‰¹é‡æŠ“å– | æ€»URLæ•°ï¼š{total} | å¹¶å‘æ•°ï¼š{MAX_FETCH_WORKERS}")
    logger.info("-" * 70)

    # çº¿ç¨‹æ± å¹¶å‘æŠ“å–
    with ThreadPoolExecutor(max_workers=MAX_FETCH_WORKERS) as executor:
        # æäº¤æ‰€æœ‰æŠ“å–ä»»åŠ¡
        future_tasks = {executor.submit(fetch_single_source, url): url for url in url_list}
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(future_tasks):
            url = future_tasks[future]
            content = future.result()
            if content:
                combined_content.append(content)
            else:
                logger.info(f"â­ï¸  è·³è¿‡æ— æ•ˆURLï¼š{url}")
            logger.info("-" * 70)

    success_count = len(combined_content)
    logger.info(f"\nğŸ“Š æŠ“å–ç»Ÿè®¡ | æˆåŠŸï¼š{success_count} ä¸ª | å¤±è´¥ï¼š{total - success_count} ä¸ª")
    return "\n".join(combined_content)


def parse_m3u(content: str) -> list[dict]:
    """è§£æM3Uæ ¼å¼ç›´æ’­æºï¼ˆä¼˜åŒ–é¢‘é“åæå–é€»è¾‘ï¼‰"""
    if not content.strip():
        logger.warning("M3Uå†…å®¹ä¸ºç©º")
        return []

    streams = []
    current_program = None
    line_count = 0

    for line in content.splitlines():
        line_count += 1
        line = line.strip()
        # è§£æé¢‘é“åï¼ˆä¼˜å…ˆtvg-nameï¼Œå…¶æ¬¡ä»æè¿°æå–ï¼‰
        if line.startswith("#EXTINF"):
            # æå–tvg-name
            tvg_match = re.search(r'tvg-name=(["\']?)([^"\']+)\1', line)
            if tvg_match:
                current_program = clean_text(tvg_match.group(2))
            else:
                # ä»æè¿°å­—æ®µæå–ï¼ˆå…¼å®¹æ— tvg-nameçš„æ ¼å¼ï¼‰
                desc_match = re.search(r',([^,]+)$', line)
                if desc_match:
                    current_program = clean_text(desc_match.group(1))
            continue
        # è§£ææ’­æ”¾åœ°å€
        if URL_PATTERN.match(line) and current_program:
            streams.append({"program_name": current_program, "stream_url": line})
            current_program = None  # é‡ç½®ï¼Œé¿å…é‡å¤åŒ¹é…

    logger.info(f"ğŸ“Š M3Uè§£æ | æ€»è¡Œæ•°ï¼š{line_count:,} | æå–æºï¼š{len(streams)} ä¸ª")
    return streams


def parse_txt(content: str) -> list[dict]:
    """è§£æTXTæ ¼å¼ç›´æ’­æºï¼ˆé¢‘é“å,URLï¼‰ï¼ˆä¼˜åŒ–æ ¼å¼å…¼å®¹æ€§ï¼‰"""
    if not content.strip():
        logger.warning("TXTå†…å®¹ä¸ºç©º")
        return []

    streams = []
    line_count = 0
    valid_count = 0

    for line in content.splitlines():
        line_count += 1
        line = line.strip()
        # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
        if not line or line.startswith("#"):
            continue
        # å…¼å®¹ç©ºæ ¼åˆ†éš”å’Œé€—å·åˆ†éš”
        line = line.replace(" ", ",")  # ç©ºæ ¼è½¬é€—å·
        parts = [p.strip() for p in line.split(",") if p.strip()]
        if len(parts) >= 2 and URL_PATTERN.match(parts[-1]):
            program_name = clean_text(",".join(parts[:-1]))  # æ”¯æŒé¢‘é“åå«é€—å·
            stream_url = parts[-1]
            streams.append({"program_name": program_name, "stream_url": stream_url})
            valid_count += 1
        else:
            logger.warning(f"ç¬¬{line_count}è¡Œï¼šæ ¼å¼æ— æ•ˆï¼ˆéœ€ä¸ºã€Œé¢‘é“å,URLã€ï¼‰ï¼Œå¿½ç•¥")

    logger.info(f"ğŸ“Š TXTè§£æ | æ€»è¡Œæ•°ï¼š{line_count:,} | æœ‰æ•ˆè¡Œï¼š{valid_count} | æå–æºï¼š{len(streams)} ä¸ª")
    return streams


def test_stream_latency(stream_url: str, timeout: int) -> int | None:
    """æµ‹è¯•ç›´æ’­æºå»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰ï¼ˆä¼˜åŒ–é”™è¯¯æç¤ºå’Œç¨³å®šæ€§ï¼‰"""
    start_time = time.time()
    try:
        # ä¼˜å…ˆHEADè¯·æ±‚ï¼ˆè½»é‡ï¼‰ï¼Œå¤±è´¥é™çº§ä¸ºGETè¯·æ±‚ï¼ˆè¯»1å­—èŠ‚ï¼‰
        for method in [requests.head, requests.get]:
            with method(
                stream_url,
                timeout=timeout,
                allow_redirects=True,
                stream=(method == requests.get)
            ) as resp:
                if resp.status_code in [200, 206]:
                    if method == requests.get:
                        resp.iter_content(1).__next__()  # è¯»1å­—èŠ‚éªŒè¯
                    return int((time.time() - start_time) * 1000)
    except requests.exceptions.Timeout:
        logger.warning(f"âš ï¸ æµ‹é€Ÿè¶…æ—¶ï¼š{stream_url[:50]}{'...' if len(stream_url) > 50 else ''}")
    except requests.exceptions.ConnectionError:
        logger.warning(f"âš ï¸ æµ‹é€Ÿå¤±è´¥ï¼š{stream_url[:50]}{'...' if len(stream_url) > 50 else ''}ï¼ˆç½‘ç»œä¸å¯è¾¾ï¼‰")
    except Exception as e:
        logger.warning(f"âš ï¸ æµ‹é€Ÿé”™è¯¯ï¼š{stream_url[:50]}{'...' if len(stream_url) > 50 else ''}ï¼ˆ{str(e)[:30]}ï¼‰")
    return None


def batch_test_latency(stream_df: pd.DataFrame, max_workers: int, timeout: int) -> pd.DataFrame:
    """æ‰¹é‡æµ‹è¯•ç›´æ’­æºå»¶è¿Ÿï¼ˆä¼˜åŒ–ä»»åŠ¡ç®¡ç†å’Œç»Ÿè®¡ï¼‰"""
    if stream_df.empty:
        logger.warning("æ— ç›´æ’­æºå¯æµ‹è¯•å»¶è¿Ÿ")
        return pd.DataFrame(columns=["program_name", "stream_url", "latency_ms"])

    total_stream = len(stream_df)
    valid_results = []
    logger.info(f"âš¡ æ‰¹é‡æµ‹é€Ÿ | æ€»æµæ•°ï¼š{total_stream} | å¹¶å‘æ•°ï¼š{max_workers} | è¶…æ—¶ï¼š{timeout}ç§’")
    logger.info("-" * 100)

    # çº¿ç¨‹æ± å¹¶å‘æµ‹é€Ÿ
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_tasks = {
            executor.submit(test_stream_latency, row["stream_url"], timeout): (row["program_name"], row["stream_url"])
            for _, row in stream_df.iterrows()
        }

        # å¤„ç†ä»»åŠ¡ç»“æœ
        for task_idx, future in enumerate(as_completed(future_tasks), 1):
            prog_name, stream_url = future_tasks[future]
            latency = future.result()
            display_url = stream_url[:70] + "..." if len(stream_url) > 70 else stream_url

            if latency is not None:
                valid_results.append({
                    "program_name": prog_name,
                    "stream_url": stream_url,
                    "latency_ms": latency
                })
                logger.info(f"âœ… [{task_idx:3d}/{total_stream}] é¢‘é“ï¼š{prog_name:<20} URLï¼š{display_url:<75} å»¶è¿Ÿï¼š{latency:4d}ms")
            else:
                logger.info(f"âŒ [{task_idx:3d}/{total_stream}] é¢‘é“ï¼š{prog_name:<20} URLï¼š{display_url:<75} çŠ¶æ€ï¼šæ— æ•ˆ")

    # è½¬æ¢ä¸ºDataFrameå¹¶æ’åº
    latency_df = pd.DataFrame(valid_results)
    if not latency_df.empty:
        latency_df = latency_df.sort_values("latency_ms").reset_index(drop=True)

    # è¾“å‡ºæµ‹é€Ÿç»Ÿè®¡
    logger.info("-" * 100)
    logger.info(f"ğŸ æµ‹é€Ÿå®Œæˆ | æœ‰æ•ˆæµï¼š{len(latency_df)} ä¸ª | æ— æ•ˆæµï¼š{total_stream - len(latency_df)} ä¸ª")
    if len(latency_df) > 0:
        avg_latency = int(latency_df["latency_ms"].mean())
        logger.info(f"ğŸ“Š å»¶è¿Ÿç»Ÿè®¡ | æœ€å¿«ï¼š{latency_df['latency_ms'].min()}ms | æœ€æ…¢ï¼š{latency_df['latency_ms'].max()}ms | å¹³å‡ï¼š{avg_latency}ms")
    return latency_df


def organize_streams(content: str, categories: list[dict], all_channels: list) -> list[dict]:
    """æŒ‰åˆ†ç±»æ•´ç†ç›´æ’­æºï¼ˆä¼˜åŒ–åŒ¹é…é€»è¾‘å’Œæ’åºï¼‰"""
    logger.info("\nğŸ”§ å¼€å§‹æ•´ç†ç›´æ’­æºï¼ˆ4ä¸ªæ­¥éª¤ï¼‰")
    logger.info("-" * 70)

    # æ­¥éª¤1ï¼šè‡ªåŠ¨è¯†åˆ«æ ¼å¼å¹¶è§£æ
    if content.startswith("#EXTM3U") or "#EXTINF" in content[:100]:
        logger.info("ğŸ”§ æ­¥éª¤1/4ï¼šè¯†åˆ«ä¸ºM3Uæ ¼å¼ï¼Œå¼€å§‹è§£æ...")
        parsed_streams = parse_m3u(content)
    else:
        logger.info("ğŸ”§ æ­¥éª¤1/4ï¼šè¯†åˆ«ä¸ºTXTæ ¼å¼ï¼Œå¼€å§‹è§£æ...")
        parsed_streams = parse_txt(content)

    stream_df = pd.DataFrame(parsed_streams)
    if stream_df.empty:
        logger.error("æ•´ç†å¤±è´¥ï¼šè§£æåæ— æœ‰æ•ˆç›´æ’­æµ")
        return []

    # æ­¥éª¤2ï¼šæŒ‰æ¨¡æ¿è¿‡æ»¤é¢‘é“ï¼ˆæ¨¡ç³ŠåŒ¹é…ä¼˜åŒ–ï¼‰
    logger.info(f"\nğŸ”§ æ­¥éª¤2/4ï¼šæŒ‰æ¨¡æ¿è¿‡æ»¤é¢‘é“...")
    stream_df["program_clean"] = stream_df["program_name"].apply(clean_text)
    template_clean = [clean_text(ch) for ch in all_channels]
    # æ¨¡ç³ŠåŒ¹é…ï¼ˆå…¼å®¹é¢‘é“åç»†å¾®å·®å¼‚ï¼Œå¦‚"CCTV1"å’Œ"CCTV-1"ï¼‰
    def fuzzy_match(clean_name):
        return any(tpl in clean_name or clean_name in tpl for tpl in template_clean)
    filtered_df = stream_df[stream_df["program_clean"].apply(fuzzy_match)].copy()
    filtered_df = filtered_df.drop_duplicates(subset=["program_name", "stream_url"]).reset_index(drop=True)

    if filtered_df.empty:
        logger.error("æ•´ç†å¤±è´¥ï¼šæ— åŒ¹é…æ¨¡æ¿çš„é¢‘é“")
        return []
    logger.info(f"  ç»“æœ | åŸå§‹ï¼š{len(stream_df)} ä¸ª | åŒ¹é…ï¼š{len(filtered_df)} ä¸ª | è¿‡æ»¤ï¼š{len(stream_df)-len(filtered_df)} ä¸ª")

    # æ­¥éª¤3ï¼šæ‰¹é‡æµ‹é€Ÿ
    logger.info(f"\nğŸ”§ æ­¥éª¤3/4ï¼šæ‰¹é‡æµ‹é€Ÿ...")
    valid_df = batch_test_latency(filtered_df[["program_name", "stream_url"]], MAX_SPEED_TEST_WORKERS, SPEED_TEST_TIMEOUT)
    if valid_df.empty:
        logger.error("æ•´ç†å¤±è´¥ï¼šæ‰€æœ‰æºæµ‹é€Ÿå¤±è´¥")
        return []

    # æ­¥éª¤4ï¼šæŒ‰åˆ†ç±»æ•´ç†ï¼ˆä¼˜åŒ–æ’åºå’Œæ¥å£é™åˆ¶ï¼‰
    logger.info(f"\nğŸ”§ æ­¥éª¤4/4ï¼šæŒ‰åˆ†ç±»æ•´ç†...")
    organized_data = []
    for cat in categories:
        cat_name = cat["category"]
        cat_ch_clean = [clean_text(ch) for ch in cat["channels"]]
        # åŒ¹é…åˆ†ç±»ä¸‹çš„é¢‘é“
        cat_df = valid_df[valid_df["program_name"].apply(clean_text).isin(cat_ch_clean)].copy()
        if cat_df.empty:
            logger.warning(f"åˆ†ç±»ã€Œ{cat_name}ã€ï¼šæ— æœ‰æ•ˆæºï¼Œè·³è¿‡")
            continue

        # æŒ‰æ¨¡æ¿é¡ºåºæ’åºï¼ˆä¼˜å…ˆæ¨¡æ¿é¡ºåºï¼Œå†æŒ‰å»¶è¿Ÿï¼‰
        ch_order = {clean_text(ch): idx for idx, ch in enumerate(cat["channels"])}
        cat_df["order"] = cat_df["program_clean"].map(ch_order).fillna(999)
        cat_df_sorted = cat_df.sort_values(["order", "latency_ms"]).reset_index(drop=True)

        # é™åˆ¶å•é¢‘é“æ¥å£æ•°
        def limit_interfaces(group):
            limited = group.head(MAX_INTERFACES_PER_CHANNEL)
            return pd.Series({
                "stream_urls": limited["stream_url"].tolist(),
                "interface_count": len(limited)
            })
        cat_grouped = cat_df_sorted.groupby("program_name").apply(limit_interfaces).reset_index()
        cat_grouped = cat_grouped[cat_grouped["interface_count"] > 0].reset_index(drop=True)

        # æ•´ç†åˆ†ç±»ç»“æœ
        cat_result = []
        for _, row in cat_grouped.iterrows():
            cat_result.append({
                "program_name": row["program_name"],
                "interface_count": row["interface_count"],
                "stream_urls": row["stream_urls"]
            })

        organized_data.append({"category": cat_name, "channels": cat_result})

    if not organized_data:
        logger.error("æ•´ç†å¤±è´¥ï¼šæ— æœ‰æ•ˆåˆ†ç±»ç»“æœ")
        return []

    # è¾“å‡ºæ•´ç†ç»Ÿè®¡
    total_cats = len(organized_data)
    total_chs = sum(len(cat["channels"]) for cat in organized_data)
    total_ifs = sum(ch["interface_count"] for cat in organized_data for ch in cat["channels"])
    logger.info(f"\nâœ… æ•´ç†å®Œæˆ | åˆ†ç±»ï¼š{total_cats} ä¸ª | é¢‘é“ï¼š{total_chs} ä¸ª | æ¥å£ï¼š{total_ifs} ä¸ª")
    return organized_data


def save_organized_results(organized_data: list[dict]) -> None:
    """ä¿å­˜æ•´ç†ç»“æœä¸ºTXTå’ŒM3Uæ–‡ä»¶ï¼ˆä¼˜åŒ–æ–‡ä»¶ç»“æ„å’Œä¿¡æ¯å®Œæ•´æ€§ï¼‰"""
    if not organized_data:
        logger.warning("æ— æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
        return

    # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
    total_cats = len(organized_data)
    total_chs = sum(len(cat["channels"]) for cat in organized_data)
    total_ifs = sum(ch["interface_count"] for cat in organized_data for ch in cat["channels"])
    timestamp = time.strftime("%Y%m%d_%H%M%S", time.localtime())
    basic_info = [
        f"# IPTVç›´æ’­æºï¼ˆæŒ‰åˆ†ç±»æ•´ç†ï¼‰",
        f"# ç”Ÿæˆæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}",
        f"# æ€»åˆ†ç±»æ•°ï¼š{total_cats} | æ€»é¢‘é“æ•°ï¼š{total_chs} | æ€»æ¥å£æ•°ï¼š{total_ifs}",
        f"# å•é¢‘é“æœ€å¤§æ¥å£æ•°ï¼š{MAX_INTERFACES_PER_CHANNEL}",
        f"# æµ‹é€Ÿè¶…æ—¶æ—¶é—´ï¼š{SPEED_TEST_TIMEOUT}ç§’"
    ]

    # 1. ä¿å­˜TXTæ–‡ä»¶ï¼ˆä¼˜åŒ–IPv4/IPv6åˆ†ç±»æ˜¾ç¤ºï¼‰
    txt_filename = os.path.join(OUTPUT_DIR, f"{OUTPUT_FILE_PREFIX}_TXT_{timestamp}_é™{MAX_INTERFACES_PER_CHANNEL}æ¥å£.txt")
    try:
        with open(txt_filename, 'w', encoding='utf-8') as f:
            f.write("\n".join(basic_info) + "\n\n")
            for cat in organized_data:
                cat_ifs_total = sum(ch["interface_count"] for ch in cat["channels"])
                f.write(f"{CATEGORY_MARKER} {cat['category']}\n")
                f.write(f"# åˆ†ç±»é¢‘é“æ•°ï¼š{len(cat['channels'])} | åˆ†ç±»æ¥å£æ•°ï¼š{cat_ifs_total}\n\n")
                for ch in cat["channels"]:
                    f.write(f"# {ch['program_name']}ï¼ˆ{ch['interface_count']}ä¸ªæ¥å£ï¼‰\n")
                    # åŒºåˆ†IPv4/IPv6
                    ipv4_urls = [url for url in ch['stream_urls'] if IPV4_PATTERN.match(url)]
                    ipv6_urls = [url for url in ch['stream_urls'] if IPV6_PATTERN.match(url)]
                    other_urls = [url for url in ch['stream_urls'] if not (IPV4_PATTERN.match(url) or IPV6_PATTERN.match(url))]
                    # å†™å…¥å„ç±»å‹æ¥å£
                    if ipv4_urls:
                        f.write("# --- IPv4 æ¥å£ ---\n")
                        f.write("\n".join([f"{ch['program_name']},{url}" for url in ipv4_urls]) + "\n\n")
                    if ipv6_urls:
                        f.write("# --- IPv6 æ¥å£ ---\n")
                        f.write("\n".join([f"{ch['program_name']},{url}" for url in ipv6_urls]) + "\n\n")
                    if other_urls:
                        f.write("# --- å…¶ä»– æ¥å£ ---\n")
                        f.write("\n".join([f"{ch['program_name']},{url}" for url in other_urls]) + "\n\n")
        logger.info(f"\nğŸ“„ TXTæ–‡ä»¶ä¿å­˜æˆåŠŸ | è·¯å¾„ï¼š{os.path.abspath(txt_filename)}")
    except Exception as e:
        logger.error(f"âŒ TXTæ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}")

    # 2. ä¿å­˜M3Uæ–‡ä»¶ï¼ˆä¼˜åŒ–æ’­æ”¾å™¨å…¼å®¹æ€§ï¼‰
    m3u_filename = os.path.join(OUTPUT_DIR, f"{OUTPUT_FILE_PREFIX}_M3U_{timestamp}_é™{MAX_INTERFACES_PER_CHANNEL}æ¥å£.m3u")
    try:
        with open(m3u_filename, 'w', encoding='utf-8') as f:
            f.write("#EXTM3U\n")
            f.write("\n".join(basic_info[1:]) + "\n\n")  # å»æ‰é¦–è¡Œï¼ˆM3Uæ ‡å‡†ï¼‰
            for cat in organized_data:
                f.write(f"# {CATEGORY_MARKER} {cat['category']}\n")
                for ch in cat["channels"]:
                    ch_remark = f"é¢‘é“ï¼š{ch['program_name']} | æ¥å£æ•°ï¼š{ch['interface_count']}"
                    f.write(f"# {ch_remark}\n")
                    for idx, url in enumerate(ch['stream_urls'], 1):
                        # å¢åŠ tvg-idå’Œtvg-logoå ä½ï¼ˆæå‡æ’­æ”¾å™¨æ˜¾ç¤ºæ•ˆæœï¼‰
                        f.write(f'#EXTINF:-1 tvg-id="{ch['program_name']}" tvg-name="{ch['program_name']}" tvg-logo="" group-title="{cat['category']}",{ch['program_name']}_{idx}\n')
                        f.write(f"{url}\n")
                f.write("\n")
        logger.info(f"ğŸ“º M3Uæ–‡ä»¶ä¿å­˜æˆåŠŸ | è·¯å¾„ï¼š{os.path.abspath(m3u_filename)}")
    except Exception as e:
        logger.error(f"âŒ M3Uæ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}")


if __name__ == "__main__":
    print_separator("IPTVç›´æ’­æºåˆ†ç±»æ•´ç†å·¥å…·ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    
    try:
        # æ­¥éª¤1ï¼šè¯»å–åˆ†ç±»æ¨¡æ¿
        logger.info("\nã€æ­¥éª¤1ï¼šè¯»å–åˆ†ç±»æ¨¡æ¿ã€‘")
        categories, all_channels = read_category_template(CATEGORY_TEMPLATE_PATH)
        if not categories or not all_channels:
            raise Exception("æ¨¡æ¿è¯»å–å¤±è´¥")

        # æ­¥éª¤2ï¼šæ‰¹é‡æŠ“å–ç›´æ’­æº
        logger.info("\nã€æ­¥éª¤2ï¼šæ‰¹é‡æŠ“å–ç›´æ’­æºã€‘")
        raw_content = batch_fetch_sources(SOURCE_URLS)
        if not raw_content.strip():
            raise Exception("æœªæŠ“å–åˆ°ä»»ä½•ç›´æ’­æºå†…å®¹")

        # æ­¥éª¤3ï¼šæŒ‰åˆ†ç±»æ•´ç†ç›´æ’­æº
        logger.info("\nã€æ­¥éª¤3ï¼šæŒ‰åˆ†ç±»æ•´ç†ç›´æ’­æºã€‘")
        organized_data = organize_streams(raw_content, categories, all_channels)
        if not organized_data:
            raise Exception("ç›´æ’­æºæ•´ç†å¤±è´¥")

        # æ­¥éª¤4ï¼šä¿å­˜ç»“æœæ–‡ä»¶
        logger.info("\nã€æ­¥éª¤4ï¼šä¿å­˜ç»“æœæ–‡ä»¶ã€‘")
        save_organized_results(organized_data)

        print_separator("æµç¨‹å®Œæˆ")
        logger.info("ğŸ‰ æ‰€æœ‰æ“ä½œæ‰§è¡Œå®Œæˆï¼ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³ï¼š" + os.path.abspath(OUTPUT_DIR))
    except Exception as e:
        print_separator("æµç¨‹ç»ˆæ­¢")
        logger.error(f"âŒ æµç¨‹ç»ˆæ­¢ï¼š{str(e)}")
        exit(1)
