import requests
import pandas as pd
import re
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

# ======================== æ ¸å¿ƒé…ç½®åŒºï¼ˆå¯æŒ‰éœ€ä¿®æ”¹ï¼‰========================
SOURCE_URLS = [
    "https://raw.githubusercontent.com/zwc456baby/iptv_alive/master/live.txt",
    "https://live.zbds.top/tv/iptv6.txt",
    "https://live.zbds.top/tv/iptv4.txt",
]
CATEGORY_TEMPLATE_PATH = "iptv_channels_template.txt"  # åˆ†ç±»æ¨¡æ¿è·¯å¾„
MAX_INTERFACES_PER_CHANNEL = 5  # å•é¢‘é“æœ€å¤§æ¥å£æ•°
SPEED_TEST_TIMEOUT = 8  # æµ‹é€Ÿè¶…æ—¶ï¼ˆç§’ï¼‰
MAX_SPEED_TEST_WORKERS = 15  # æµ‹é€Ÿå¹¶å‘æ•°
OUTPUT_FILE_PREFIX = "iptv_organized"  # è¾“å‡ºæ–‡ä»¶å‰ç¼€
CATEGORY_MARKER = "##"  # æ¨¡æ¿åˆ†ç±»æ ‡è®°ï¼ˆå¦‚"## å¤®è§†é¢‘é“"ï¼‰
# =========================================================================

# æ­£åˆ™è¡¨è¾¾å¼ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
IPV4_PATTERN = re.compile(r'^http://(\d{1,3}\.){3}\d{1,3}')
IPV6_PATTERN = re.compile(r'^http://\[([a-fA-F0-9:]+)\]')
URL_PATTERN = re.compile(r'^https?://')
SPACE_CLEAN_PATTERN = re.compile(r'\s+')


def print_separator(title: str = "", length: int = 70) -> None:
    """æ‰“å°åˆ†éš”çº¿ï¼Œä¼˜åŒ–æ—¥å¿—å¯è¯»æ€§"""
    if title:
        print(f"\n{'=' * length}")
        print(f"ğŸ“Œ {title}")
        print(f"{'=' * length}")
    else:
        print(f"{'=' * length}")


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºæ ¼ã€æ¢è¡Œç¬¦"""
    return SPACE_CLEAN_PATTERN.sub("", str(text).strip())


def read_category_template(template_path: str) -> tuple[list[dict], list[str]] | tuple[None, None]:
    """è¯»å–åˆ†ç±»æ¨¡æ¿ï¼Œè¿”å›(åˆ†ç±»ç»“æ„, å»é‡é¢‘é“åˆ—è¡¨)æˆ–(None, None)"""
    if not os.path.exists(template_path):
        print(f"âŒ é”™è¯¯ï¼šæ¨¡æ¿æ–‡ä»¶ã€Œ{template_path}ã€ä¸å­˜åœ¨ï¼")
        print(f"ğŸ“ æ¨¡æ¿æ ¼å¼ç¤ºä¾‹ï¼š\n  {CATEGORY_MARKER} å¤®è§†é¢‘é“\n  CCTV1\n  CCTV2\n  {CATEGORY_MARKER} å«è§†é¢‘é“\n  æ¹–å—å«è§†")
        return None, None

    categories = []
    current_category = None
    all_channels = []

    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                if line.startswith("#") and not line.startswith(CATEGORY_MARKER):
                    continue

                # å¤„ç†åˆ†ç±»è¡Œ
                if line.startswith(CATEGORY_MARKER):
                    cat_name = clean_text(line.lstrip(CATEGORY_MARKER))
                    if not cat_name:
                        print(f"âš ï¸ ç¬¬{line_num}è¡Œï¼šåˆ†ç±»åæ— æ•ˆï¼Œå¿½ç•¥")
                        current_category = None
                        continue
                    # åˆå¹¶é‡å¤åˆ†ç±»
                    existing = next((c for c in categories if c["category"] == cat_name), None)
                    if existing:
                        current_category = cat_name
                    else:
                        categories.append({"category": cat_name, "channels": []})
                        current_category = cat_name
                    continue

                # å¤„ç†é¢‘é“è¡Œ
                if current_category is None:
                    print(f"âš ï¸ ç¬¬{line_num}è¡Œï¼šé¢‘é“æœªåˆ†ç±»ï¼Œå½’å…¥ã€Œæœªåˆ†ç±»ã€")
                    if not any(c["category"] == "æœªåˆ†ç±»" for c in categories):
                        categories.append({"category": "æœªåˆ†ç±»", "channels": []})
                    current_category = "æœªåˆ†ç±»"

                ch_name = clean_text(line.split(",")[0])
                if not ch_name:
                    print(f"âš ï¸ ç¬¬{line_num}è¡Œï¼šé¢‘é“åæ— æ•ˆï¼Œå¿½ç•¥")
                    continue
                if ch_name not in all_channels:
                    all_channels.append(ch_name)
                    for cat in categories:
                        if cat["category"] == current_category:
                            cat["channels"].append(ch_name)
                            break
    except Exception as e:
        print(f"âŒ è¯»å–æ¨¡æ¿å¤±è´¥ï¼š{str(e)}")
        return None, None

    if not categories:
        print("âš ï¸ è­¦å‘Šï¼šæ¨¡æ¿æ— æœ‰æ•ˆåˆ†ç±»/é¢‘é“")
        return None, None

    total_ch = sum(len(cat["channels"]) for cat in categories)
    print(f"âœ… æ¨¡æ¿è¯»å–å®Œæˆ | åˆ†ç±»æ•°ï¼š{len(categories)} | é¢‘é“æ•°ï¼š{total_ch}")
    print("  " + "-" * 50)
    for idx, cat in enumerate(categories, 1):
        print(f"  {idx:2d}. åˆ†ç±»ï¼š{cat['category']:<20} é¢‘é“æ•°ï¼š{len(cat['channels']):2d}")
    print("  " + "-" * 50)
    return categories, all_channels


def fetch_single_source(url: str) -> str | None:
    """æŠ“å–å•ä¸ªURLçš„ç›´æ’­æºå†…å®¹"""
    print(f"\nğŸ” æŠ“å–ï¼š{url}")
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/124.0.0.0 Safari/537.36"
        }
        resp = requests.get(url, timeout=10, headers=headers, allow_redirects=True)
        resp.raise_for_status()
        if not resp.encoding or resp.encoding.lower() == "iso-8859-1":
            resp.encoding = resp.apparent_encoding
        print(f"âœ… æˆåŠŸ | é•¿åº¦ï¼š{len(resp.text):,} å­—ç¬¦")
        return resp.text
    except requests.exceptions.Timeout:
        print(f"âŒ å¤±è´¥ï¼šè¯·æ±‚è¶…æ—¶")
    except requests.exceptions.ConnectionError:
        print(f"âŒ å¤±è´¥ï¼šç½‘ç»œé”™è¯¯")
    except requests.exceptions.HTTPError as e:
        print(f"âŒ å¤±è´¥ï¼šHTTP {e.response.status_code}")
    except Exception as e:
        print(f"âŒ å¤±è´¥ï¼š{str(e)[:50]}")
    return None


def batch_fetch_sources(url_list: list) -> str:
    """æ‰¹é‡æŠ“å–å¤šä¸ªURLçš„ç›´æ’­æº"""
    if not url_list:
        print("âš ï¸ è­¦å‘Šï¼šURLåˆ—è¡¨ä¸ºç©º")
        return ""

    total = len(url_list)
    success = 0
    combined = []
    print(f"ğŸ“¥ æ‰¹é‡æŠ“å– | æ€»URLæ•°ï¼š{total}")
    print("-" * 70)

    for url in url_list:
        content = fetch_single_source(url)
        if content:
            combined.append(content)
            success += 1
        else:
            print(f"â­ï¸  è·³è¿‡æ— æ•ˆURLï¼š{url}")
        print("-" * 70)

    print(f"\nğŸ“Š æŠ“å–ç»Ÿè®¡ | æˆåŠŸï¼š{success} ä¸ª | å¤±è´¥ï¼š{total - success} ä¸ª")
    return "\n".join(combined)


def parse_m3u(content: str) -> list[dict]:
    """è§£æM3Uæ ¼å¼ç›´æ’­æº"""
    if not content.strip():
        print("âš ï¸ è­¦å‘Šï¼šM3Uå†…å®¹ä¸ºç©º")
        return []

    streams = []
    current_prog = None
    line_count = 0

    for line in content.splitlines():
        line_count += 1
        line = line.strip()
        if line.startswith("#EXTINF"):
            match = re.search(r'tvg-name=(["\']?)([^"\']+)\1', line)
            if match:
                current_prog = clean_text(match.group(2))
            continue
        if URL_PATTERN.match(line) and current_prog:
            streams.append({"program_name": current_prog, "stream_url": line})
            current_prog = None

    print(f"ğŸ“Š M3Uè§£æ | æ€»è¡Œæ•°ï¼š{line_count:,} | æå–æºï¼š{len(streams)} ä¸ª")
    return streams


def parse_txt(content: str) -> list[dict]:
    """è§£æTXTæ ¼å¼ç›´æ’­æºï¼ˆé¢‘é“å,URLï¼‰"""
    if not content.strip():
        print("âš ï¸ è­¦å‘Šï¼šTXTå†…å®¹ä¸ºç©º")
        return []

    streams = []
    line_count = 0
    valid = 0

    for line in content.splitlines():
        line_count += 1
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        match = re.match(r'(.+?)\s*,\s*(https?://.+)$', line)
        if match:
            prog = clean_text(match.group(1))
            url = match.group(2).strip()
            if prog and url:
                streams.append({"program_name": prog, "stream_url": url})
                valid += 1
        else:
            print(f"âš ï¸ ç¬¬{line_count}è¡Œï¼šæ ¼å¼æ— æ•ˆï¼Œå¿½ç•¥")

    print(f"ğŸ“Š TXTè§£æ | æ€»è¡Œæ•°ï¼š{line_count:,} | æœ‰æ•ˆè¡Œï¼š{valid} | æå–æºï¼š{len(streams)} ä¸ª")
    return streams


def test_stream_latency(stream_url: str, timeout: int) -> int | None:
    """æµ‹è¯•ç›´æ’­æºå»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰ï¼Œä¼˜å…ˆHEADè¯·æ±‚"""
    start = time.time()
    try:
        resp = requests.head(stream_url, timeout=timeout, allow_redirects=True)
        if resp.status_code in [200, 206]:
            return int((time.time() - start) * 1000)
        resp = requests.get(stream_url, timeout=timeout, allow_redirects=True, stream=True)
        if resp.status_code in [200, 206]:
            resp.iter_content(1).__next__()
            return int((time.time() - start) * 1000)
    except Exception:
        pass
    return None


def batch_test_latency(stream_df: pd.DataFrame, max_workers: int, timeout: int) -> pd.DataFrame:
    """æ‰¹é‡æµ‹è¯•ç›´æ’­æºå»¶è¿Ÿï¼Œè¿”å›æŒ‰å»¶è¿Ÿæ’åºçš„æœ‰æ•ˆæº"""
    if stream_df.empty:
        print("âš ï¸ è­¦å‘Šï¼šæ— ç›´æ’­æºå¯æµ‹è¯•")
        return pd.DataFrame(columns=["program_name", "stream_url", "latency_ms"])

    total = len(stream_df)
    results = []
    print(f"âš¡ æ‰¹é‡æµ‹é€Ÿ | æºæ•°ï¼š{total} | å¹¶å‘ï¼š{max_workers} | è¶…æ—¶ï¼š{timeout}ç§’")
    print("-" * 95)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_map = {
            executor.submit(test_stream_latency, row["stream_url"], timeout): (row["program_name"], row["stream_url"])
            for _, row in stream_df.iterrows()
        }

        for idx, future in enumerate(as_completed(future_map), 1):
            prog, url = future_map[future]
            latency = future.result()
            display_url = url[:65] + "..." if len(url) > 65 else url

            if latency is not None:
                results.append({"program_name": prog, "stream_url": url, "latency_ms": latency})
                print(f"âœ… [{idx:3d}/{total}] é¢‘é“ï¼š{prog:<20} URLï¼š{display_url:<70} å»¶è¿Ÿï¼š{latency:4d}ms")
            else:
                print(f"âŒ [{idx:3d}/{total}] é¢‘é“ï¼š{prog:<20} URLï¼š{display_url:<70} çŠ¶æ€ï¼šå¤±è´¥")

    latency_df = pd.DataFrame(results)
    if not latency_df.empty:
        latency_df = latency_df.sort_values("latency_ms").reset_index(drop=True)

    print("-" * 95)
    print(f"ğŸ æµ‹é€Ÿå®Œæˆ | æœ‰æ•ˆï¼š{len(latency_df)} ä¸ª | æ— æ•ˆï¼š{total - len(latency_df)} ä¸ª")
    if len(latency_df) > 0:
        avg = latency_df["latency_ms"].mean()
        print(f"ğŸ“Š ç»Ÿè®¡ | æœ€å¿«ï¼š{latency_df['latency_ms'].min()}ms | æœ€æ…¢ï¼š{latency_df['latency_ms'].max()}ms | å¹³å‡ï¼š{avg:.0f}ms")
    return latency_df


def organize_streams(content: str, categories: list[dict], all_channels: list) -> list[dict]:
    """æŒ‰åˆ†ç±»æ•´ç†ç›´æ’­æºï¼šè§£æâ†’è¿‡æ»¤â†’æµ‹é€Ÿâ†’é™åˆ¶æ¥å£æ•°"""
    # æ­¥éª¤1ï¼šè§£æ
    if content.startswith("#EXTINF"):
        print("\nğŸ”§ æ­¥éª¤1/4ï¼šè§£æM3Uæ ¼å¼...")
        parsed = parse_m3u(content)
    else:
        print("\nğŸ”§ æ­¥éª¤1/4ï¼šè§£æTXTæ ¼å¼...")
        parsed = parse_txt(content)
    stream_df = pd.DataFrame(parsed)
    if stream_df.empty:
        print("âŒ æ•´ç†å¤±è´¥ï¼šæ— è§£æç»“æœ")
        return []

    # æ­¥éª¤2ï¼šè¿‡æ»¤+å»é‡
    print(f"\nğŸ”§ æ­¥éª¤2/4ï¼šè¿‡æ»¤å¹¶å»é‡...")
    stream_df["program_clean"] = stream_df["program_name"].apply(clean_text)
    template_clean = [clean_text(ch) for ch in all_channels]
    filtered_df = stream_df[stream_df["program_clean"].isin(template_clean)].copy()
    filtered_df = filtered_df.drop_duplicates(subset=["program_name", "stream_url"]).reset_index(drop=True)
    if filtered_df.empty:
        print("âŒ æ•´ç†å¤±è´¥ï¼šæ— åŒ¹é…æ¨¡æ¿çš„é¢‘é“")
        return []
    print(f"  ç»“æœ | åŸå§‹ï¼š{len(stream_df)} ä¸ª | åŒ¹é…ï¼š{len(filtered_df)} ä¸ª | è¿‡æ»¤ï¼š{len(stream_df)-len(filtered_df)} ä¸ª")

    # æ­¥éª¤3ï¼šæµ‹é€Ÿ
    print(f"\nğŸ”§ æ­¥éª¤3/4ï¼šæ‰¹é‡æµ‹é€Ÿ...")
    valid_df = batch_test_latency(filtered_df[["program_name", "stream_url"]], MAX_SPEED_TEST_WORKERS, SPEED_TEST_TIMEOUT)
    if valid_df.empty:
        print("âŒ æ•´ç†å¤±è´¥ï¼šæ‰€æœ‰æºæµ‹é€Ÿå¤±è´¥")
        return []

    # æ­¥éª¤4ï¼šåˆ†ç±»æ•´ç†
    print(f"\nğŸ”§ æ­¥éª¤4/4ï¼šæŒ‰åˆ†ç±»æ•´ç†...")
    organized = []
    for cat in categories:
        cat_name = cat["category"]
        cat_ch_clean = [clean_text(ch) for ch in cat["channels"]]
        cat_df = valid_df[valid_df["program_name"].apply(clean_text).isin(cat_ch_clean)].copy()

        if cat_df.empty:
            print(f"âš ï¸ åˆ†ç±»ã€Œ{cat_name}ã€ï¼šæ— æœ‰æ•ˆæºï¼Œè·³è¿‡")
            continue

        # æŒ‰æ¨¡æ¿é¡ºåºæ’åº
        ch_order = {ch: idx for idx, ch in enumerate(cat["channels"])}
        cat_df["order"] = cat_df["program_name"].apply(
            lambda x: ch_order.get(next((ch for ch in cat["channels"] if clean_text(ch) == clean_text(x)), ""), 999)
        )
        cat_df_sorted = cat_df.sort_values(["order", "latency_ms"]).reset_index(drop=True)

        # é™åˆ¶å•é¢‘é“æ¥å£æ•°
        def limit_ifs(group):
            limited = group.head(MAX_INTERFACES_PER_CHANNEL)
            return pd.Series({
                "stream_urls": limited["stream_url"].tolist(),
                "interface_count": len(limited)
            })
        cat_grouped = cat_df_sorted.groupby("program_name").apply(limit_ifs).reset_index()
        cat_grouped = cat_grouped[cat_grouped["interface_count"] > 0].reset_index(drop=True)

        # æ•´ç†åˆ†ç±»ç»“æœ
        cat_result = []
        for _, row in cat_grouped.iterrows():
            cat_result.append({
                "program_name": row["program_name"],
                "interface_count": row["interface_count"],
                "stream_urls": row["stream_urls"]
            })

        organized.append({"category": cat_name, "channels": cat_result})

    if not organized:
        print("âŒ æ•´ç†å¤±è´¥ï¼šæ— æœ‰æ•ˆåˆ†ç±»ç»“æœ")
        return []

    # ç»Ÿè®¡ç»“æœ
    total_cats = len(organized)
    total_chs = sum(len(cat["channels"]) for cat in organized)
    total_ifs = sum(ch["interface_count"] for cat in organized for ch in cat["channels"])
    print(f"\nâœ… æ•´ç†å®Œæˆ | åˆ†ç±»ï¼š{total_cats} ä¸ª | é¢‘é“ï¼š{total_chs} ä¸ª | æ¥å£ï¼š{total_ifs} ä¸ª")
    return organized


def save_organized_results(organized_data: list[dict]) -> None:
    """ä¿å­˜æ•´ç†ç»“æœä¸ºTXTå’ŒM3Uæ–‡ä»¶"""
    if not organized_data:
        print("âš ï¸ æ— æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
        return

    total_cats = len(organized_data)
    total_chs = sum(len(cat["channels"]) for cat in organized_data)
    total_ifs = sum(ch["interface_count"] for cat in organized_data for ch in cat["channels
