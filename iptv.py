import requests
import pandas as pd
import re
import os
import time
import concurrent.futures
from typing import List, Dict, Optional, Tuple, Set
from urllib.parse import urlparse

class IPTVTool:
    """IPTVç›´æ’­æºæŠ“å–ä¸æµ‹é€Ÿå·¥å…·"""
    
    def __init__(self, timeout=8, max_workers=5, test_size_kb=64):
        """
        åˆå§‹åŒ–å·¥å…·
        
        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
            test_size_kb: æµ‹é€Ÿæ•°æ®å¤§å°ï¼ˆKBï¼‰
        """
        # é…ç½®å‚æ•°
        self.timeout = timeout
        self.max_workers = max_workers
        self.test_size = test_size_kb * 1024
        
        # è¯·æ±‚ä¼šè¯é…ç½®
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Encoding': 'gzip, deflate'
        })
        
        # æ•°æ®æºé…ç½®
        self.source_urls = [
            "https://raw.githubusercontent.com/zwc456baby/iptv_alive/master/live.txt",
            "https://live.zbds.top/tv/iptv6.txt", 
            "https://live.zbds.top/tv/iptv4.txt",
        ]
        
        # æ­£åˆ™è¡¨è¾¾å¼é¢„ç¼–è¯‘
        self.ipv4_pattern = re.compile(r'^http://(\d{1,3}\.){3}\d{1,3}')
        self.ipv6_pattern = re.compile(r'^http://\[([a-fA-F0-9:]+)\]')
        self.channel_pattern = re.compile(r'^([^,#]+)')
        
        # æ–‡ä»¶è·¯å¾„é…ç½®
        self.template_file = os.path.join(os.path.dirname(__file__), "demo.txt")
        self.output_files = {
            'txt': os.path.join(os.path.dirname(__file__), "iptv.txt"),
            'm3u': os.path.join(os.path.dirname(__file__), "iptv.m3u"),
            'log': os.path.join(os.path.dirname(__file__), "process.log")
        }
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.valid_channels = self.load_template_channels()
        self.setup_logging()

    def setup_logging(self):
        """åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶"""
        with open(self.output_files['log'], 'w', encoding='utf-8') as f:
            f.write(f"IPTV Tool Process Log - {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*50 + "\n")

    def log(self, message: str, console_print=True):
        """è®°å½•æ—¥å¿—"""
        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
        log_entry = f"[{timestamp}] {message}\n"
        
        with open(self.output_files['log'], 'a', encoding='utf-8') as f:
            f.write(log_entry)
        
        if console_print:
            print(message)

    def load_template_channels(self) -> Set[str]:
        """åŠ è½½æ¨¡æ¿æ–‡ä»¶ä¸­çš„æœ‰æ•ˆé¢‘é“åˆ—è¡¨"""
        channels = set()
        if not os.path.exists(self.template_file):
            self.log(f"âš ï¸ æ¨¡æ¿æ–‡ä»¶ {self.template_file} ä¸å­˜åœ¨ï¼Œå°†å¤„ç†æ‰€æœ‰é¢‘é“")
            return channels
        
        try:
            with open(self.template_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        if match := self.channel_pattern.match(line):
                            channels.add(match.group(1).strip())
            self.log(f"åŠ è½½æ¨¡æ¿é¢‘é“ {len(channels)} ä¸ª")
        except Exception as e:
            self.log(f"åŠ è½½æ¨¡æ¿æ–‡ä»¶é”™è¯¯: {str(e)}")
        
        return channels

    # ==================== æ•°æ®è·å–ä¸å¤„ç† ====================
    
    def fetch_streams(self) -> Optional[str]:
        """ä»æ‰€æœ‰æºURLæŠ“å–ç›´æ’­æº"""
        contents = []
        for url in self.source_urls:
            self.log(f"æŠ“å–æº: {url}")
            try:
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                contents.append(response.text)
            except Exception as e:
                self.log(f"æŠ“å–å¤±è´¥ {url}: {str(e)}")
        
        return "\n".join(contents) if contents else None

    def parse_content(self, content: str) -> pd.DataFrame:
        """è§£æç›´æ’­æºå†…å®¹"""
        streams = []
        
        # è‡ªåŠ¨æ£€æµ‹æ ¼å¼å¹¶è§£æ
        if content.startswith("#EXTM3U"):
            current_program = None
            for line in content.splitlines():
                if line.startswith("#EXTINF"):
                    if match := re.search(r'tvg-name="([^"]+)"', line):
                        current_program = match.group(1).strip()
                elif line.startswith("http"):
                    if current_program:
                        streams.append({"program_name": current_program, "stream_url": line.strip()})
        else:
            for line in content.splitlines():
                if match := re.match(r"^([^,]+?)\s*,\s*(http.+)$", line):
                    streams.append({
                        "program_name": match.group(1).strip(),
                        "stream_url": match.group(2).strip()
                    })
        
        if not streams:
            return pd.DataFrame(columns=['program_name', 'stream_url'])
        
        df = pd.DataFrame(streams)
        
        # è¿‡æ»¤å’Œå»é‡
        if self.valid_channels:
            df = df[df['program_name'].isin(self.valid_channels)]
        
        return df.drop_duplicates(subset=['program_name', 'stream_url'])

    def organize_streams(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•´ç†ç›´æ’­æºæ•°æ®"""
        return df.groupby('program_name')['stream_url'].apply(list).reset_index()

    # ==================== æµ‹é€ŸåŠŸèƒ½ ====================
    
    def test_single_url(self, url: str) -> Tuple[Optional[float], Optional[str]]:
        """æµ‹è¯•å•ä¸ªURLçš„é€Ÿåº¦"""
        try:
            start_time = time.time()
            with self.session.get(url, timeout=self.timeout, stream=True) as response:
                response.raise_for_status()
                
                content_length = 0
                for chunk in response.iter_content(chunk_size=8192):
                    content_length += len(chunk)
                    if content_length >= self.test_size:
                        break
                
                speed = content_length / (time.time() - start_time) / 1024
                return (speed, None) if speed > 0 else (0, "é›¶é€Ÿåº¦")
                
        except requests.exceptions.Timeout:
            return (None, "è¶…æ—¶")
        except requests.exceptions.SSLError:
            return (None, "SSLé”™è¯¯")
        except requests.exceptions.ConnectionError:
            return (None, "è¿æ¥å¤±è´¥")
        except requests.exceptions.HTTPError as e:
            return (None, f"HTTPé”™è¯¯ {e.response.status_code}")
        except Exception as e:
            return (None, f"é”™è¯¯: {str(e)}")

    def test_urls_concurrently(self, urls: List[str]) -> List[Tuple[str, Optional[float], Optional[str]]]:
        """å¹¶å‘æµ‹è¯•URLåˆ—è¡¨"""
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_url = {executor.submit(self.test_single_url, url): url for url in urls}
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                speed, error = future.result()
                results.append((url, speed, error))
        return results

    def test_all_channels(self, grouped_streams: pd.DataFrame, max_test=8, keep_best=8) -> Dict[str, List[Tuple[str, float]]]:
        """æµ‹è¯•æ‰€æœ‰é¢‘é“å¹¶ä¿ç•™æœ€ä½³æº"""
        results = {}
        total_channels = len(grouped_streams)
        
        self.log(f"å¼€å§‹æµ‹é€Ÿ {total_channels} ä¸ªé¢‘é“")
        
        for idx, (_, row) in enumerate(grouped_streams.iterrows(), 1):
            channel = row['program_name']
            urls = row['stream_url'][:max_test]
            
            self.log(f"[{idx}/{total_channels}] æµ‹è¯•é¢‘é“: {channel} ({len(urls)}ä¸ªæº)")
            
            test_results = self.test_urls_concurrently(urls)
            valid_streams = []
            
            for url, speed, error in test_results:
                if speed is not None:
                    valid_streams.append((url, speed))
                    status = "âœ“" if speed > 100 else "âš ï¸"
                    self.log(f"    {status} {self._extract_domain(url)}: {speed:.1f} KB/s")
                else:
                    self.log(f"    âœ— {self._extract_domain(url)}: {error}")
            
            valid_streams.sort(key=lambda x: x[1], reverse=True)
            results[channel] = valid_streams[:keep_best]
            
            if results[channel]:
                best_speed = results[channel][0][1]
                self.log(f"    âœ… æœ€ä½³æº: {best_speed:.1f} KB/s (ä¿ç•™{len(results[channel])}ä¸ª)")
            else:
                self.log("    âŒ æ— æœ‰æ•ˆæº")
        
        return results

    # ==================== ç»“æœè¾“å‡º ====================
    
    def generate_output_files(self, speed_results: Dict[str, List[Tuple[str, float]]]):
        """ç”Ÿæˆæ‰€æœ‰è¾“å‡ºæ–‡ä»¶"""
        self.generate_txt_file(speed_results)
        self.generate_m3u_file(speed_results)
        self.generate_report(speed_results)

    def generate_txt_file(self, results: Dict[str, List[Tuple[str, float]]]):
        """ç”ŸæˆTXTæ ¼å¼æ–‡ä»¶"""
        categories = {
            "å¤®è§†é¢‘é“,#genre#": ["CCTV", "å¤®è§†"],
            "å«è§†é¢‘é“,#genre#": ["å«è§†", "æ¹–å—", "æµ™æ±Ÿ", "æ±Ÿè‹", "ä¸œæ–¹", "åŒ—äº¬"],
            "åœ°æ–¹é¢‘é“,#genre#": ["é‡åº†", "å¹¿ä¸œ", "æ·±åœ³", "å—æ–¹"],
            "å…¶ä»–é¢‘é“,#genre#": []
        }
        
        categorized = {cat: [] for cat in categories}
        
        for channel in self.get_ordered_channels(results.keys()):
            streams = results.get(channel, [])
            if not streams:
                continue
                
            matched = False
            for cat, keywords in categories.items():
                if any(keyword in channel for keyword in keywords):
                    categorized[cat].extend(
                        f"{channel},{url} # é€Ÿåº¦: {speed:.1f}KB/s" 
                        for url, speed in streams
                    )
                    matched = True
                    break
            
            if not matched:
                categorized["å…¶ä»–é¢‘é“,#genre#"].extend(
                    f"{channel},{url} # é€Ÿåº¦: {speed:.1f}KB/s" 
                    for url, speed in streams
                )
        
        with open(self.output_files['txt'], 'w', encoding='utf-8') as f:
            for cat, items in categorized.items():
                if items:
                    f.write(f"\n{cat}\n")
                    f.write("\n".join(items) + "\n")
        
        self.log(f"ç”ŸæˆTXTæ–‡ä»¶: {self.output_files['txt']}")

    def generate_m3u_file(self, results: Dict[str, List[Tuple[str, float]]]):
        """ç”ŸæˆM3Uæ ¼å¼æ–‡ä»¶"""
        with open(self.output_files['m3u'], 'w', encoding='utf-8') as f:
            f.write("#EXTM3U\n")
            
            # å†™å…¥åˆ†ç±»ä¿¡æ¯
            f.write('#EXTINF:-1 group-title="å¤®è§†é¢‘é“",å¤®è§†é¢‘é“\n')
            f.write('#EXTINF:-1 group-title="å«è§†é¢‘é“",å«è§†é¢‘é“\n')
            f.write('#EXTINF:-1 group-title="åœ°æ–¹é¢‘é“",åœ°æ–¹é¢‘é“\n')
            f.write('#EXTINF:-1 group-title="å…¶ä»–é¢‘é“",å…¶ä»–é¢‘é“\n')
            
            for channel in self.get_ordered_channels(results.keys()):
                streams = results.get(channel, [])
                for url, speed in streams:
                    quality = self.get_speed_quality(speed)
                    f.write(f'#EXTINF:-1 tvg-name="{channel}",{channel} [é€Ÿåº¦: {speed:.1f}KB/s {quality}]\n{url}\n')
        
        self.log(f"ç”ŸæˆM3Uæ–‡ä»¶: {self.output_files['m3u']}")

    def generate_report(self, results: Dict[str, List[Tuple[str, float]]]):
        """ç”Ÿæˆæµ‹é€ŸæŠ¥å‘Š"""
        speed_stats = []
        valid_channels = []
        
        for channel, streams in results.items():
            if streams:
                best_speed = streams[0][1]
                speed_stats.append(best_speed)
                valid_channels.append((channel, best_speed, len(streams)))
        
        if not speed_stats:
            self.log("âš ï¸ æ— æœ‰æ•ˆæµ‹é€Ÿç»“æœ")
            return
        
        # æŒ‰é€Ÿåº¦æ’åºé¢‘é“
        valid_channels.sort(key=lambda x: x[1], reverse=True)
        
        report = [
            "="*50,
            "æµ‹é€ŸæŠ¥å‘Š",
            "="*50,
            f"æœ‰æ•ˆé¢‘é“æ•°: {len(valid_channels)}",
            f"æ€»æºæ•°é‡: {sum(x[2] for x in valid_channels)}",
            f"å¹³å‡é€Ÿåº¦: {sum(speed_stats)/len(speed_stats):.1f} KB/s",
            f"æœ€å¿«é€Ÿåº¦: {max(speed_stats):.1f} KB/s",
            f"æœ€æ…¢é€Ÿåº¦: {min(speed_stats):.1f} KB/s",
            "\né¢‘é“é€Ÿåº¦æ’å:"
        ]
        
        for i, (channel, speed, count) in enumerate(valid_channels[:20], 1):
            report.append(f"{i:2d}. {channel}: {speed:.1f} KB/s ({count}ä¸ªæº)")
        
        if len(valid_channels) > 20:
            report.append(f"...(å…±{len(valid_channels)}ä¸ªé¢‘é“)")
        
        report_content = "\n".join(report)
        self.log("\n" + report_content)
        
        with open(self.output_files['log'], 'a', encoding='utf-8') as f:
            f.write("\n" + report_content + "\n")

    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def get_ordered_channels(self, channels: List[str]) -> List[str]:
        """æŒ‰ç…§æ¨¡æ¿é¡ºåºæ’åºé¢‘é“åˆ—è¡¨"""
        if not self.valid_channels:
            return sorted(channels)
        
        ordered = []
        with open(self.template_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    if match := self.channel_pattern.match(line):
                        channel = match.group(1).strip()
                        if channel in channels and channel not in ordered:
                            ordered.append(channel)
        
        # æ·»åŠ æœªåœ¨æ¨¡æ¿ä¸­çš„é¢‘é“
        for channel in channels:
            if channel not in ordered:
                ordered.append(channel)
                
        return ordered

    def _extract_domain(self, url: str) -> str:
        """ä»URLæå–åŸŸå"""
        try:
            netloc = urlparse(url).netloc
            return netloc.split(':')[0]  # ç§»é™¤ç«¯å£å·
        except:
            return url[:30] + "..." if len(url) > 30 else url

    def get_speed_quality(self, speed: float) -> str:
        """è·å–é€Ÿåº¦è´¨é‡è¯„çº§"""
        if speed > 1000: return "æä½³"
        if speed > 500: return "ä¼˜ç§€"
        if speed > 200: return "è‰¯å¥½"
        if speed > 100: return "ä¸€èˆ¬"
        if speed > 50: return "è¾ƒå·®"
        return "æå·®"

    # ==================== ä¸»æµç¨‹ ====================
    
    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        self.log("="*50)
        self.log("IPTVç›´æ’­æºå¤„ç†å·¥å…·")
        self.log("="*50)
        
        # æ˜¾ç¤ºæ¨¡æ¿ä¿¡æ¯
        if self.valid_channels:
            self.log(f"æ¨¡æ¿é¢‘é“: {len(self.valid_channels)}ä¸ª")
        else:
            self.log("âš ï¸ æœªä½¿ç”¨æ¨¡æ¿è¿‡æ»¤")
        
        # æŠ“å–å’Œå¤„ç†æ•°æ®
        self.log("\nå¼€å§‹æŠ“å–ç›´æ’­æº...")
        if content := self.fetch_streams():
            self.log("\nè§£æç›´æ’­æºæ•°æ®...")
            df = self.parse_content(content)
            
            # æ˜¾ç¤ºé¢‘é“åŒ¹é…æƒ…å†µ
            matched = set(df['program_name'].unique())
            self.log(f"\né¢‘é“åŒ¹é…ç»“æœ:")
            self.log(f"  å‘ç°é¢‘é“æ€»æ•°: {len(matched)}")
            
            if self.valid_channels:
                unmatched = self.valid_channels - matched
                self.log(f"  åŒ¹é…æ¨¡æ¿é¢‘é“: {len(matched & self.valid_channels)}/{len(self.valid_channels)}")
                if unmatched:
                    self.log(f"  æœªåŒ¹é…æ¨¡æ¿é¢‘é“: {len(unmatched)}ä¸ª")
            
            # æ•´ç†å’Œç»„ç»‡æ•°æ®
            grouped = self.organize_streams(df)
            self.log(f"\næœ‰æ•ˆç›´æ’­æº: {len(grouped)}ä¸ªé¢‘é“")
            
            # æµ‹é€Ÿå’Œä¼˜åŒ–
            self.log("\nå¼€å§‹æµ‹é€Ÿ(æ¯ä¸ªé¢‘é“æµ‹è¯•æœ€å¤š8ä¸ªæºï¼Œä¿ç•™æœ€ä½³8ä¸ª)...")
            speed_results = self.test_all_channels(grouped)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
            self.log("\nç”Ÿæˆè¾“å‡ºæ–‡ä»¶ä¸­...")
            self.generate_output_files(speed_results)
            
            self.log("\nğŸ‰ å¤„ç†å®Œæˆï¼")
        else:
            self.log("âš ï¸ æœªèƒ½è·å–æœ‰æ•ˆæ•°æ®")

if __name__ == "__main__":
    # é…ç½®å‚æ•°
    config = {
        'timeout': 6,      # è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
        'max_workers': 3,  # æœ€å¤§å¹¶å‘æ•°
        'test_size_kb': 32 # æµ‹é€Ÿæ•°æ®å¤§å°(KB)
    }
    
    tool = IPTVTool(**config)
    tool.run()
