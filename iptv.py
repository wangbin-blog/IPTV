#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IPTVç›´æ’­æºæŠ“å–ä¸æµ‹é€Ÿå·¥å…·
åŠŸèƒ½ï¼šä»å¤šä¸ªæºè·å–ç›´æ’­æºï¼Œè¿›è¡Œæµ‹é€Ÿç­›é€‰ï¼Œç”Ÿæˆå¤šç§æ ¼å¼çš„è¾“å‡ºæ–‡ä»¶
ä½œè€…ï¼šAIåŠ©æ‰‹
ç‰ˆæœ¬ï¼š2.0
"""

import requests
import pandas as pd
import re
import os
import time
import concurrent.futures
import logging
from typing import List, Dict, Optional, Tuple, Set, Any, Union
from urllib.parse import urlparse
from dataclasses import dataclass
from pathlib import Path
import hashlib
import json
import argparse
from collections import defaultdict
import threading

@dataclass
class TestResult:
    """æµ‹é€Ÿç»“æœæ•°æ®ç±»"""
    url: str                    # æµ‹è¯•çš„URLåœ°å€
    speed: Optional[float]      # æµ‹é€Ÿç»“æœ(KB/s)ï¼ŒNoneè¡¨ç¤ºæµ‹è¯•å¤±è´¥
    error: Optional[str]        # é”™è¯¯ä¿¡æ¯ï¼ŒæˆåŠŸæ—¶ä¸ºNone
    response_time: float        # å“åº”æ—¶é—´(ç§’)
    status_code: Optional[int]  # HTTPçŠ¶æ€ç 
    content_type: Optional[str] # å†…å®¹ç±»å‹
    success: bool              # æµ‹è¯•æ˜¯å¦æˆåŠŸ

class IPTVConfig:
    """IPTVå·¥å…·é…ç½®ç±»"""
    
    def __init__(self):
        # ç½‘ç»œé…ç½®
        self.timeout = 10                    # è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
        self.max_workers = 6               # æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
        self.test_size_kb = 1024            # æµ‹é€Ÿæ•°æ®å¤§å°(KB)ï¼Œå¢åŠ æ•°æ®é‡æé«˜å‡†ç¡®æ€§
        self.retry_times = 2               # é‡è¯•æ¬¡æ•°
        self.request_delay = 0.3           # è¯·æ±‚é—´å»¶è¿Ÿ(ç§’)ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        
        # æµ‹é€Ÿé…ç½®
        self.min_speed_threshold = 500      # æœ€å°é€Ÿåº¦é˜ˆå€¼(KB/s)ï¼Œä½äºæ­¤å€¼çš„æºå°†è¢«ä¸¢å¼ƒ
        self.max_test_per_channel = 30     # æ¯ä¸ªé¢‘é“æœ€å¤§æµ‹è¯•æºæ•°
        self.keep_best_sources = 8         # æ¯ä¸ªé¢‘é“ä¿ç•™æœ€ä½³æºæ•°é‡
        self.speed_test_duration = 10       # æµ‹é€Ÿæœ€å¤§æŒç»­æ—¶é—´(ç§’)
        
        # æ•°æ®æºé…ç½® - å¤šä¸ªç›´æ’­æºURL
        self.source_urls = [
            "https://ghfast.top/raw.githubusercontent.com/Supprise0901/TVBox_live/main/live.txt",
            "https://gh-proxy.com/https://raw.githubusercontent.com/wwb521/live/main/tv.m3u",
            "https://raw.githubusercontent.com/Guovin/iptv-api/gd/output/ipv4/result.m3u",  
            "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/cn.m3u",
            "https://raw.githubusercontent.com/suxuang/myIPTV/main/ipv4.m3u",
            "https://raw.githubusercontent.com/vbskycn/iptv/master/tv/iptv4.txt",
            "https://gh-proxy.com/https://raw.githubusercontent.com/develop202/migu_video/refs/heads/main/interface.txt",
            "http://47.120.41.246:8899/zb.txt",
        ]
        
        # æ–‡ä»¶è·¯å¾„é…ç½®
        self.base_dir = Path(__file__).parent  # åŸºç¡€ç›®å½•
        self.template_file = self.base_dir / "demo.txt"  # æ¨¡æ¿æ–‡ä»¶è·¯å¾„
        self.cache_file = self.base_dir / "cache.json"   # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        
        # è¾“å‡ºæ–‡ä»¶é…ç½®
        self.output_files = {
            'txt': self.base_dir / "iptv.txt",           # TXTæ ¼å¼è¾“å‡º
            'm3u': self.base_dir / "iptv.m3u",           # M3Uæ ¼å¼è¾“å‡º
            'log': self.base_dir / "process.log",        # å¤„ç†æ—¥å¿—
            'report': self.base_dir / "speed_report.txt", # æµ‹é€ŸæŠ¥å‘Š
            'json': self.base_dir / "iptv_data.json"     # JSONæ ¼å¼æ•°æ®
        }
        
        # é¢‘é“åˆ†ç±»é…ç½® - ç”¨äºè‡ªåŠ¨åˆ†ç±»é¢‘é“
        self.channel_categories = {
            "å¤®è§†é¢‘é“,#genre#": ["CCTV", "å¤®è§†", "Cctv", "cctv"],  # å¤®è§†ç›¸å…³é¢‘é“
            "é«˜æ¸…é¢‘é“,#genre#": ["é«˜æ¸…", "HD", "hd", "4K", "4k"],  # é«˜æ¸…é¢‘é“
            "å«è§†é¢‘é“,#genre#": ["å«è§†", "æ¹–å—", "æµ™æ±Ÿ", "æ±Ÿè‹", "ä¸œæ–¹", "åŒ—äº¬", "å¤©æ´¥", "æ²³åŒ—", "å±±ä¸œ", "å®‰å¾½"],  # å«è§†é¢‘é“
            "åœ°æ–¹é¢‘é“,#genre#": ["é‡åº†", "å¹¿ä¸œ", "æ·±åœ³", "å—æ–¹", "å¹¿å·", "å››å·", "ç¦å»º", "æ¹–åŒ—", "è¾½å®"],  # åœ°æ–¹é¢‘é“
            "æ¸¯æ¾³é¢‘é“,#genre#": ["å‡¤å‡°", "ç¿¡ç¿ ", "æ˜ç ", "æ¾³é—¨", "é¦™æ¸¯", "æ¸¯æ¾³"],  # æ¸¯æ¾³é¢‘é“
            "å½±è§†é¢‘é“,#genre#": ["ç”µå½±", "å½±é™¢", "å‰§åœº", "å½±è§†"],  # å½±è§†ç›¸å…³é¢‘é“
            "ä½“è‚²é¢‘é“,#genre#": ["ä½“è‚²", "è¶³çƒ", "ç¯®çƒ", "å¥¥è¿", "NBA", "CBA"],  # ä½“è‚²é¢‘é“
            "å…¶ä»–é¢‘é“,#genre#": []  # æœªåˆ†ç±»é¢‘é“
        }
        
        # HTTPè¯·æ±‚å¤´é…ç½®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }

class IPTVTool:
    """IPTVç›´æ’­æºæŠ“å–ä¸æµ‹é€Ÿå·¥å…·ä¸»ç±»"""
    
    def __init__(self, config: Optional[IPTVConfig] = None):
        """
        åˆå§‹åŒ–IPTVå·¥å…·
        
        Args:
            config: é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or IPTVConfig()  # ä½¿ç”¨ä¼ å…¥é…ç½®æˆ–é»˜è®¤é…ç½®
        
        # è¯·æ±‚ä¼šè¯é…ç½® - å¤ç”¨è¿æ¥æé«˜æ•ˆç‡
        self.session = requests.Session()
        self.session.headers.update(self.config.headers)
        
        # æ­£åˆ™è¡¨è¾¾å¼é¢„ç¼–è¯‘ - æé«˜è§£ææ•ˆç‡
        self.ipv4_pattern = re.compile(r'^http://(\d{1,3}\.){3}\d{1,3}')  # IPv4åœ°å€åŒ¹é…
        self.ipv6_pattern = re.compile(r'^http://\[([a-fA-F0-9:]+)\]')     # IPv6åœ°å€åŒ¹é…
        self.channel_pattern = re.compile(r'^([^,#]+)')                    # é¢‘é“åç§°åŒ¹é…
        self.extinf_pattern = re.compile(r'#EXTINF:.*?,(.+)', re.IGNORECASE)  # M3Uæ ¼å¼é¢‘é“ä¿¡æ¯
        self.tvg_name_pattern = re.compile(r'tvg-name="([^"]*)"', re.IGNORECASE)  # M3Ué¢‘é“å
        self.tvg_logo_pattern = re.compile(r'tvg-logo="([^"]*)"', re.IGNORECASE)  # M3Uå°æ ‡
        self.group_title_pattern = re.compile(r'group-title="([^"]*)"', re.IGNORECASE)  # M3Uåˆ†ç»„
        
        # çŠ¶æ€å˜é‡
        self.valid_channels = self.load_template_channels()  # æœ‰æ•ˆé¢‘é“åˆ—è¡¨
        self.url_cache = {}              # URLæµ‹é€Ÿç¼“å­˜ï¼Œé¿å…é‡å¤æµ‹é€Ÿ
        self.processed_count = 0         # å·²å¤„ç†URLè®¡æ•°
        self.lock = threading.Lock()     # çº¿ç¨‹é”ï¼Œç”¨äºå¹¶å‘å®‰å…¨
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        self.setup_logging()    # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
        self.setup_directories()  # åˆ›å»ºå¿…è¦ç›®å½•

    def setup_logging(self):
        """åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼Œåˆ›å»ºæ—¥å¿—æ–‡ä»¶å¹¶è®¾ç½®æ ¼å¼"""
        # åˆ›å»ºæ‰€æœ‰è¾“å‡ºæ–‡ä»¶çš„ç›®å½•
        for file_path in self.config.output_files.values():
            file_path.parent.mkdir(exist_ok=True)
            
        # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶ï¼Œå†™å…¥å¤´éƒ¨ä¿¡æ¯
        with open(self.config.output_files['log'], 'w', encoding='utf-8') as f:
            f.write(f"IPTV Tool Process Log - {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*60 + "\n")

    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„æ–‡ä»¶ç›®å½•"""
        self.config.base_dir.mkdir(exist_ok=True)

    def log(self, message: str, level="INFO", console_print=True):
        """
        è®°å½•æ—¥å¿—åˆ°æ–‡ä»¶å’Œæ§åˆ¶å°
        
        Args:
            message: æ—¥å¿—æ¶ˆæ¯
            level: æ—¥å¿—çº§åˆ« (INFO, SUCCESS, WARNING, ERROR, DEBUG)
            console_print: æ˜¯å¦åœ¨æ§åˆ¶å°æ˜¾ç¤º
        """
        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')  # æ—¶é—´æˆ³
        log_entry = f"[{timestamp}] [{level}] {message}\n"
        
        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        with open(self.config.output_files['log'], 'a', encoding='utf-8') as f:
            f.write(log_entry)
        
        # æ§åˆ¶å°è¾“å‡ºï¼ˆå¸¦é¢œè‰²ï¼‰
        if console_print:
            # å®šä¹‰ä¸åŒæ—¥å¿—çº§åˆ«çš„é¢œè‰²
            level_color = {
                "INFO": "\033[94m",    # è“è‰²
                "SUCCESS": "\033[92m", # ç»¿è‰²
                "WARNING": "\033[93m", # é»„è‰²
                "ERROR": "\033[91m",   # çº¢è‰²
                "DEBUG": "\033[90m"    # ç°è‰²
            }
            color = level_color.get(level, "\033[0m")  # è·å–é¢œè‰²ï¼Œé»˜è®¤æ— è‰²
            reset = "\033[0m"  # é‡ç½®é¢œè‰²
            print(f"{color}[{level}] {message}{reset}")

    def load_template_channels(self) -> Set[str]:
        """
        åŠ è½½æ¨¡æ¿æ–‡ä»¶ä¸­çš„æœ‰æ•ˆé¢‘é“åˆ—è¡¨
        
        Returns:
            Set[str]: é¢‘é“åç§°é›†åˆ
        """
        channels = set()  # ä½¿ç”¨é›†åˆé¿å…é‡å¤
        if not self.config.template_file.exists():
            self.log(f"æ¨¡æ¿æ–‡ä»¶ {self.config.template_file} ä¸å­˜åœ¨ï¼Œå°†å¤„ç†æ‰€æœ‰é¢‘é“", "WARNING")
            return channels
        
        try:
            # è¯»å–æ¨¡æ¿æ–‡ä»¶
            with open(self.config.template_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()  # å»é™¤é¦–å°¾ç©ºç™½
                    if line and not line.startswith('#'):  # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
                        if match := self.channel_pattern.match(line):
                            channel_name = match.group(1).strip()  # æå–é¢‘é“åç§°
                            channels.add(channel_name)  # æ·»åŠ åˆ°é›†åˆ
            self.log(f"ä»æ¨¡æ¿åŠ è½½é¢‘é“ {len(channels)} ä¸ª", "SUCCESS")
        except Exception as e:
            self.log(f"åŠ è½½æ¨¡æ¿æ–‡ä»¶é”™è¯¯: {str(e)}", "ERROR")
        
        return channels

    # ==================== æ•°æ®è·å–ä¸å¤„ç† ====================
    
    def fetch_single_source(self, url: str) -> Tuple[str, Optional[str]]:
        """
        æŠ“å–å•ä¸ªæºçš„æ•°æ®
        
        Args:
            url: æ•°æ®æºURL
            
        Returns:
            Tuple[str, Optional[str]]: (URL, å†…å®¹) æˆ– (URL, None) å¦‚æœå¤±è´¥
        """
        self.log(f"æŠ“å–æº: {self._extract_domain(url)}")
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(self.config.retry_times + 1):
            try:
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                if attempt > 0:
                    time.sleep(1)
                    
                # å‘é€HTTPè¯·æ±‚
                response = self.session.get(url, timeout=self.config.timeout)
                response.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç 
                
                # éªŒè¯å†…å®¹æœ‰æ•ˆæ€§
                content = response.text
                if self.validate_content(content):
                    self.log(f"æˆåŠŸæŠ“å–: {self._extract_domain(url)} (å¤§å°: {len(content)} å­—ç¬¦)", "SUCCESS")
                    return url, content
                else:
                    raise ValueError("å†…å®¹æ ¼å¼æ— æ•ˆ")
                    
            except Exception as e:
                if attempt < self.config.retry_times:
                    self.log(f"ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥ {self._extract_domain(url)}: {str(e)}ï¼Œé‡è¯•...", "WARNING")
                else:
                    self.log(f"æŠ“å–å¤±è´¥ {self._extract_domain(url)}: {str(e)}", "ERROR")
        return url, None

    def validate_content(self, content: str) -> bool:
        """
        éªŒè¯å†…å®¹æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ç›´æ’­æºæ ¼å¼
        
        Args:
            content: è¦éªŒè¯çš„å†…å®¹
            
        Returns:
            bool: æ˜¯å¦ä¸ºæœ‰æ•ˆç›´æ’­æº
        """
        if not content or len(content.strip()) < 10:
            return False  # å†…å®¹ä¸ºç©ºæˆ–å¤ªçŸ­
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›´æ’­æºç‰¹å¾æ¨¡å¼
        patterns = [
            r'http://[^\s]+',  # HTTP URL
            r'#EXTINF',        # M3Uæ ¼å¼æ ‡è®°
            r',http',          # TXTæ ¼å¼åˆ†éš”ç¬¦
            r'\.m3u8?',        # M3U8æ–‡ä»¶
            r'\.ts'            # TSæµ
        ]
        # ç»Ÿè®¡åŒ¹é…çš„æ¨¡å¼æ•°é‡
        valid_patterns = sum(1 for pattern in patterns if re.search(pattern, content, re.IGNORECASE))
        return valid_patterns >= 2  # è‡³å°‘åŒ¹é…2ä¸ªæ¨¡å¼è®¤ä¸ºæ˜¯æœ‰æ•ˆå†…å®¹

    def fetch_streams(self) -> Optional[str]:
        """
        ä»æ‰€æœ‰æºURLå¹¶å‘æŠ“å–ç›´æ’­æº
        
        Returns:
            Optional[str]: åˆå¹¶åçš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        contents = []  # å­˜å‚¨æˆåŠŸè·å–çš„å†…å®¹
        successful_sources = 0  # æˆåŠŸæºè®¡æ•°
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æŠ“å–
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=min(3, len(self.config.source_urls))  # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°
        ) as executor:
            # æäº¤æ‰€æœ‰æŠ“å–ä»»åŠ¡
            future_to_url = {
                executor.submit(self.fetch_single_source, url): url 
                for url in self.config.source_urls
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in concurrent.futures.as_completed(future_to_url):
                url, content = future.result()
                if content:
                    contents.append(content)
                    successful_sources += 1
        
        # è®°å½•æŠ“å–ç»“æœ
        self.log(f"æˆåŠŸæŠ“å– {successful_sources}/{len(self.config.source_urls)} ä¸ªæ•°æ®æº", 
                "SUCCESS" if successful_sources > 0 else "ERROR")
        
        return "\n".join(contents) if contents else None  # åˆå¹¶æ‰€æœ‰å†…å®¹

    def parse_content(self, content: str) -> pd.DataFrame:
        """
        è§£æç›´æ’­æºå†…å®¹ä¸ºDataFrame
        
        Args:
            content: ç›´æ’­æºå†…å®¹
            
        Returns:
            pd.DataFrame: è§£æåçš„ç›´æ’­æºæ•°æ®
        """
        streams = []  # å­˜å‚¨è§£æåçš„æµæ•°æ®
        
        # æ£€æµ‹æ ¼å¼å¹¶é€‰æ‹©ç›¸åº”çš„è§£ææ–¹æ³•
        if content.startswith("#EXTM3U"):
            streams.extend(self._parse_m3u_content(content))  # M3Uæ ¼å¼è§£æ
        else:
            streams.extend(self._parse_txt_content(content))  # TXTæ ¼å¼è§£æ
        
        # æ£€æŸ¥æ˜¯å¦è§£æåˆ°æ•°æ®
        if not streams:
            self.log("æœªè§£æåˆ°ä»»ä½•ç›´æ’­æº", "WARNING")
            return pd.DataFrame(columns=['program_name', 'stream_url', 'tvg_logo', 'group_title'])
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(streams)
        
        # è¿‡æ»¤å’Œå»é‡å¤„ç†
        initial_count = len(df)
        if self.valid_channels:
            # æ ¹æ®æ¨¡æ¿è¿‡æ»¤é¢‘é“
            df = df[df['program_name'].isin(self.valid_channels)]
            filtered_count = initial_count - len(df)
            if filtered_count > 0:
                self.log(f"æ ¹æ®æ¨¡æ¿è¿‡æ»¤æ‰ {filtered_count} ä¸ªé¢‘é“", "INFO")
        
        # å»é‡å¤„ç†
        df = self.deduplicate_streams(df)
        self.log(f"è§£æåˆ° {len(df)} ä¸ªæœ‰æ•ˆç›´æ’­æº", "SUCCESS")
        
        return df

    def _parse_m3u_content(self, content: str) -> List[Dict[str, str]]:
        """
        è§£æM3Uæ ¼å¼å†…å®¹
        
        Args:
            content: M3Uæ ¼å¼å†…å®¹
            
        Returns:
            List[Dict[str, str]]: è§£æåçš„æµæ•°æ®åˆ—è¡¨
        """
        streams = []  # å­˜å‚¨è§£æç»“æœ
        lines = content.splitlines()  # æŒ‰è¡Œåˆ†å‰²
        current_program = None  # å½“å‰èŠ‚ç›®åç§°
        current_logo = None     # å½“å‰å°æ ‡URL
        current_group = None    # å½“å‰åˆ†ç»„
        
        # éå†æ‰€æœ‰è¡Œ
        for i, line in enumerate(lines):
            line = line.strip()  # å»é™¤ç©ºç™½
            if line.startswith("#EXTINF"):
                # è§£æEXTINFè¡Œï¼Œæå–èŠ‚ç›®ä¿¡æ¯
                program_name = self.extinf_pattern.search(line)
                if program_name:
                    current_program = program_name.group(1).strip()
                
                # ä¼˜å…ˆä½¿ç”¨tvg-nameä½œä¸ºèŠ‚ç›®åç§°
                tvg_name = self.tvg_name_pattern.search(line)
                if tvg_name and tvg_name.group(1).strip():
                    current_program = tvg_name.group(1).strip()
                
                # æå–å°æ ‡å’Œåˆ†ç»„ä¿¡æ¯
                logo_match = self.tvg_logo_pattern.search(line)
                current_logo = logo_match.group(1) if logo_match else ""
                
                group_match = self.group_title_pattern.search(line)
                current_group = group_match.group(1) if group_match else ""
                
            elif line.startswith(("http://", "https://")) and current_program:
                # é‡åˆ°URLè¡Œï¼Œä¸å‰é¢çš„EXTINFä¿¡æ¯ç»„åˆ
                streams.append({
                    "program_name": current_program,
                    "stream_url": line,
                    "tvg_logo": current_logo or "",
                    "group_title": current_group or ""
                })
                # é‡ç½®å½“å‰ä¿¡æ¯
                current_program = None
                current_logo = None
                current_group = None
        
        return streams

    def _parse_txt_content(self, content: str) -> List[Dict[str, str]]:
        """
        è§£æTXTæ ¼å¼å†…å®¹
        
        Args:
            content: TXTæ ¼å¼å†…å®¹
            
        Returns:
            List[Dict[str, str]]: è§£æåçš„æµæ•°æ®åˆ—è¡¨
        """
        streams = []
        
        # é€è¡Œè§£æ
        for line_num, line in enumerate(content.splitlines(), 1):
            line = line.strip()
            if not line or line.startswith('#'):  # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                continue
                
            # åŒ¹é… "é¢‘é“åç§°,http://url" æ ¼å¼
            if match := re.match(r"^([^,]+?)\s*,\s*(http.+)$", line):
                program_name = match.group(1).strip()
                stream_url = match.group(2).strip()
                
                # æ¸…ç†URLå‚æ•°ä¸­çš„é¢å¤–ä¿¡æ¯ï¼ˆå¦‚æ³¨é‡Šï¼‰
                stream_url = re.sub(r'\s+#.*$', '', stream_url)
                
                streams.append({
                    "program_name": program_name,
                    "stream_url": stream_url,
                    "tvg_logo": "",
                    "group_title": ""
                })
        
        return streams

    def deduplicate_streams(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å»é‡ç›´æ’­æºï¼Œä¼˜å…ˆä¿ç•™M3Uæ ¼å¼çš„æº
        
        Args:
            df: åŸå§‹æ•°æ®DataFrame
            
        Returns:
            pd.DataFrame: å»é‡åçš„DataFrame
        """
        # è®¡ç®—URLçš„å“ˆå¸Œå€¼ç”¨äºå»é‡
        def get_url_key(url):
            # ç§»é™¤å‚æ•°è¿›è¡ŒåŸºç¡€å»é‡ï¼Œåªæ¯”è¾ƒåŸºç¡€URL
            base_url = url.split('?')[0].split('#')[0]
            return hashlib.md5(base_url.encode()).hexdigest()
        
        df['url_key'] = df['stream_url'].apply(get_url_key)
        
        # ä¼˜å…ˆä¿ç•™æœ‰logoå’Œgroupä¿¡æ¯çš„æºï¼ˆé€šå¸¸æ˜¯M3Uæ ¼å¼ï¼Œè´¨é‡æ›´å¥½ï¼‰
        df['priority'] = df.apply(
            lambda x: 2 if x['tvg_logo'] or x['group_title'] else 1, 
            axis=1
        )
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºå¹¶å»é‡ï¼Œä¿ç•™ä¼˜å…ˆçº§é«˜çš„
        df = df.sort_values('priority', ascending=False)
        df = df.drop_duplicates(subset=['program_name', 'url_key'], keep='first')
        
        # æ¸…ç†ä¸´æ—¶åˆ—
        return df.drop(['url_key', 'priority'], axis=1)

    def organize_streams(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ•´ç†ç›´æ’­æºæ•°æ®ï¼ŒæŒ‰é¢‘é“åˆ†ç»„
        
        Args:
            df: è§£æåçš„ç›´æ’­æºæ•°æ®
            
        Returns:
            pd.DataFrame: åˆ†ç»„æ•´ç†åçš„æ•°æ®
        """
        # æŒ‰é¢‘é“åç§°åˆ†ç»„ï¼Œèšåˆæ‰€æœ‰URL
        grouped = df.groupby('program_name')['stream_url'].apply(list).reset_index()
        
        # ç»Ÿè®¡æ¯ä¸ªé¢‘é“çš„æºæ•°é‡
        source_counts = grouped['stream_url'].apply(len)
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        self.log(f"é¢‘é“æºæ•°é‡ç»Ÿè®¡: å¹³å‡{source_counts.mean():.1f}, æœ€å¤š{source_counts.max()}, æœ€å°‘{source_counts.min()}", "INFO")
        
        # æ˜¾ç¤ºæºæ•°é‡åˆ†å¸ƒè¯¦æƒ…
        count_distribution = source_counts.value_counts().sort_index()
        for count, freq in count_distribution.items():
            self.log(f"  {count}ä¸ªæº: {freq}ä¸ªé¢‘é“", "DEBUG")
        
        return grouped

    # ==================== æµ‹é€ŸåŠŸèƒ½ ====================
    
    def test_single_url(self, url: str) -> TestResult:
        """
        æµ‹è¯•å•ä¸ªURLçš„é€Ÿåº¦å’Œè´¨é‡
        
        Args:
            url: è¦æµ‹è¯•çš„URL
            
        Returns:
            TestResult: æµ‹è¯•ç»“æœ
        """
        start_time = time.time()  # å¼€å§‹æ—¶é—´
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(self.config.retry_times + 1):
            try:
                # æ·»åŠ è¯·æ±‚å»¶è¿Ÿï¼Œé¿å…è¿‡å¿«è¯·æ±‚
                if attempt > 0:
                    time.sleep(0.5)
                
                # æ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é‡å¤æµ‹é€Ÿ
                cache_key = hashlib.md5(url.encode()).hexdigest()
                if cache_key in self.url_cache:
                    cached_result = self.url_cache[cache_key]
                    # 5åˆ†é’Ÿç¼“å­˜æœ‰æ•ˆæœŸ
                    if time.time() - cached_result['timestamp'] < 300:
                        self.log(f"ä½¿ç”¨ç¼“å­˜ç»“æœ: {self._extract_domain(url)}", "DEBUG")
                        return cached_result['result']
                
                # å¼€å§‹æµ‹è¯•
                test_start = time.time()
                with self.session.get(
                    url, 
                    timeout=self.config.timeout, 
                    stream=True  # æµå¼ä¼ è¾“ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½å¤§æ–‡ä»¶
                ) as response:
                    response_time = time.time() - test_start  # å“åº”æ—¶é—´
                    
                    # æ£€æŸ¥HTTPçŠ¶æ€å’Œå†…å®¹ç±»å‹
                    status_code = response.status_code
                    content_type = response.headers.get('content-type', '')
                    
                    if status_code != 200:
                        return TestResult(
                            url, None, f"HTTP {status_code}", 
                            response_time, status_code, content_type, False
                        )
                    
                    # æµ‹é€Ÿï¼šä¸‹è½½æŒ‡å®šå¤§å°çš„æ•°æ®è®¡ç®—é€Ÿåº¦
                    content_length = 0
                    chunk_count = 0
                    start_download = time.time()
                    
                    # åˆ†å—è¯»å–æ•°æ®
                    for chunk in response.iter_content(chunk_size=8192):
                        content_length += len(chunk)
                        chunk_count += 1
                        
                        # è¾¾åˆ°æµ‹è¯•æ•°æ®é‡æˆ–è¶…æ—¶åˆ™åœæ­¢
                        if (content_length >= self.config.test_size or 
                            time.time() - start_download > self.config.speed_test_duration):
                            break
                    
                    download_time = time.time() - start_download
                    
                    # è®¡ç®—é€Ÿåº¦ï¼ˆè‡³å°‘1KBæ•°æ®æ‰è®¤ä¸ºæœ‰æ•ˆï¼‰
                    if content_length > 1024:
                        speed = content_length / download_time / 1024  # è½¬æ¢ä¸ºKB/s
                        
                        result = TestResult(
                            url, speed, None, response_time, 
                            status_code, content_type, True
                        )
                        
                        # ç¼“å­˜æˆåŠŸç»“æœ
                        self.url_cache[cache_key] = {
                            'result': result,
                            'timestamp': time.time()
                        }
                        
                        return result
                    else:
                        return TestResult(
                            url, 0, "æ•°æ®é‡ä¸è¶³", response_time,
                            status_code, content_type, False
                        )
                        
            except requests.exceptions.Timeout:
                error = "è¯·æ±‚è¶…æ—¶"
            except requests.exceptions.SSLError:
                error = "SSLè¯ä¹¦é”™è¯¯"
            except requests.exceptions.ConnectionError:
                error = "è¿æ¥å¤±è´¥"
            except requests.exceptions.HTTPError as e:
                error = f"HTTPé”™è¯¯ {e.response.status_code}"
            except Exception as e:
                error = f"æœªçŸ¥é”™è¯¯: {str(e)}"
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        return TestResult(
            url, None, error, time.time() - start_time,
            None, None, False
        )

    def test_urls_concurrently(self, urls: List[str]) -> List[TestResult]:
        """
        å¹¶å‘æµ‹è¯•URLåˆ—è¡¨
        
        Args:
            urls: è¦æµ‹è¯•çš„URLåˆ—è¡¨
            
        Returns:
            List[TestResult]: æµ‹è¯•ç»“æœåˆ—è¡¨
        """
        results = []
        total = len(urls)
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æµ‹è¯•
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            # æäº¤æ‰€æœ‰æµ‹è¯•ä»»åŠ¡
            future_to_url = {executor.submit(self.test_single_url, url): url for url in urls}
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡å¹¶æ˜¾ç¤ºè¿›åº¦
            for i, future in enumerate(concurrent.futures.as_completed(future_to_url), 1):
                result = future.result()
                results.append(result)
                
                # æ›´æ–°è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                with self.lock:
                    self.processed_count += 1
                    # æ¯5ä¸ªæˆ–æœ€åä¸€ä¸ªæ˜¾ç¤ºè¿›åº¦
                    if i % 5 == 0 or i == total:
                        self.log(f"æµ‹é€Ÿè¿›åº¦: {i}/{total} ({i/total*100:.1f}%)", "INFO")
        
        return results

    def test_all_channels(self, grouped_streams: pd.DataFrame) -> Dict[str, List[Tuple[str, float]]]:
        """
        æµ‹è¯•æ‰€æœ‰é¢‘é“å¹¶ä¿ç•™æœ€ä½³æº
        
        Args:
            grouped_streams: åˆ†ç»„åçš„ç›´æ’­æºæ•°æ®
            
        Returns:
            Dict[str, List[Tuple[str, float]]]: é¢‘é“åˆ°æœ€ä½³æºåˆ—è¡¨çš„æ˜ å°„
        """
        results = {}  # å­˜å‚¨ç»“æœ
        total_channels = len(grouped_streams)  # æ€»é¢‘é“æ•°
        successful_channels = 0  # æˆåŠŸé¢‘é“è®¡æ•°
        
        self.log(f"å¼€å§‹æµ‹é€Ÿ {total_channels} ä¸ªé¢‘é“", "INFO")
        self.log(f"æ¯ä¸ªé¢‘é“æµ‹è¯•æœ€å¤š{self.config.max_test_per_channel}ä¸ªæºï¼Œä¿ç•™æœ€ä½³{self.config.keep_best_sources}ä¸ª", "INFO")
        
        self.processed_count = 0  # é‡ç½®è®¡æ•°å™¨
        
        # éå†æ‰€æœ‰é¢‘é“
        for idx, (_, row) in enumerate(grouped_streams.iterrows(), 1):
            channel = row['program_name']
            urls = row['stream_url'][:self.config.max_test_per_channel]  # é™åˆ¶æµ‹è¯•æ•°é‡
            
            self.log(f"[{idx}/{total_channels}] æµ‹è¯•é¢‘é“: {channel} ({len(urls)}ä¸ªæº)")
            
            # å¹¶å‘æµ‹è¯•è¯¥é¢‘é“çš„æ‰€æœ‰URL
            test_results = self.test_urls_concurrently(urls)
            valid_streams = []  # æœ‰æ•ˆæºåˆ—è¡¨
            
            # å¤„ç†æµ‹è¯•ç»“æœ
            for result in test_results:
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸä¸”è¾¾åˆ°é€Ÿåº¦é˜ˆå€¼
                if result.success and result.speed and result.speed >= self.config.min_speed_threshold:
                    valid_streams.append((result.url, result.speed))
                    status = "âœ“" if result.speed > 200 else "âš ï¸"  # é€Ÿåº¦çŠ¶æ€å›¾æ ‡
                    speed_quality = self.get_speed_quality(result.speed)  # é€Ÿåº¦è´¨é‡è¯„çº§
                    response_info = f"{result.response_time:.2f}s"  # å“åº”æ—¶é—´
                    self.log(f"    {status} {self._extract_domain(result.url)}: {result.speed:.1f} KB/s ({speed_quality}) [{response_info}]")
                else:
                    error_info = result.error or "é€Ÿåº¦è¿‡ä½"  # é”™è¯¯ä¿¡æ¯
                    self.log(f"    âœ— {self._extract_domain(result.url)}: {error_info}")
            
            # æŒ‰é€Ÿåº¦æ’åºå¹¶ä¿ç•™æœ€ä½³æº
            valid_streams.sort(key=lambda x: x[1], reverse=True)  # é™åºæ’åº
            results[channel] = valid_streams[:self.config.keep_best_sources]
            
            # è®°å½•é¢‘é“æµ‹è¯•ç»“æœ
            if results[channel]:
                successful_channels += 1
                best_speed = results[channel][0][1]  # æœ€ä½³é€Ÿåº¦
                self.log(f"    âœ… æœ€ä½³æº: {best_speed:.1f} KB/s (ä¿ç•™{len(results[channel])}ä¸ª)", "SUCCESS")
            else:
                self.log("    âŒ æ— æœ‰æ•ˆæº", "WARNING")
        
        # æœ€ç»ˆç»Ÿè®¡
        self.log(f"æµ‹é€Ÿå®Œæˆ: {successful_channels}/{total_channels} ä¸ªé¢‘é“æœ‰æœ‰æ•ˆæº", 
                "SUCCESS" if successful_channels > 0 else "ERROR")
        
        return results

    # ==================== ç»“æœè¾“å‡º ====================
    
    def generate_output_files(self, speed_results: Dict[str, List[Tuple[str, float]]]):
        """ç”Ÿæˆæ‰€æœ‰è¾“å‡ºæ–‡ä»¶"""
        self.generate_txt_file(speed_results)    # ç”ŸæˆTXTæ–‡ä»¶
        self.generate_m3u_file(speed_results)    # ç”ŸæˆM3Uæ–‡ä»¶
        self.generate_json_file(speed_results)   # ç”ŸæˆJSONæ–‡ä»¶
        self.generate_report(speed_results)      # ç”Ÿæˆæµ‹é€ŸæŠ¥å‘Š

    def generate_txt_file(self, results: Dict[str, List[Tuple[str, float]]]):
        """
        ç”ŸæˆTXTæ ¼å¼æ–‡ä»¶
        
        Args:
            results: æµ‹é€Ÿç»“æœå­—å…¸
        """
        # åˆå§‹åŒ–åˆ†ç±»å­—å…¸
        categorized = {cat: [] for cat in self.config.channel_categories}
        
        # æŒ‰åˆ†ç±»ç»„ç»‡é¢‘é“
        for channel in self.get_ordered_channels(results.keys()):
            streams = results.get(channel, [])
            if not streams:
                continue
                
            matched = False
            # åŒ¹é…é¢‘é“åˆ†ç±»
            for cat, keywords in self.config.channel_categories.items():
                if any(keyword in channel for keyword in keywords):
                    # æ·»åŠ æ ¼å¼åŒ–çš„é¢‘é“ä¿¡æ¯
                    categorized[cat].extend(
                        f"{channel},{url} # é€Ÿåº¦: {speed:.1f}KB/s" 
                        for url, speed in streams
                    )
                    matched = True
                    break
            
            # æœªåŒ¹é…çš„é¢‘é“å½’ä¸ºå…¶ä»–
            if not matched:
                categorized["å…¶ä»–é¢‘é“,#genre#"].extend(
                    f"{channel},{url} # é€Ÿåº¦: {speed:.1f}KB/s" 
                    for url, speed in streams
                )
        
        # å†™å…¥æ–‡ä»¶
        with open(self.config.output_files['txt'], 'w', encoding='utf-8') as f:
            for cat, items in categorized.items():
                if items:
                    f.write(f"\n{cat}\n")  # åˆ†ç±»æ ‡é¢˜
                    f.write("\n".join(items) + "\n")  # é¢‘é“åˆ—è¡¨
        
        total_streams = sum(len(items) for items in categorized.values())
        self.log(f"ç”ŸæˆTXTæ–‡ä»¶: {self.config.output_files['txt']} (å…± {total_streams} ä¸ªæº)", "SUCCESS")

    def generate_m3u_file(self, results: Dict[str, List[Tuple[str, float]]]):
        """
        ç”ŸæˆM3Uæ ¼å¼æ–‡ä»¶
        
        Args:
            results: æµ‹é€Ÿç»“æœå­—å…¸
        """
        total_streams = 0
        
        with open(self.config.output_files['m3u'], 'w', encoding='utf-8') as f:
            f.write('#EXTM3U x-tvg-url=""\n')  # M3Uæ–‡ä»¶å¤´
            
            # éå†æ‰€æœ‰é¢‘é“
            for channel in self.get_ordered_channels(results.keys()):
                streams = results.get(channel, [])
                for url, speed in streams:
                    quality = self.get_speed_quality(speed)  # é€Ÿåº¦è´¨é‡
                    group = self.categorize_channel(channel)  # é¢‘é“åˆ†ç±»
                    
                    # å†™å…¥EXTINFè¡Œ
                    f.write(f'#EXTINF:-1 tvg-id="" tvg-name="{channel}" tvg-logo="" group-title="{group}",{channel} [é€Ÿåº¦: {speed:.1f}KB/s {quality}]\n')
                    f.write(f'{url}\n')  # URLè¡Œ
                    total_streams += 1
        
        self.log(f"ç”ŸæˆM3Uæ–‡ä»¶: {self.config.output_files['m3u']} (å…± {total_streams} ä¸ªæº)", "SUCCESS")

    def generate_json_file(self, results: Dict[str, List[Tuple[str, float]]]):
        """
        ç”ŸæˆJSONæ ¼å¼æ–‡ä»¶
        
        Args:
            results: æµ‹é€Ÿç»“æœå­—å…¸
        """
        # æ„å»ºæ•°æ®ç»“æ„
        data = {
            "metadata": {
                "generated_time": time.strftime('%Y-%m-%d %H:%M:%S'),
                "total_channels": len(results),
                "total_streams": sum(len(streams) for streams in results.values())
            },
            "channels": {}
        }
        
        # å¡«å……é¢‘é“æ•°æ®
        for channel, streams in results.items():
            data["channels"][channel] = {
                "best_speed": streams[0][1] if streams else 0,  # æœ€ä½³é€Ÿåº¦
                "stream_count": len(streams),  # æºæ•°é‡
                "streams": [
                    {
                        "url": url,
                        "speed": speed,
                        "quality": self.get_speed_quality(speed),  # è´¨é‡è¯„çº§
                        "domain": self._extract_domain(url)  # åŸŸå
                    }
                    for url, speed in streams
                ],
                "category": self.categorize_channel(channel)  # åˆ†ç±»
            }
        
        # å†™å…¥JSONæ–‡ä»¶
        with open(self.config.output_files['json'], 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)  # ç¾åŒ–è¾“å‡º
        
        self.log(f"ç”ŸæˆJSONæ–‡ä»¶: {self.config.output_files['json']}", "SUCCESS")

    def generate_report(self, results: Dict[str, List[Tuple[str, float]]]):
        """
        ç”Ÿæˆè¯¦ç»†æµ‹é€ŸæŠ¥å‘Š
        
        Args:
            results: æµ‹é€Ÿç»“æœå­—å…¸
        """
        speed_stats = []  # é€Ÿåº¦ç»Ÿè®¡
        valid_channels = []  # æœ‰æ•ˆé¢‘é“åˆ—è¡¨
        
        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        for channel, streams in results.items():
            if streams:
                best_speed = streams[0][1]  # æ¯ä¸ªé¢‘é“çš„æœ€ä½³é€Ÿåº¦
                speed_stats.append(best_speed)
                valid_channels.append((channel, best_speed, len(streams)))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
        if not speed_stats:
            self.log("æ— æœ‰æ•ˆæµ‹é€Ÿç»“æœï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ", "WARNING")
            return
        
        # æŒ‰é€Ÿåº¦æ’åºé¢‘é“ï¼ˆé™åºï¼‰
        valid_channels.sort(key=lambda x: x[1], reverse=True)
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_lines = [
            "="*60,
            "IPTVç›´æ’­æºæµ‹é€ŸæŠ¥å‘Š",
            "="*60,
            f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"æœ‰æ•ˆé¢‘é“æ•°: {len(valid_channels)}",
            f"æ€»æºæ•°é‡: {sum(x[2] for x in valid_channels)}",
            f"å¹³å‡é€Ÿåº¦: {sum(speed_stats)/len(speed_stats):.1f} KB/s",
            f"æœ€å¿«é€Ÿåº¦: {max(speed_stats):.1f} KB/s",
            f"æœ€æ…¢é€Ÿåº¦: {min(speed_stats):.1f} KB/s",
            f"é€Ÿåº¦ä¸­ä½æ•°: {sorted(speed_stats)[len(speed_stats)//2]:.1f} KB/s",
            "\né€Ÿåº¦åˆ†å¸ƒ:",
        ]
        
        # é€Ÿåº¦åˆ†å¸ƒç»Ÿè®¡
        speed_ranges = [
            (1000, "æé€Ÿ(>1000)"),
            (500, "ä¼˜ç§€(500-1000)"), 
            (200, "è‰¯å¥½(200-500)"),
            (100, "ä¸€èˆ¬(100-200)"),
            (50, "è¾ƒå·®(50-100)"),
            (0, "æå·®(<50)")
        ]
        
        range_counts = {}
        total = len(speed_stats)
        
        # è®¡ç®—æ¯ä¸ªé€Ÿåº¦åŒºé—´çš„é¢‘é“æ•°é‡
        for i, (min_speed, range_name) in enumerate(speed_ranges):
            if i == len(speed_ranges) - 1:  # æœ€åä¸€ä¸ªåŒºé—´
                count = len([s for s in speed_stats if s <= min_speed])
            else:
                next_min = speed_ranges[i+1][0]
                count = len([s for s in speed_stats if min_speed < s <= next_min])
            range_counts[range_name] = count
            percentage = count / total * 100  # ç™¾åˆ†æ¯”
            report_lines.append(f"  {range_name:<15} KB/s: {count:>3}ä¸ªé¢‘é“ ({percentage:5.1f}%)")
        
        # æ·»åŠ TOP 20é¢‘é“æ’å
        report_lines.extend(["\né¢‘é“é€Ÿåº¦æ’å TOP 20:", "-"*50])
        
        for i, (channel, speed, count) in enumerate(valid_channels[:20], 1):
            quality = self.get_speed_quality(speed)
            report_lines.append(f"{i:2d}. {channel:<20} {speed:6.1f} KB/s ({quality:>4}, {count}ä¸ªæº)")
        
        # å¦‚æœé¢‘é“å¤šäº20ä¸ªï¼Œæ·»åŠ æç¤º
        if len(valid_channels) > 20:
            report_lines.append(f"...(å…±{len(valid_channels)}ä¸ªé¢‘é“)")
        
        report_content = "\n".join(report_lines)
        
        # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
        with open(self.config.output_files['report'], 'w', encoding='utf-8') as f:
            f.write(report_content + "\n")
        
        self.log(f"ç”Ÿæˆæµ‹é€ŸæŠ¥å‘Š: {self.config.output_files['report']}", "SUCCESS")
        
        # åœ¨æ§åˆ¶å°æ˜¾ç¤ºæ‘˜è¦
        self.log("\n" + "\n".join(report_lines[:15]))

    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def get_ordered_channels(self, channels: List[str]) -> List[str]:
        """
        æŒ‰ç…§æ¨¡æ¿é¡ºåºæ’åºé¢‘é“åˆ—è¡¨
        
        Args:
            channels: é¢‘é“åç§°åˆ—è¡¨
            
        Returns:
            List[str]: æ’åºåçš„é¢‘é“åˆ—è¡¨
        """
        # å¦‚æœæ²¡æœ‰æ¨¡æ¿ï¼ŒæŒ‰å­—æ¯é¡ºåºæ’åº
        if not self.valid_channels:
            return sorted(channels)
        
        ordered = []
        # é¦–å…ˆæ·»åŠ æ¨¡æ¿ä¸­çš„é¢‘é“ï¼ˆæŒ‰æ¨¡æ¿é¡ºåºï¼‰
        if self.config.template_file.exists():
            with open(self.config.template_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        if match := self.channel_pattern.match(line):
                            channel = match.group(1).strip()
                            if channel in channels and channel not in ordered:
                                ordered.append(channel)
        
        # æ·»åŠ æœªåœ¨æ¨¡æ¿ä¸­çš„é¢‘é“ï¼ˆæŒ‰å­—æ¯é¡ºåºï¼‰
        remaining_channels = [ch for ch in channels if ch not in ordered]
        ordered.extend(sorted(remaining_channels))
                
        return ordered

    def _extract_domain(self, url: str) -> str:
        """
        ä»URLæå–åŸŸå
        
        Args:
            url: å®Œæ•´URL
            
        Returns:
            str: åŸŸåæˆ–æˆªæ–­çš„URL
        """
        try:
            netloc = urlparse(url).netloc  # è§£æç½‘ç»œä½ç½®
            return netloc.split(':')[0]  # ç§»é™¤ç«¯å£å·
        except:
            # è§£æå¤±è´¥æ—¶è¿”å›æˆªæ–­çš„URL
            return url[:25] + "..." if len(url) > 25 else url

    def categorize_channel(self, channel: str) -> str:
        """
        æ ¹æ®é¢‘é“åç§°åˆ†ç±»
        
        Args:
            channel: é¢‘é“åç§°
            
        Returns:
            str: åˆ†ç±»åç§°
        """
        for category, keywords in self.config.channel_categories.items():
            if any(keyword in channel for keyword in keywords):
                return category.replace(",#genre#", "")  # ç§»é™¤æ ¼å¼åç¼€
        return "å…¶ä»–é¢‘é“"  # é»˜è®¤åˆ†ç±»

    def get_speed_quality(self, speed: float) -> str:
        """
        æ ¹æ®é€Ÿåº¦å€¼è·å–è´¨é‡è¯„çº§
        
        Args:
            speed: é€Ÿåº¦å€¼(KB/s)
            
        Returns:
            str: è´¨é‡è¯„çº§æè¿°
        """
        if speed > 1000: return "æé€Ÿ"
        if speed > 500: return "ä¼˜ç§€" 
        if speed > 200: return "è‰¯å¥½"
        if speed > 100: return "ä¸€èˆ¬"
        if speed > 50: return "è¾ƒå·®"
        return "æå·®"

    # ==================== ä¸»æµç¨‹ ====================
    
    def run(self):
        """è¿è¡Œä¸»å¤„ç†æµç¨‹"""
        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
        
        # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
        self.log("="*60)
        self.log("ğŸ¬ IPTVç›´æ’­æºå¤„ç†å·¥å…·å¯åŠ¨")
        self.log("="*60)
        
        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        self.log(f"ğŸ“‹ é…ç½®å‚æ•°:")
        self.log(f"   è¶…æ—¶æ—¶é—´: {self.config.timeout}s")
        self.log(f"   å¹¶å‘çº¿ç¨‹: {self.config.max_workers}")
        self.log(f"   æµ‹é€Ÿæ•°æ®: {self.config.test_size_kb}KB")
        self.log(f"   é‡è¯•æ¬¡æ•°: {self.config.retry_times}")
        self.log(f"   æ•°æ®æºæ•°: {len(self.config.source_urls)}")
        
        # æ˜¾ç¤ºæ¨¡æ¿ä¿¡æ¯
        if self.valid_channels:
            self.log(f"ğŸ“º æ¨¡æ¿é¢‘é“: {len(self.valid_channels)}ä¸ª")
        else:
            self.log("âš ï¸  æœªä½¿ç”¨æ¨¡æ¿è¿‡æ»¤ï¼Œå°†å¤„ç†æ‰€æœ‰é¢‘é“", "WARNING")
        
        try:
            # é˜¶æ®µ1: æŠ“å–ç›´æ’­æº
            self.log("\nğŸš€ é˜¶æ®µ1: æŠ“å–ç›´æ’­æº...")
            if content := self.fetch_streams():
                
                # é˜¶æ®µ2: è§£æç›´æ’­æºæ•°æ®
                self.log("\nğŸ” é˜¶æ®µ2: è§£æç›´æ’­æºæ•°æ®...")
                df = self.parse_content(content)
                
                # æ˜¾ç¤ºé¢‘é“åŒ¹é…æƒ…å†µ
                matched_channels = set(df['program_name'].unique())
                self.log(f"\nğŸ“Š é¢‘é“åŒ¹é…ç»“æœ:")
                self.log(f"   å‘ç°é¢‘é“æ€»æ•°: {len(matched_channels)}")
                self.log(f"   ç›´æ’­æºæ€»æ•°: {len(df)}")
                
                # æ¨¡æ¿åŒ¹é…ç»Ÿè®¡
                if self.valid_channels:
                    matched_template = len(matched_channels & self.valid_channels)
                    self.log(f"   åŒ¹é…æ¨¡æ¿é¢‘é“: {matched_template}/{len(self.valid_channels)}")
                    
                    unmatched = self.valid_channels - matched_channels
                    if unmatched:
                        self.log(f"   æœªåŒ¹é…æ¨¡æ¿é¢‘é“: {len(unmatched)}ä¸ª", "WARNING")
                
                # æ•´ç†å’Œç»„ç»‡æ•°æ®
                grouped = self.organize_streams(df)
                self.log(f"\nğŸ“‹ æ•´ç†å: {len(grouped)}ä¸ªé¢‘é“")
                
                # é˜¶æ®µ3: æµ‹é€Ÿå’Œä¼˜åŒ–
                self.log("\nâ±ï¸  é˜¶æ®µ3: å¼€å§‹æµ‹é€Ÿ...")
                speed_results = self.test_all_channels(grouped)
                
                # é˜¶æ®µ4: ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
                self.log("\nğŸ’¾ é˜¶æ®µ4: ç”Ÿæˆè¾“å‡ºæ–‡ä»¶...")
                self.generate_output_files(speed_results)
                
                # ç»Ÿè®¡æœ€ç»ˆç»“æœ
                total_streams = sum(len(streams) for streams in speed_results.values())
                valid_channel_count = len([ch for ch in speed_results if speed_results[ch]])
                
                elapsed_time = time.time() - start_time
                self.log(f"\nğŸ‰ å¤„ç†å®Œæˆ!")
                self.log(f"   âœ… æœ‰æ•ˆé¢‘é“: {valid_channel_count}ä¸ª")
                self.log(f"   ğŸ“º æ€»ç›´æ’­æº: {total_streams}ä¸ª") 
                self.log(f"   â° æ€»è€—æ—¶: {elapsed_time:.1f}ç§’")
                self.log(f"   ğŸ’¾ è¾“å‡ºæ–‡ä»¶:")
                # æ˜¾ç¤ºæ‰€æœ‰è¾“å‡ºæ–‡ä»¶ä¿¡æ¯
                for file_type, file_path in self.config.output_files.items():
                    if file_path.exists():
                        size = file_path.stat().st_size
                        self.log(f"      {file_type.upper()}: {file_path} ({size} bytes)")
                
            else:
                self.log("âŒ æœªèƒ½è·å–æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æºURL", "ERROR")
                
        except KeyboardInterrupt:
            self.log("ğŸ‘‹ ç”¨æˆ·ä¸­æ–­æ“ä½œ", "WARNING")
        except Exception as e:
            self.log(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", "ERROR")
            import traceback
            self.log(traceback.format_exc(), "ERROR")  # è®°å½•å®Œæ•´å †æ ˆè·Ÿè¸ª

def main():
    """ä¸»å‡½æ•° - ç¨‹åºå…¥å£ç‚¹"""
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description='IPTVç›´æ’­æºæŠ“å–ä¸æµ‹é€Ÿå·¥å…·')
    parser.add_argument('--timeout', type=int, default=8, help='è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)')
    parser.add_argument('--workers', type=int, default=4, help='å¹¶å‘çº¿ç¨‹æ•°')
    parser.add_argument('--test-size', type=int, default=128, help='æµ‹é€Ÿæ•°æ®å¤§å°(KB)')
    parser.add_argument('--retry', type=int, default=2, help='é‡è¯•æ¬¡æ•°')
    parser.add_argument('--template', type=str, help='æ¨¡æ¿æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', type=str, help='è¾“å‡ºç›®å½•è·¯å¾„')
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºé…ç½®å¯¹è±¡
        config = IPTVConfig()
        config.timeout = args.timeout
        config.max_workers = args.workers
        config.test_size_kb = args.test_size
        config.retry_times = args.retry
        
        # å¤„ç†è‡ªå®šä¹‰æ¨¡æ¿æ–‡ä»¶
        if args.template:
            config.template_file = Path(args.template)
        # å¤„ç†è‡ªå®šä¹‰è¾“å‡ºç›®å½•
        if args.output_dir:
            config.base_dir = Path(args.output_dir)
            # æ›´æ–°æ‰€æœ‰è¾“å‡ºæ–‡ä»¶è·¯å¾„
            for key in config.output_files:
                config.output_files[key] = config.base_dir / config.output_files[key].name
        
        # åˆ›å»ºå¹¶è¿è¡Œå·¥å…·
        tool = IPTVTool(config)
        tool.run()
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()  # æ‰“å°é”™è¯¯å †æ ˆ

if __name__ == "__main__":
    main()  # ç¨‹åºå…¥å£
