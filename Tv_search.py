import random
import requests
from lxml import etree
import os
import time
import sys
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import logging.handlers
from retrying import retry
from dotenv import load_dotenv
from urllib.parse import urlparse

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('tv_search.log', encoding='utf-8'),
        logging.handlers.RotatingFileHandler(
            'tv_search_debug.log',
            maxBytes=5*1024*1024,  # 5MB
            backupCount=3,
            encoding='utf-8'
        )
    ]
)
logger = logging.getLogger(__name__)

class TVSearchCrawler:
    def __init__(self, speed_threshold=1.0, max_workers=3):
        self.speed_threshold = float(speed_threshold)
        self.max_workers = max_workers
        self.current_directory = os.getcwd()
        self.output_file_path = os.path.join(self.current_directory, 'live.txt')
        
        # ç”¨æˆ·ä»£ç†åˆ—è¡¨
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:117.0) Gecko/20100101 Firefox/117.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.179 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 12_6_3) AppleWebKit/537.36 (KHTML, like Gecko) Version/15.6 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.179 Safari/537.36',
        ]
        
        # ä»£ç†é…ç½®
        self.proxies = self._init_proxies()
        
        # è¯·æ±‚å»¶è¿Ÿ
        self.request_delays = [1, 2, 3]
        
        # æœç´¢æºé…ç½®
        self.search_sources = [
            {'name': 'tonkiang', 'url': 'http://tonkiang.us/'},
            {'name': 'iptv', 'url': 'http://example.iptvsearch.com/'}
        ]
        
        self.setup_output_file()
    
    def _init_proxies(self):
        """åˆå§‹åŒ–ä»£ç†é…ç½®"""
        proxies = []
        # ä»ç¯å¢ƒå˜é‡è·å–ä»£ç†
        env_proxy = os.getenv('HTTP_PROXY')
        if env_proxy:
            proxies.append(env_proxy)
        
        # æ·»åŠ å¤‡ç”¨ä»£ç†
        proxies.extend([
            'http://proxy1.example.com:8080',
            'http://proxy2.example.com:8080'
        ])
        return proxies
    
    def setup_output_file(self):
        """åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶"""
        with open(self.output_file_path, 'w', encoding='utf-8') as f:
            f.write('# TV Search è‡ªåŠ¨ç”Ÿæˆçš„ç›´æ’­æºæ–‡ä»¶\n')
            f.write('# æ›´æ–°æ—¶é—´: {}\n'.format(time.strftime('%Y-%m-%d %H:%M:%S')))
            f.write('# é€Ÿåº¦é˜ˆå€¼: {} MB/s\n'.format(self.speed_threshold))
            f.write('# ç”Ÿæˆå·¥å…·: Tv_search.py\n\n')
    
    def setup_driver(self):
        """é…ç½®Chromeæµè§ˆå™¨é©±åŠ¨"""
        user_agent = random.choice(self.user_agents)
        proxy = random.choice(self.proxies) if self.proxies else None
        
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument(f"user-agent={user_agent}")
        
        if proxy:
            chrome_options.add_argument(f'--proxy-server={proxy}')
        
        # GitHub Actions ç¯å¢ƒç‰¹æ®Šé…ç½®
        chrome_options.binary_location = "/usr/bin/google-chrome"
        
        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(30)
        return driver
    
    @retry(stop_max_attempt_number=3, wait_fixed=2000)
    def search_tv_channels(self, name):
        """æœç´¢æŒ‡å®šé¢‘é“åç§°çš„M3U8é“¾æ¥"""
        all_m3u8 = []
        for source in self.search_sources:
            try:
                logger.info(f"ğŸ” åœ¨ {source['name']} æœç´¢é¢‘é“: {name}")
                m3u8_list = self._search_single_source(source['url'], name)
                all_m3u8.extend(m3u8_list)
                time.sleep(random.choice(self.request_delays))
            except Exception as e:
                logger.error(f"âŒ åœ¨ {source['name']} æœç´¢å¤±è´¥: {e}")
                continue
        return all_m3u8
    
    def _search_single_source(self, url, name):
        """åœ¨å•ä¸ªæºæœç´¢é¢‘é“"""
        driver = self.setup_driver()
        m3u8_list = []
        
        try:
            driver.get(url)
            
            # ç­‰å¾…æœç´¢æ¡†åŠ è½½
            search_input = WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.ID, 'search'))
            )
            search_input.clear()
            search_input.send_keys(name)
            
            # ç‚¹å‡»æœç´¢æŒ‰é’®
            submit_button = driver.find_element(By.NAME, 'Submit')
            submit_button.click()
            
            # ç­‰å¾…ç»“æœåŠ è½½
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CLASS_NAME, 'resultplus'))
            )
            
            # è§£æé¡µé¢è·å–M3U8é“¾æ¥
            page_source = driver.page_source
            root = etree.HTML(page_source)
            result_divs = root.xpath("//div[@class='resultplus']")
            
            logger.info(f"ğŸ“º é¢‘é“ '{name}' æ‰¾åˆ° {len(result_divs)} ä¸ªç»“æœ")
            
            for div in result_divs:
                for element in div.xpath(".//tba"):
                    if element.text and element.text.strip():
                        url = element.text.strip()
                        if url.startswith('http') and 'm3u8' in url:
                            m3u8_list.append(url)
                            logger.debug(f"âœ… æ‰¾åˆ°æœ‰æ•ˆé“¾æ¥: {url}")
                            
        except Exception as e:
            logger.error(f"âŒ æœç´¢é¢‘é“ '{name}' æ—¶å‡ºé”™: {e}")
            raise
        finally:
            driver.quit()
            
        return m3u8_list
    
    def test_stream_quality(self, url, name):
        """æµ‹è¯•ç›´æ’­æµè´¨é‡å’Œé€Ÿåº¦"""
        try:
            logger.info(f"ğŸ§ª æµ‹è¯•ç›´æ’­æµ: {name}")
            
            # é¦–æ¬¡è¿æ¥æµ‹è¯•
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            # å†…å®¹ç±»å‹æ£€æŸ¥
            content_type = response.headers.get('content-type', '')
            if 'application/x-mpegurl' not in content_type and '#EXTM3U' not in response.text:
                logger.debug(f"âš ï¸ éM3U8æ ¼å¼: {url}")
                return None
            
            # é€Ÿåº¦æµ‹è¯•
            download_speed = self.measure_download_speed(url, response.text)
            if not download_speed or download_speed < self.speed_threshold:
                logger.debug(f"ğŸŒ é¢‘é“ {name} é€Ÿåº¦è¿‡æ…¢: {download_speed:.2f} MB/s")
                return None
            
            # äºŒæ¬¡éªŒè¯ç¡®ä¿ç¨³å®šæ€§
            try:
                response = requests.get(url, timeout=5)
                if response.status_code != 200:
                    return None
            except:
                return None
            
            logger.info(f"ğŸ¯ é¢‘é“ {name} é€šè¿‡æ‰€æœ‰æ£€æŸ¥: {download_speed:.2f} MB/s")
            return url
        except Exception as e:
            logger.debug(f"ğŸ”´ æµæµ‹è¯•å¤±è´¥ {url}: {e}")
            return None
    
    def measure_download_speed(self, base_url, m3u8_content):
        """æµ‹é‡ä¸‹è½½é€Ÿåº¦"""
        try:
            lines = m3u8_content.split('\n')
            segments = [line.strip() for line in lines if line and not line.startswith('#')]
            
            if not segments:
                return None
            
            # æµ‹è¯•å‰3ä¸ªç‰‡æ®µå–å¹³å‡å€¼
            test_segments = segments[:3]
            total_speed = 0
            valid_tests = 0
            
            for segment in test_segments:
                if not segment.startswith('http'):
                    segment = base_url.rsplit('/', 1)[0] + '/' + segment
                
                try:
                    start_time = time.time()
                    response = requests.get(segment, timeout=10, stream=True)
                    content = response.content
                    end_time = time.time()
                    
                    if response.status_code == 200:
                        download_time = end_time - start_time
                        file_size = len(content)
                        speed = file_size / download_time / (1024 * 1024)  # MB/s
                        total_speed += speed
                        valid_tests += 1
                except Exception:
                    continue
            
            return total_speed / valid_tests if valid_tests > 0 else None
        except Exception as e:
            logger.debug(f"â±ï¸ é€Ÿåº¦æµ‹é‡å¤±è´¥: {e}")
            return None
    
    def process_tv_category(self, category_name):
        """å¤„ç†ä¸€ä¸ªç”µè§†é¢‘é“åˆ†ç±»"""
        category_file = f'{category_name}.txt'
        if not os.path.exists(category_file):
            logger.warning(f"ğŸ“„ é¢‘é“æ–‡ä»¶ä¸å­˜åœ¨: {category_file}")
            return
        
        # è¯»å–é¢‘é“åˆ—è¡¨
        with open(category_file, 'r', encoding='utf-8') as f:
            channel_names = [line.strip() for line in f if line.strip()]
        
        logger.info(f"ğŸ¬ å¤„ç†ç”µè§†åˆ†ç±» '{category_name}', å…± {len(channel_names)} ä¸ªé¢‘é“")
        
        # å†™å…¥åˆ†ç±»æ ‡é¢˜
        with open(self.output_file_path, 'a', encoding='utf-8') as f:
            f.write(f'\n{category_name},#genre#\n')
        
        valid_count = 0
        
        # å¤„ç†æ¯ä¸ªé¢‘é“
        for channel_name in channel_names:
            logger.info(f"ğŸ“¡ å¤„ç†é¢‘é“: {channel_name}")
            
            # æœç´¢M3U8é“¾æ¥
            m3u8_urls = self.search_tv_channels(channel_name)
            if not m3u8_urls:
                logger.warning(f"ğŸ” æœªæ‰¾åˆ°é¢‘é“ {channel_name} çš„é“¾æ¥")
                continue
            
            # æµ‹è¯•é“¾æ¥è´¨é‡
            valid_urls = []
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_url = {
                    executor.submit(self.test_stream_quality, url, channel_name): url 
                    for url in m3u8_urls[:5]  # é™åˆ¶æµ‹è¯•æ•°é‡é¿å…è¶…æ—¶
                }
                
                for future in as_completed(future_to_url):
                    result = future.result()
                    if result:
                        valid_urls.append(result)
            
            # ä¿å­˜æœ‰æ•ˆé“¾æ¥
            if valid_urls:
                with open(self.output_file_path, 'a', encoding='utf-8') as f:
                    for url in valid_urls:
                        f.write(f'{channel_name},{url}\n')
                valid_count += len(valid_urls)
                logger.info(f"âœ… é¢‘é“ '{channel_name}' æ‰¾åˆ° {len(valid_urls)} ä¸ªæœ‰æ•ˆé“¾æ¥")
            else:
                logger.warning(f"âŒ é¢‘é“ '{channel_name}' æ— æœ‰æ•ˆé“¾æ¥")
            
            time.sleep(random.choice(self.request_delays))  # éšæœºå»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        
        logger.info(f"ğŸ‰ ç”µè§†åˆ†ç±» '{category_name}' å®Œæˆï¼Œå…±æ‰¾åˆ° {valid_count} ä¸ªæœ‰æ•ˆé“¾æ¥")
    
    def remove_duplicate_streams(self):
        """å»é™¤é‡å¤çš„ç›´æ’­æº"""
        if not os.path.exists(self.output_file_path):
            return
        
        with open(self.output_file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # åˆ†ç¦»æ–‡ä»¶å¤´å’Œä¿¡æ¯è¡Œ
        header = []
        content_lines = []
        seen_urls = set()
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#') or line.endswith('#genre#'):
                header.append(line + '\n')
            else:
                parts = line.split(',', 1)
                if len(parts) == 2:
                    channel, url = parts[0].strip(), parts[1].strip()
                    parsed_url = urlparse(url)
                    clean_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}"
                    if clean_url not in seen_urls:
                        seen_urls.add(clean_url)
                        content_lines.append(line + '\n')
        
        # é‡æ–°å†™å…¥æ–‡ä»¶
        with open(self.output_file_path, 'w', encoding='utf-8') as f:
            f.writelines(header)
            f.writelines(content_lines)
        
        logger.info(f"ğŸ”„ å»é‡å®Œæˆï¼Œå‰©ä½™ {len(content_lines)} ä¸ªå”¯ä¸€ç›´æ’­æº")
    
    def cleanup_old_streams(self, days=7):
        """æ¸…ç†è¿‡æœŸçš„ç›´æ’­æº"""
        # å®ç°åŸºäºæ—¶é—´æˆ³çš„æ¸…ç†é€»è¾‘
        # å¯ä»¥æ‰©å±•ä¸ºä»æ–‡ä»¶å†…å®¹ä¸­è§£æå‡ºæ—¶é—´ä¿¡æ¯
        pass
    
    def run_tv_search(self, categories=None):
        """è¿è¡ŒTVæœç´¢ä¸»ç¨‹åº"""
        if categories is None:
            categories = ['å¤®è§†é¢‘é“']
        elif isinstance(categories, str):
            categories = [cat.strip() for cat in categories.split(',')]
        
        logger.info("ğŸš€ å¼€å§‹TVæœç´¢ç›´æ’­æºçˆ¬è™«")
        logger.info(f"âš¡ é€Ÿåº¦é˜ˆå€¼: {self.speed_threshold} MB/s")
        logger.info(f"ğŸ“º å¤„ç†åˆ†ç±»: {categories}")
        
        start_time = time.time()
        total_valid_streams = 0
        
        for category in categories:
            self.process_tv_category(category)
        
        # å»é‡å¤„ç†
        self.remove_duplicate_streams()
        
        # ç»Ÿè®¡æœ€ç»ˆç»“æœ
        if os.path.exists(self.output_file_path):
            with open(self.output_file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                content_lines = [line for line in lines if line.strip() and not line.startswith('#') and not line.endswith('#genre#\n')]
                total_valid_streams = len(content_lines)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        logger.info("ğŸŠ TVæœç´¢å®Œæˆï¼")
        logger.info(f"â° æ€»è€—æ—¶: {execution_time:.2f} ç§’")
        logger.info(f"ğŸ“Š æ€»æœ‰æ•ˆç›´æ’­æº: {total_valid_streams} ä¸ª")
        
        return total_valid_streams

def main():
    """ä¸»å‡½æ•°"""
    try:
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        speed_threshold = os.getenv('SPEED_THRESHOLD', '1.0')
        categories = os.getenv('CATEGORIES', 'å¤®è§†é¢‘é“')
        
        # åˆ›å»ºTVæœç´¢å®ä¾‹
        tv_crawler = TVSearchCrawler(
            speed_threshold=speed_threshold, 
            max_workers=3
        )
        
        # è¿è¡Œæœç´¢
        total_streams = tv_crawler.run_tv_search(categories)
        
        # æ¸…ç†æ—§æ•°æ®
        tv_crawler.cleanup_old_streams()
        
        # è¾“å‡ºç»“æœæ‘˜è¦
        print(f"\n{'='*50}")
        print(f"TVæœç´¢å®Œæˆæ‘˜è¦:")
        print(f"  é€Ÿåº¦é˜ˆå€¼: {speed_threshold} MB/s")
        print(f"  å¤„ç†åˆ†ç±»: {categories}")
        print(f"  æœ‰æ•ˆç›´æ’­æº: {total_streams} ä¸ª")
        print(f"  è¾“å‡ºæ–‡ä»¶: live.txt")
        print(f"{'='*50}")
        
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"ğŸ’¥ TVæœç´¢ç¨‹åºå¼‚å¸¸: {e}", exc_info=True)
        sys.exit(1)

if __name__ == '__main__':
    main()
